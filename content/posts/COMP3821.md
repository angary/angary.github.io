+++
title = "COMP3821"
description = "Extended Algorithms and Programming Techniques"
type = ["posts","post"]
tags = []
date = "2021-04-25"
categories = []
series = ["UNSW"]
toc = true
[ author ]
  name = "Gary Sun"
+++

# Work in progress
---
The course is split into four topics
1. Divide and Conquer
2. Greedy Algorithms
3. Dynamic Programming
4. Linear Programming and Reductions

However for the sake of organising notes, I've included an extra section to cover knowledge not covered in the course's prerequisite (COMP2521), and split Linear Programming and Reductions into two sections.


# Misc Knowledge
---
## Asymptotic Runtime
### $O(n)$, Big O
- Denotes the upper bound of the runtime of an algorithm
- If $f(n) = O(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq f(n) \leq cg(n),  \forall n \geq n_0$
- $f(n) = O(g(n))$ means that $f(n)$ does not grow substantially faster than $g(n)$ because a multiple of $g(n)$ eventually dominates $f(n)$
- Most commonly used as we are concerned with the worst runtime

### $\Theta(n)$, Big Theta
- Denotes a tight bound of the runtime of an algorithm
- $f(n) = \Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$
- Less commonly used than $O(n)$, but more common than $\Omega(n)$

### $\Omega(n)$, Big Omega
- Denotes the lower bound of the runtime of an algorithm
- If $f(n) = \Omega(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq cg(n) \leq f(n),  \forall n \geq n_0$
- $f(n) = \Omega(g(n))$ means that $f(n)$ grows at least as fast as $g(n)$, because $f(n)$ eventually dominates a multiple of $g(n)$
- Least often used as we usually aren't concerned with the best runtime

## Math
### Log Identity
If $a, b, c > 0$ then
$$a^{\log_{b}c} = c^{\log_{b}a}$$
Proof
$$
\begin{align*}
\log_{b}c \cdot \log_{b}a &= log_{b}a \cdot \log_{b}c \\\\
\log_{b}(a^{\log_{b}c}) &= \log_{b}(c^{\log_{b}a}) \\\\
a^{log_{b}c} &= c^{\log_{b}a}
\end{align*}
$$
### Roots of Unity
#### Representation
Complex numbers $z = a + ib$ can be represented using 
- $modulus$ $|z| = \sqrt{a^2 + b^2}$
- $argument$ $\arg(z)$, which is an angle taking values in $(-\pi, \pi]$ 

and satisfying:
$$z = |z|e^{i \arg(z)} = |z|(\cos(\arg(z)) + i \sin(\arg(z))).$$

and

$$z^n = \left(|z|e^{i \arg(z)}\right)^n = |z|^ne^{in\arg(z)} = |z|^n(n\cos(\arg(z)) + i \sin(n\arg(z))).$$

#### Properties
Roots of unity of order $n$ are complex numbers which satisfy $z^n = 1$.
- If $z^n = |z|^n(\cos(n \arg(z)) + i\sin(n\arg(z))) = 1$ then $|z| = 1$ and $n\arg(z)$ is a multiple of $2\pi$
- Thus, $n\arg( z) = 2\pi k$, i.e. $arg(z) = \frac{2\pi k}{n}$
- We denote $\omega_n = e^{i \frac{2\pi}{n}}$, such that $\omega_n$ is called a primitive root of unity order $n$
- A root of unity $\omega$ of order $n$ is primitive if all other roots of unity of the same order can be obtained as its powers $\omega^k$.

For $\omega_n = e^{i \frac{2\pi}{n}}$ and for all $k$ such that $0 \leq k \leq n - 1$,
$$ (\omega_n^k)^n = ((\omega_n)^k)^n = (\omega_n)^{nk} = ((\omega_n)^n)^k = 1^k = 1.$$

If $k + m \geq n$ then $k + m = n + l$ for $l = (k + m) \mod(n)$ and we have 
$$\omega_n^k \omega_n^m = \omega_n^{k+m} = \omega_n^{n+l} = \omega_n^n \omega_n^l = 1 \cdot \omega_n^l = \omega_n^l \text{ where } 0 \leq l \leq n.$$

Therefore, the product of any two roots of unity of the same order is just another root of unity of the same order (this is not the case for addition, as the sum of two roots of unity is usually not another root of unity).

The Cancellation Lemma: $\omega_{kn}^{km} = \omega_n^m$, and its proof is below
$$\omega_{kn}^{km} = (\omega_{kn})^{km} = \left(e^{i \frac{2\pi}{kn}}\right)^{km} = e^{i \frac{2\pi k m}{k n}} = e^{i \frac{2\pi m}{n}} = \left(e^{i \frac{2\pi}{n}}\right)^m = \omega_n^m.$$

# Divide and Conquer
---
Divide and conquer algorithms recursively break down a problem into two or more non overlapping subproblems, before merging them together to become easy to solve.

Common examples of Divide and Conquer algorithms include
- Binary Search
- Merge Sort
- Fast Fourier Transform

## Sample algorithms
### Karatsuba's Fast Integer Multiplication
#### Naive method
Given 2 'big integers', i.e. a number, stored as an array, where the values at an the $i^{th}$ index is the $i^{th}$ digit of the integer, and there are $n$ digits, a sample solution would look like
```
def mul(A, B, n, m):
    C = [0] * (n + m)
    for i in range(n):
        for j in range(m):
            A[i + j] += B[i] * Y[j]
    for i in range(n + m - 1):
        C[i + 1] += C[i] // 10
        C[i] %= 10
    return C
```
and hence have a runtime of $\Theta(n^{2})$, where $n$ is the greater number of digits.

#### Karatsuba's method
Say we want to find the solution to the multiplication of 
$$P(x) = a_1x + a_0,$$
$$Q(x) = b_1x + b_0,$$
a naive solution will require 4 multiplications
1. $A = a_1 \times b_1$
2. $B = a_1 \times b_0$
3. $C = b_1 \times a_1$
4. $D = b_0 \times a_0$

$$P(x)Q(x) = Ax^2 + (B + C)x + D$$
However we know that 
$$(a_1x + a_0)(b_1x + b_0) = (a_1b_1 + a_1b_0 + a_0b_1 + a_0b_0) = (A + B + C + D),$$
requiring 2 additions (which can be computed easily) and 1 multiplication. Hence to find the final multiplication, we only need three multiplications
1. $E = (A + B + C + D) = (a_1 + a_0) \times (b_1 + b_0)$
2. $A = a_1 \times b_1$
3. $D = b_0 \times a_0$

Hence to find the final solution, we can do 
$$P(x)Q(x) = Ax^2 + (E - A - D)x + D$$

#### Karatsuba's method for integers
If we want to multiply $A$ with $B$, we can represent them as 
$$
\begin{align*}
A &= 10^{\frac{n}{2}}A_1 + A_0 \\\\
B &= 10^{\frac{n}{2}}B_1 + B_0.
\end{align*}
$$

With this, we can apply Karatsuba's method, where
$$
\begin{align*}
  x &= A_1B_1 \\\\
  z &= A_0B_0 \\\\
  y &= (A_1 + A_0)(B_1 + B_0) - x - z \\\\
  AB 
    &= 10^{n}x + 10^{\frac{n}{2}}y + z.
\end{align*}
$$
At each function call, there are 3 multiplications being performed (each of which is multiplied by the same algorithm), where the input size is halved in each recursive call, and the addition runs in $O(n)$. Therefore Karatsuba's method has the recurrence 
$$ T(n) = 3T\left(\frac{n}{2}\right) + O(n) = O(n^{\lg 3}). $$

#### Karatsuba's method for polynomials
We can view this representation of integers similar to that of polynomials. Say we are given polynomials $A(x)$, $B(x)$, where
$$A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$$
$$B(x) = b_nx^n + b_{n-1}x^{n-1} + \ldots + b_0.$$
We have $C(x) = A(x) \cdots B(x)$ of degree $2n$
$$C(x) = \sum_{j = 0}^{2n}c_jx^j = A(x)B(x) = \sum_{j=0}^{2n} \left( \sum_{i + k = j}a_ib_k \right)x^j$$
however, we want to find the coefficients of $c_j = \sum_{i+k=j}a_ib_k$ without performing $(n+1)^2$ many multiplications necessary to get all products of the form $a_ib_k$.

### Fast Fourier Transform
A naive way to multiply two polynomials together would be to multiply each term of the first polynomial with a term of the second polynomial. The runtime of this would be $O(n^2)$, where $n$ is the degree/ number of terms of the polynomials. 

Another method would be to convert both polynomial into it's point value form, multiply those points and convert it back into the polynomial form.
The fast Fourier transform (FFT) is an algorithm to calculate the Discrete Fourier transform (DFT) of a sequence, or the inverse (IDFT) in $O(n \log n)$.

### Polynomial Interpolation (Vandermonde Matrix)z
#### From Coefficient to Value Representation
A polynomial $A(x)$ of degree $n$ is uniquely determined by its values at any $n + 1$ distinct input values $x_0, x_1, \ldots, x_n$:
$$A(x) \leftrightarrow \langle(x_0, A(x_0)), (x_1, A(x_1)), \ldots, (x_n, A(x_n))\rangle$$

For $A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$, these values can be obtained via a matrix multiplication:
$$
\begin{pmatrix}
1      & x_0    & x_0^2  & \ldots & x_0^n  \\\\
1      & x_1    & x_1^2  & \ldots & x_1^n  \\\\
\vdots & \ldots & \ldots & \ddots & \vdots \\\\
1      & x_n    & x_n^2  & \ldots & x_n^n  \\\\
\end{pmatrix}
\begin{pmatrix}
a_0    \\\\
a_1    \\\\
\vdots \\\\
a_n    \\\\
\end{pmatrix} =
\begin{pmatrix}
A(x_0)    \\\\
A(x_1)    \\\\
\vdots    \\\\
A(x_n)    \\\\
\end{pmatrix}
$$
Such a matrix is called the Vandermonde matrix.

#### From Value to Coefficient Representation
It can be shown that if $x_i$ are all distinct, then this matrix is invertible.
Thus, if all $x_i$ are all distinct, given any values $A(x_0), A(x_1), \ldots, A(x_n)$ the coefficients $a_0, a_1, \ldots, a_n$ of the polynomial $A(x)$ are uniquely determined:
$$
\begin{pmatrix}
1      & x_0    & x_0^2  & \ldots & x_0^n  \\\\
1      & x_1    & x_1^2  & \ldots & x_1^n  \\\\
\vdots & \ldots & \ldots & \ddots & \vdots \\\\
1      & x_n    & x_n^2  & \ldots & x_n^n  \\\\
\end{pmatrix}^{-1}
\begin{pmatrix}
A(x_0)    \\\\
A(x_1)    \\\\
\vdots    \\\\
A(x_n)    \\\\
\end{pmatrix} =
\begin{pmatrix}
a_0    \\\\
a_1    \\\\
\vdots \\\\
a_n    \\\\
\end{pmatrix}
$$

#### Complexity of Swapping Representation 
If we fix the inputs $x_0, x_1, \ldots, x_n$ then commuting between a representation of a polynomial $A(x)$ via its coefficients and a representation via its values at these points is done via the following two matrix multiplications, with matrices made from constants, and thus, for fixed inputs $x_0, x_1, \ldots, x_n$, this switch between the two kinds of representations is done in linear time.



#### Discrete Fourier Transform
For $\mathbf{a} = \langle a_0, a_1, \ldots a_{n-1} \rangle$ a sequence of $n$ real or complex numbers, we can form the corresponding polynomial $P_A(x) = \sum_{j=0}^{n-1}a_jx^j$, and evaluate it at all complex roots of unity of order n,
$$\forall \ 0 \leq k \leq n - 1, \quad P_A(\omega_n^k) = A_k = \sum_{j=0}^{n-1}a_j\omega_n^{jk}.$$

The DFT of a sequence $\mathbf{a}$ is a sequence $\mathbf{A}$ of the same length.

#### Inverse Discrete Fourier Transform
The IDFT of a sequence $\mathbf{A} = \langle A_0, A_1, \ldots, A_{n-1} \rangle$ is the sequence of values $\mathbf{a} = \langle a_0, a_1, \ldots, a_{n-1} \rangle = \langle \frac{P_a(1)}{n}, \frac{P_a(\omega_n^{-1})}{n}, \frac{P_a(\omega_n^{-2})}{n}, \ldots, \frac{P_a(\omega_n^{1-n})}{n} \rangle$.

We can show that IDFT(DFT($\mathbf{a}$)) = $\mathbf{a}$ and DFT(IDFT($\mathbf{A}$)) = $\mathbf{A}$.

#### Computation of DFT/ IDFT
Brute force computation of the DFT takes $\Theta(n^2)$, same for IDFT. The DFT of a sequence can be computed in $\Theta(n \lg n)$ using the FFT (as can be the IDFT).


## Proofs
### Proof of Correctness
For Divide and Conquer proof of correctness, mathematical induction is used.

Say that we want to prove that an algorithm $A(n)$ is correct.
1. Prove that the base case is correct, i.e. $A(0)$, $A(1)$ is correct (depends on your base case)
2. Prove induction step
    1. Assume that $A(k)$ is true, where $k < n$ (Inductive hypothesis)
    2. Therefore, recursive calls are correct as they call $A(k)$ where $k < n$, which is true by the inductive hypothesis
    3. Prove that the conquer part of the algorithm is correct
3. Hence algorithm is correct for input size $n$

### Proof of Runtime (Master Theorem)
#### Representation
The time complexity of a divide and conquer algorithm can be represented in the form 
$$T(n) = aT \left( \frac{n}{b} \right) + f(n)$$
where, 
- $T(n)$
  - The divide and conquer algorithm, with an input size of $n$
- $a \geq 1$
  - $a$ is the number of recursive calls per function
  - The constraint implies we make at least 1 recursive call, otherwise there would be no recursion
- $b > 1$
  - $b$ is the decrease in input size per recursive call, the constraint implying that the input size must decrease in each recursive call
  - The constraint implies that the input size must decrease at each recursive call, otherwise we would loop infinitely, never reaching the base case
- $f(n)$
  - The runtime within each function call

#### Runtime cases
Using this information, we can find the runtime of most DAQ algorithms through one of the 3 cases where if $\epsilon > 0$ is a constant, then 
1. If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$
2. If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$
3. If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$

#### Examples

##### Case 1
If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$ \
Work per subproblem is dwarfed by number of subproblems
- Binary tree DFS | $T(n) = 2T \left( \frac{n}{2} \right) + c$
  - Proof:
    - $n^{log_b a} = n^{\lg 2} = n^{1} = n$
    - $f(n) = c = \Theta(1)$
  - Therefore $T(n) = \Theta(n^{\lg 2}) = \Theta(n)$
- Karatsuba's integer multiplication | $T(n) = 3T \left( \frac{n}{2} \right) + n$
  - Proof
    - $n^{\log_2 3} = n^{\lg 3} \approx n^{1.58}$ 
    - $f(n) = n = \Theta(n)$
  - Therefore $T(n) = \Theta(n^{1.58})$

##### Case 2
If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$ \
Work per subproblem is comparable to number of subproblems
- Binary search | $T(n) = T \left( \frac{n}{2} \right) + c$
  - Proof:
    - $n^{\log_b a} = n^{\lg 1} = n^{0} = 1$
    - $f(n) = c = \Theta(1)$
  - Therefore $T(n) = \Theta(n^{\lg 1} \cdot \Theta(1)) = \Theta(\log n)$
- Merge sort | $T(n) = 2T \left( \frac{n}{2} \right) + cn$
  - Proof:
    - $n^{\log_b a} = n^{\lg 2} = n^{1} = n$
    - $f(n) = cn = \Theta(n)$
  - Therefore $T(n) = \Theta(n^{\lg 2} \cdot \Theta(n)) = \Theta(n \log n)$

##### Case 3
If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$ \
Work per subproblem dominates number of subproblems


# Greedy Algorithms
---
Greedy algorithms make the optimal choice at each step to find the overall optimal way to solve the entire problem.

Common examples of Greedy Algorithms include
- Kruksal's algorithm/ Prim's algorithm
- A* path finding algorithm
- Huffman encoding

## More to come

# Dynamic Programming
---
Dynamic programming is a method to find optimal solution to problems, from optimal solutions to subproblem. Because sets of subproblems to solve larger problems overlap, each subproblem can be calculated once, it's solutions are stored for next future calls.

Common examples of Dynamic Programming include
- Dijkstra's algorithm
- 0 - 1 Knapsack
- Optimal matrix chain multiplication

## More to come


# Linear Programming
---
Linear programming is an optimisation method to maximise or minimise a linear function given linear constraints.


## Canonical form
Another representation of a linear programming formulation. Is through matrices



# Reductions
---
Reductions are the conversion of one problem to another. An efficient reduction from A to B may be used to show that B is at least as difficult as A.

## Complexity Classes

### P (Polynomial)
A problem $A(n)$ is in class P, denoted by $A \in \mathbf{P}$, if there is an algorithm which solve it in polynomial time with regard to the size of the input.

### NP (Non deterministic Polynomial Time)
A problem $A(n)$ is in class NP, denoted by $A \in \mathbf{NP}$, if there is an algorithm which can verify if a solution is correct or not in polynomial time with regard to the size of the input.
- A problem that is in P is also in NP.
- Hence, NP problems are at least as hard as P problems.

### NP-Hard and NP-Complete 
A problem is NP-Hard if any problem in NP is reducible to it (informally, a  problem is NP-Hard if it it is at least as hard as the hardest problems in NP).

A problem is NP-Complete if it is in both NP, and NP-Hard (informally, a problem is NP-Complete if it is one of the hardest problems in NP, the "complete" meaning that it is able to simulate any problem in the same complexity class through reductions).
- Hence, NP-Hard problems are at least as hard as NP-Complete.
