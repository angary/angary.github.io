<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>COMP4121</title><meta name="next-head-count" content="3"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#ffffff"/><link rel="preload" href="/_next/static/css/f553854d468f6c8c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f553854d468f6c8c.css" data-n-g=""/><link rel="preload" href="/_next/static/css/33bcc2031ebe3241.css" as="style"/><link rel="stylesheet" href="/_next/static/css/33bcc2031ebe3241.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-0ecb9ccfcb6c9b24.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7db202e315b716c1.js" defer=""></script><script src="/_next/static/chunks/996-502a5e68cb1e9d91.js" defer=""></script><script src="/_next/static/chunks/207-81e7acd450235f3f.js" defer=""></script><script src="/_next/static/chunks/pages/writings/%5Bslug%5D-7e316cc9cfef0534.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_buildManifest.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,n='data-theme',s='setAttribute';var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';d[s](n,'dark')}else{d.style.colorScheme = 'light';d[s](n,'light')}}else if(e){d[s](n,e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="Writing_body__d5Mqz"><div class="Writing_layout__X8ptu"><aside class="Writing_toc__C4kbO" aria-label="Table of contents"><h2 class="Writing_tocTitle__8Z0ek">Contents</h2><ul class="Writing_tocList__2Nnoy"><li class="Writing_tocItem__rjAOC"><a href="#statistics" class="Writing_tocLinkActive__0kigr">Statistics</a></li><li class="Writing_tocItem__rjAOC"><a href="#order-statistics-quickselect" class="Writing_tocLink__ztBHG">Order Statistics (QuickSelect)</a></li><li class="Writing_tocItem__rjAOC"><a href="#database-access" class="Writing_tocLink__ztBHG">Database access</a></li><li class="Writing_tocItem__rjAOC"><a href="#skip-lists" class="Writing_tocLink__ztBHG">Skip Lists</a></li><li class="Writing_tocItem__rjAOC"><a href="#kargers-min-cut-algorithm" class="Writing_tocLink__ztBHG">Karger&#x27;s Min Cut Algorithm</a></li><li class="Writing_tocItem__rjAOC"><a href="#randomised-hashing" class="Writing_tocLink__ztBHG">Randomised Hashing</a></li><li class="Writing_tocItem__rjAOC"><a href="#gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas" class="Writing_tocLink__ztBHG">Gaussian Annulus, Random Projection and Johnson Lindenstrauss Lemmas</a></li><li class="Writing_tocItem__rjAOC"><a href="#page-rank" class="Writing_tocLink__ztBHG">Page Rank</a></li><li class="Writing_tocItem__rjAOC"><a href="#hidden-markov-models-and-the-viterbi-algorithm-and-its-applications" class="Writing_tocLink__ztBHG">Hidden Markov Models and the Viterbi Algorithm and its applications</a></li></ul></aside><aside class="Writing_balanceNav__vSGQh" aria-label="Site navigation"><a class="Writing_siteTitle__O17hB" href="/">Gary Sun // <span class="cn">孫健</span></a><ul class="Writing_siteNavList__zzxAq"><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#about">About</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="https://github.com/angary/">Projects</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#writings">Writings</a></li></ul></aside><div class="Writing_articleHeader__rl9TS"><h1>COMP4121</h1><p class="Writing_description__Rk8wD">Advanced Algorithms</p><p class="Writing_date__BYvoK">26 November 2021</p></div><div class="Writing_articleBody__A1uFB"><span style="display:block"><article><h1 id="statistics"><a href="#statistics">Statistics</a></h1>

<h2 id="expectation-and-variance-of-a-random-variable">Expectation and Variance of a Random Variable</h2>
<p>The <strong>expected value</strong> of a random variable is its mean. Assuming that the expected value converges, the expected value can be calculated as shown below.</p>
<table>
<thead>
<tr>
<th>Discrete random variable</th>
<th>Continuous random variable</th>
</tr>
</thead>
<tbody><tr>
<td>$E(X) = \sum_{i = 1}^{\infty} v_i \cdot p_i$</td>
<td>$E(X) = \int_{-\infty}^{\infty}x \cdot f(x) dx$</td>
</tr>
</tbody></table>
<p>The <strong>variance</strong> of a random variable is defined as $V(X) = E(X - E(X))^2$ assuming that both expectations involved are finite; the standard deviation of a random variable $X$ is given by $\sigma = \sqrt{V(X)}$.</p>
<h2 id="simple-inequalities">Simple inequalities</h2>
<p>$$1 + x \leq e^x \text{ for all } x \in \mathbb{R}$$
$$\left(1 - \frac{1}{n}\right)^n \leq \frac{1}{e} \leq \left(1 - \frac{1}{n} \right)^{n - 1} \text{ for all } n \in \mathbb{N}$$</p>
<h2 id="probability-inequalities">Probability Inequalities</h2>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>The Markov Inequality</strong></td>
<td>Let $X &gt; 0$ be a non-negative random variable. Then for all $t &gt; 0$, $$P(X \geq t) \leq \frac{E(X)}{t}$$</td>
</tr>
<tr>
<td><strong>Chebyshev Inequality</strong></td>
<td>Let $X &gt; 0$ be a random variable with the expected value $\mu = E(X)$ and standard deviation $\sigma = \sqrt{E((X - \mu)^2)}$. Then for all $\lambda &gt; 0$, $$P(| X - \mu | \geq \lambda \sigma) \leq \frac{1}{\lambda^2}$$</td>
</tr>
<tr>
<td><strong>Chernoff Bound</strong></td>
<td>Let $X = \sum_{k = 1}^{n} X_k$, where $X_k, 1 \leq k \leq n$, are independent Bernoulli trials with the probability of success $P(X_k = 1) = p_k$, where $0 &lt; p_k &lt; 1$. Thus, $\mu = E(X) = \sum_{k = 1}^{n} p_k$. Let $\sigma &gt; 0$ be any positive real. Then, $$P(X &gt; (1 + \sigma) \mu) &lt; \left( \frac{e^{\sigma}}{(1 + \sigma)^{1 + \sigma}}\right)^{\mu}$$</td>
</tr>
</tbody></table>
<h1 id="order-statistics-quickselect"><a href="#order-statistics-quickselect">Order Statistics (QuickSelect)</a></h1>

<p><strong>Can we find the $i^{th}$ largest or smallest item in linear time?</strong></p>
<h2 id="non-deterministic-algorithm">Non-Deterministic Algorithm</h2>
<p>In the quicksort algorithm, we note that after a partition, the pivot element ends up being in its correct index. Therefore, we can perform a combination of partitioning and binary search to find the item at the $i^{th}$ index.</p>
<p>For example, we partition first using a random pivot, and as a result we will know the index of our pivot.
If the pivot is index $i$, we&#39;ve found the element.
Else if the pivot is at an index greater than $i$, then we partition the smaller side, else we partition the bigger side and continue until we&#39;ve found the $i^{th}$ element.</p>
<pre><code class="language-py"># Implementation of rand-select

def partition(A: list[int], lo: int, hi: int) -&gt; int:
    # Partition the array A[lo..hi] and return partition index
    i = lo - 1
    pivot = A[hi]
    for j in range(lo, hi):
        if A[j] &lt;= pivot:
            i += 1
            A[i], A[j] = A[j], A[i]
    A[i + 1], A[hi] = A[hi], A[i + 1]
    return i + 1


def rand_select(A: list[int], i: int, lo: int, hi: int) -&gt; int:
    # Return the ith largest index in A
    if lo &lt;= hi:
        pi = partition(A, lo, hi)
        if i &lt; pi:
            return rand_select(A, i, lo, pi - 1)
        return rand_select(A, i, pi + 1, hi)
    return A[hi]
</code></pre>
<h2 id="runtime">Runtime</h2>
<p>The runtime can be represented through the recurrence relation $T(n) = T\left( \frac{n}{2} \right) + O(n)$ which results in a $O(n)$ runtime. However, it can be analysed more in-depth statistically.</p>
<h3 id="worst-case">Worst Case</h3>
<p>The worst case runtime is $\Theta(n^2)$ which happens when the smallest or largest element is picked as the pivot - resulting in an <em>unbalanced partition</em>.</p>
<h3 id="average-case">Average Case</h3>
<p>However (assuming all elements are <strong>distinct</strong>), let us call a partition a <em>balanced partition</em> if the ratio between the smaller side and larger side is less than 9 (9 is arbitrary, any small number &gt; 2 would do).
Then the probability of having a balanced partition is 1 - (chance of choosing smallest 1/10 or biggest 1/10) = 1 - 2/10 = 8/10.</p>
<p>Then let us find the expected number of partitions between two consecutive balanced partitions.
In general, the probability that you need $k$ partitions to end up with another balanced partition is $\left( \frac{2}{10} \right)^{k - 1} \cdot \frac{8}{10}$.</p>
<p>Thus, the expected number of partitions between two balanced partitions is</p>
<p>$$
\begin{align*}
E
  &amp;= 1 \cdot \frac{8}{10} + 2 \cdot \left( \frac{2}{10} \right) \cdot \frac{8}{10} + 3 \cdot \left( \frac{2}{10} \right)^2 \cdot \frac{8}{10} + ... \\
  &amp;= \frac{8}{10} \cdot \sum_{k = 0}^{\infty}(k + 1) \left( \frac{2}{10} \right)^k \\
  &amp;= \frac{8}{10}S.
\end{align*}
$$</p>
<ul>
<li><p>To find $S$, note that</p>
<p>$$\sum_{k = 0}^{\infty} q^k = \frac{1}{1 - q}.$$</p>
<p>By differentiating both sides with respect to $q$ we get</p>
<p>$$\sum_{q = 1}^{\infty} k q^{k - 1} = \frac{1}{(1 - q)^2}.$$</p>
<p>Substituting $q = \frac{2}{10}$ we get that $S = \left(\frac{10}{8} \right)^2$.</p>
</li>
</ul>
<p>Therefore,</p>
<p>$$E = \frac{8}{10} \cdot \left( \frac{8}{10} \right)^2 = \frac{5}{4}.$$</p>
<p>And so there are only 5/4 partitions between two balanced partitions.</p>
<ul>
<li>Note after 1 <em>balanced partition</em>, the size of the array is $\leq 9/10 n$, after the second <em>balanced partition</em>, it is $\leq (9/10)^2n$ and so on.</li>
</ul>
<p>Therefore, the total <strong>average</strong> (expected) run time satisfies</p>
<p>$$
\begin{align*}
T(n)
  &amp;&lt; 5/4n + 5/4 \left( \frac{9}{10} \right)n + 5/4 \left( \frac{9}{10} \right)^2 n + 5/4 \left( \frac{9}{10} \right)^3 n + ... \\
  &amp;= \frac{5/4 n}{1 - \frac{9}{10}} \\
  &amp;= 12.5n.
\end{align*}
$$</p>
<p>Overall, the expected runtime of this algorithm is linear.</p>
<h1 id="database-access"><a href="#database-access">Database access</a></h1>

<p>Assume that $n$ processes want to access a database, and that the time $t$ is discrete.
If two processes simultaneously request access, there is a conflict and all processes are locked out of access.</p>
<p>Assume that processes cannot communicate with each other on when to access.
One possible for a process to determine if it should access the database at time $t$ is to &quot;request access&quot; with probability $p$ and &quot;do not request access&quot; with probability $(1 - p)$.</p>
<p><strong>What should $p$ be to maximise the probability of a successful access to the database for a process at any instant $t$?</strong></p>
<h2 id="efficient-selection-of-p">Efficient selection of p</h2>
<p>The probability of success of process $i$ at any instant $t$ is</p>
<p>$$P(S(i, t)) = p(1 - p)^{n - 1},$$</p>
<p>because a process $i$ requests access with probability $p$ and the probability that no other process has requested access is $(1 - p)^{n - 1}$.</p>
<h3 id="visualization-of-success-probability">Visualization of Success Probability</h3>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-38cfabdfb53946732153ddfb07946ac0.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<p>The graph clearly shows:</p>
<ul>
<li><strong>Higher n</strong> → <strong>Lower optimal p</strong>: As processes increase, optimal request probability decreases</li>
<li><strong>Peak success probability decreases</strong>: More processes lead to lower maximum success rates</li>
<li><strong>Sharper curves for large n</strong>: The optimal region becomes narrower as n increases</li>
</ul>
<p>The extreme points of this is found by solving</p>
<p>$$\frac{d}{dp}P(S(i, t)) = (1 - p)^{n - 1} - p(n - 1)(1 - p)^{n - 2} = 0$$</p>
<p>which gives $p = 1/n$, which gives a probability of success of</p>
<p>$$P(S(i, t)) = p(1 - p)^{n - 1} = \frac{1}{n} \left( 1 - \frac{1}{n} \right)^{n - 1}.$$</p>
<p>However,</p>
<p>$$\lim_{n \rightarrow \infty} \left(1 - \frac{1}{n} \right)^n = e$$</p>
<p>and hence $P(S(i, t)) = \Theta \left( \frac{1}{n} \right)$.
Thus, the probability of failure after $t$ instances is</p>
<p>$$
\begin{align*}
  P(\text{failure after $t$ instants})
    &amp;= \left( 1 - \frac{1}{n} \left( 1 - \frac{1}{n} \right)^{n - 1} \right)^t \\
    &amp;\approx \left( 1 - \frac{1}{e n}\right)^t
\end{align*}
$$</p>
<p>We observe that</p>
<table>
<thead>
<tr>
<th>P(failure after $t = en$ instances)</th>
<th>P(failure after $t = en \cdot 2 \ln n$ instances)</th>
</tr>
</thead>
<tbody><tr>
<td>$$\left( 1 - \frac{1}{en}\right)^{en} \approx \frac{1}{e}$$</td>
<td>$$\left(1 - \frac{1}{en} \right)^{en \cdot 2 \ln n} \approx \frac{1}{n^2}$$</td>
</tr>
</tbody></table>
<p>Thus, a small increase in the number of time instants, from $en$ to $en \cdot 2 \ln n$ caused a dramatic reduction in the probability of failure.</p>
<p>After $en \cdot 2 \ln n$ instances, the probability of failure of each process $\leq 1/n^2$ and since there are $n$ processes, then the probability that at least one process failed is $\leq 1/n$.
Thus after $en \cdot 2 \ln n$ instances all processes succeeded to access the database with probability at least $1 - 1/n$.</p>
<h2 id="comparison-with-centralised-algorithm">Comparison with centralised algorithm</h2>
<p>If the processes could communicate, then it would take $n$ instances for all of them to access the database.</p>
<p>If they can&#39;t communicate, then the above method will allow them to access the database with probability $1 - 1/n$ time, which is larger only by a relatively small factor of $2e \ln n$.</p>
<h1 id="skip-lists"><a href="#skip-lists">Skip Lists</a></h1>

<p>A skip list is a probabilistic data structure that functions similarly to a binary search tree.</p>
<pre><code class="language-py"># Example structure of a skip list

[H]--------------------------[35]----------------[T]
[H]----------------[21]------[35]----------------[T]
[H]------[12]------[21]-[24]-[35]------[55]------[T]
[H]-[02]-[12]-[17]-[21]-[24]-[35]-[43]-[55]-[62]-[T]
</code></pre>
<h2 id="operations">Operations</h2>
<ul>
<li><p><strong>Search of $k$</strong></p>
<ol>
<li>Start from the highest level of head H and go as far right without exceeding $k$</li>
<li>Drop one level down and repeat the procedure using lower level links until you find $k$</li>
</ol>
</li>
<li><p><strong>Insertion of $k$</strong></p>
<ol>
<li>Search for the correct location</li>
<li>Toss a coin until you get a head, and count the number of tails $t$ you got</li>
<li>Insert $k$ and link it at levels $0 - t$ from the bottom up</li>
</ol>
</li>
<li><p><strong>Deletion</strong></p>
<p>Deleting an element is just like in a standard doubly linked list</p>
</li>
</ul>
<h2 id="analysis">Analysis</h2>
<h3 id="expected-levels">Expected Levels</h3>
<p>The probability of getting $i$ consecutive tails when flipping a coin $i$ times is $1/2^i$.
Thus, an $n$ element Skip List has on average $n/2^i$ elements with links on level $i$.
Since an element has links only on levels $0 - i$ with probability $1/2^{i + 1}$, the total <strong>expected</strong> number of link levels per element is</p>
<p>$$\sum_{i = 0}^{\infty} \frac{i + 1}{2^{i + 1}} = \sum_{i = 1}^{\infty} \frac{i}{2^i} = 2.$$</p>
<p>Let $\#(i)$ be the number of elements on level $i$.</p>
<p>Then, $E[\#(i)] = \frac{n}{2^i}$, and by the Markov inequality, the probability of having at least one element at level $i$ satisfies</p>
<p>$$P(\#(i) \geq 1) \leq \frac{E[\#(i)]}{1} = \frac{n}{2^i}.$$</p>
<p>Thus, the probability to have an element on level $2 \log n$ is smaller than $n/2^{2 \log n} = 1/n.$</p>
<p>More generally, the probability to have an element (be nonempty) on level $k \log n$ $&lt; n /2^{k \log n} = 1/n^{k - 1}$.
The expected value $E(k)$ such that $k$ is the lowest integer so that the number of levels is $\leq k \log n$ is</p>
<p>$$E(k) \leq \sum_{k = 1}^{\infty} \frac{k}{n^{k - 1}} = \left( \frac{n}{n - 1} \right)^2.$$</p>
<p>Thus, the expected number of levels is barely larger than $\log n$.</p>
<h3 id="expected-search-per-level">Expected search per level</h3>
<p>Thus, the expected number of elements between any two consecutive elements with a link on level $i + 1$ which have links only up to level $i$ is smaller than</p>
<p>$$\frac{0}{2} + \frac{1}{2^2} + \frac{2}{2^3} + \frac{3}{2^4} + ... = 1.$$</p>
<p>So once on level $i$, on average we will have to inspect only two elements on that level before going to a lower level.</p>
<h3 id="search-time-complexity">Search Time Complexity</h3>
<p>On average, levels $&lt; 2 \log n$, and 2 elements are visited per level.
Therefore, on average, the search will be in time $O(4 \log n) = O(\log n)$.</p>
<h3 id="space-complexity">Space Complexity</h3>
<p>For an element on levels $0 - i$, we store $O(i + 1)$ pointers, and expected number of elements with highest link on level $i$ is $O(n/2^{i + 1})$. Thus, total expected space for is</p>
<p>$$O \left( \sum_{i = 0}^{\infty}2(i + 1) \frac{n}{2^{i + 1}}\right) = O \left( 2n \sum_{i = 0}^{\infty} \frac{i + 1}{2^{i + 1}} \right) = O(4n) = O(n).$$</p>
<h1 id="kargers-min-cut-algorithm"><a href="#kargers-min-cut-algorithm">Karger's Min Cut Algorithm</a></h1>

<p>Given a graph $G = (V, E)$, Karger&#39;s Min Cut algorithm finds a cut $T$ that partitions vertices $V$ into two non empty disjoint subsets $X$ and $Y$, with the lowest capacity of edges which have one edge in $X$ and the other in $Y$.</p>
<p>A deterministic way to solve this is through max flow from one vertex to all other vertices, however this runs in $O(|V|^4)$.</p>
<h2 id="procedure">Procedure</h2>
<h3 id="contraction">Contraction</h3>
<p>The algorithm makes use of contracting edges in a graph.
To contract an edge $e(u, v)$, fuse $u$ and $v$ into a single vertex $[uv]$ and replace edges $e(u, x)$ and $e(v, x)$ with a single edge $e([uv], x)$ of weight $w([uv], x) = w(u, x) + w(v, x)$.
The obtained graph after this is called $G_{uv}$.</p>
<h2 id="claims">Claims</h2>
<p>After collapsing $u$ and $v$ into a single vertex</p>
<ul>
<li>If $u$ and $v$ belong to the <strong>same side of a min cut</strong>, the capacity of the min cut in $G_{uv}$ is the same as that of $G$.</li>
<li>If $u$ and $v$ belong to <strong>opposite sides of a min cut</strong>, the capacity of the min cut in $G_{uv}$ is larger or equal to the capacity of the min cut in $G$.</li>
</ul>
<p>Therefore,</p>
<ul>
<li><p>Let $T_1 = (X_1, Y_1)$ be a min cut in $G_{uv}$</p>
</li>
<li><p>Split $[uv]$ back into $u$ and $v$, but keep them on the same side of the cut $T_1$. This produces a cut $T_2$ in $G$ of the same capacity as the min cut $T_1$ in $G_{uv}$.</p>
<p>Thus, the capacity of the min cut in $G$ can only be smaller than the min cut $T_1$ in $G_{uv}$</p>
</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<ol>
<li>While there are more than 2 vertices<ol>
<li>Pick an edge to with probability proportional to the weight of that edge
$$P(e(u, v)) = \frac{w(u, v)}{\sum_{e(p, q) \in E} w(p, q)}$$</li>
<li>Contract the edge, and remove self loops</li>
</ol>
</li>
<li>Take the capacity of that last edge to be the estimate of the capacity of the min cut in $G$</li>
</ol>
<h2 id="theorems">Theorems</h2>
<p>Let $M(G)$ represent the min cut capacity of $G$.</p>
<h3 id="theorem-1">Theorem 1</h3>
<p>The probability that the capacity of a min cut in $G_{uv}$ is larger than the capacity of a min cut in $G$ is smaller than $2/n$ where $n = |V|$.</p>
<p>$$P(M(G_{uv}) &gt; M(G)) &lt; \frac{2}{n}.$$</p>
<p>This is because this probability is less than or equal to the probability that the edge $e(u, v)$ belonged in the set of edges along the min cut $M$.
The probability of the $e(u, v)$ in $M$ is less than or equal to the final min cut capacity divided by the total capacity of the graph (which is equal to $\frac{n}{2}$), resulting in the final probability of $\frac{2}{n}$.</p>
<h3 id="theorem-2">Theorem 2</h3>
<p>If we run edge contraction until there is 1 edge, then the probability $\pi$ that the capacity of that edge is equal to the capacity of the min cut in G is $\Omega \left( \frac{1}{n^2} \right)$.</p>
<p>From the first theorem</p>
<p>$$
\begin{align*}
  \pi
    &amp;= P(M(G) = M(G_{n-2})) \\
    &amp;= \prod_{i = 1}^{n - 2} P(M(G_i) = M(G_{i - 1})) \\
    &amp;\leq \left(1 - \frac{2}{n} \right) \left(1-\frac{2}{n-1}\right) \left(1-\frac{2}{n-2}\right) ... \left(1-\frac{2}{3} \right) \\
    &amp;= \frac{n - 2}{n} \times \frac{n - 3}{n - 1} \times \frac{n - 4}{n - 2} \times ... \times \frac{1}{3} \\
    &amp;= \frac{2}{n(n-1)}.
\end{align*}
$$</p>
<p>Since the contraction runs in $O(n^2)$, and has a $\Omega \left( \frac{1}{n^2} \right)$ chance of being correct, it needs to be run $\Theta(n^2)$ times, resulting in a final runtime of $O(n^4)$ to find the min cut.</p>
<h2 id="kargers-min-cut-refinement">Karger&#39;s Min Cut Refinement</h2>
<p>The algorithm can be improved to run in $O(n^2 \log^3 n)$ with the high probability of being correct through a divide and conquer approach.</p>
<p>If after running the contraction algorithm until there is $\lfloor \frac{n}{2} \rfloor$ vertices runs in $O(n^2)$ and the chance of being correct is $\approx 1/4$.
Hence, by running the algorithm until there are $\lfloor \frac{n}{2} \rfloor$ vertices 4 times, and then recursing on those smaller graphs, we have a runtime of</p>
<p>$$T(n) = 4T\left(\frac{n}{2}\right) + O\left(n^2\right) = O(n^2 \log n).$$</p>
<pre><code class="language-py"># Python flavoured pseudo code of the refined algorithm

def karger_refined(G: Graph) -&gt; int:
    V, E = G
    # Base case, return the last and only edge weight
    if len(V) == 2:
        return E[V[0], V[1]]

    # Run contraction 4 times and recurse on those 4 new graphs
    min_cuts = [karger_refined(contract(G)) for _ in range(4)]
    return min(min_cuts)
</code></pre>
<p>Let $p(n) = P(\text{success for a graph of size $n$})$, then</p>
<p>$$
\begin{align*}
  p(n)
    &amp;= 1 - P(\text{failure on one branch})^4 \\
    &amp;= 1 - (1 - P(\text{success on one branch}))^4 \\
    &amp;= 1 - \left( 1 - \frac{1}{4}p \left(\frac{n}{2}\right)\right)^4 \\
    &amp;&gt; p\left(\frac{n}{2}\right) - \frac{3}{8}p\left(\frac{n}{2}\right)^2 \\
    &amp;&gt; \frac{1}{log(n)}.
\end{align*}
$$</p>
<p>If we run our algorithm $(\log n)^2$ times, the probability we are correct $\pi$ is</p>
<p>$$\pi = 1 - \left(1 - \frac{1}{\log n} \right)^{(\log n)^2}$$</p>
<p>However, since for all reasonably large $k$, $(1 - 1/k)^k \approx e^{-1}$.
As a result,</p>
<p>$$\pi \approx 1 - e^{-\log n} = 1 - 1/n.$$</p>
<p>Hence, if we run <code>karger_refined</code> $(\log n)^2$ times, we have a probability of being correct $1 - 1/n$, with a runtime of</p>
<p>$$O\left(n^2 \log^3 n \right) \lt\lt O(\left(n^4\right).$$</p>
<h1 id="randomised-hashing"><a href="#randomised-hashing">Randomised Hashing</a></h1>

<p>If the hash function can be analysed, and a sequence of worst keys is chosen, then a lookup in a hash table can take $O(n)$, though ideally we want to have $O(1)$.</p>
<h2 id="universal-hashing">Universal Hashing</h2>
<p>Let $H$ be a finite collection of hash functions that map a given universe $U$ of keys into the smaller range {0 .. m - 1}.
$H$ is said to be <strong>universal</strong> if for each pair of distinct keys $x, y \in U$, the number of hash functions $h \in H$ for which $h(x) = h(y)$ is $\frac{|H|}{m}$.</p>
<p>Assume that a family of hash functions $H$ is universal, and we are hashing $n$ keys into a hash table of size $m$.
Let $C_x$ be the total number of collisions involving key $x$, then the expected value $E[C_x]$ satisifies</p>
<p>$$E[C_x] = \frac{n - 1}{m}.$$</p>
<h3 id="designing-a-universal-family-of-hash-functions">Designing a universal family of hash functions</h3>
<ul>
<li>Choose the size of the hash table $m$ to be a prime number</li>
<li>Let $r$ be such that the size $|U|$ of the universe of all keys satisifies $m^r \leq |U| \leq m^{r+1}$, i.e. $r = \lfloor \log_m |U|\rfloor$</li>
<li>Hence, we can represent each key $x$ in base $m$ where
$$\vec{x} = \langle x_0, x_1, ..., x_r\rangle \text{ and } x = \sum_{i=0}^{r}x_i m^i$$</li>
<li>Let $\vec{a} = \langle a_0, a_1, ..., a_r \rangle$ be a vector of $r + 1$ <strong>randomly chosen</strong> elements from the set $\{0, 1, ..., m - 1 \}$</li>
<li>Define a corresponding hash function $h_{\vec{a}}(x) = \left( \sum_{i=0}^{r} x_i a_i \right) \mod m$</li>
</ul>
<h3 id="proving-universality">Proving universality</h3>
<p>Assume $x, y$ are two distinct keys. Then for there to be a hash collision,</p>
<p>$$
\begin{align*}
h_{\vec{a}}(x) = h_{\vec{a}}(y)
    &amp;\Leftrightarrow \sum_{i=0}^{r}x_ia_i = \sum_{i=0}^{r}y_ia_i \mod m \\
    &amp;\Leftrightarrow \sum_{i=0}^{r}(x_i - y_ia_i) = 0 \mod m
\end{align*}
$$</p>
<p>Since $x \neq y$ there exits $0 \leq k \leq r$ such that $x_k \neq y_k$.
Assume that $x_0 \neq y_0$, then</p>
<p>$$(x_0 - y_0)a_0 = - \sum_{i=1}^{r}(x_i - y_i)a_i \mod m$$</p>
<p>Since $m$ is a prime, every non-zero element $z \in \{0, 1, ..., m - 1\}$ has a multiplicative inverse $z^{-1}$, such that $z \cdot z^{-1} = 1 \mod m$ and so</p>
<p>$$a_0 = \left( - \sum_{i=0}^{r} (x_i - y_i)a_i \right) (x_0 - y_0)^{-1} \mod m$$</p>
<p>this implies that</p>
<ol>
<li>for any 2 keys $x, y$ such that $x_0 \neq y_0$ and</li>
<li>for any randomly chosen $r$ numbers $a_1, a_2, ..., a_r$</li>
</ol>
<p>there exists <strong>exactly one</strong> $a_0$ such that $h_{\vec{a}}(x) = h_{\vec{a}}(y)$.</p>
<p>Since there are</p>
<ul>
<li>$m^r$ sequences of the form $\langle a_{1}, ..., a_r \rangle$</li>
<li>$m^{r+1}$ sequences of the form $\langle a_0, a_1, ..., a_r \rangle$</li>
</ul>
<p>as a result, the probability to choose a sequence $\vec{a}$ such that $h_{\vec{a}}(x) = h_{\vec{a}}(y)$ is equal to</p>
<p>$$\frac{m^r}{m^{r+1}} = \frac{1}{m}.$$</p>
<p>Thus, the family $H$ is a universal collection of hash functions.</p>
<h3 id="using-universal-family-of-hash-functions">Using universal family of hash functions</h3>
<ol>
<li>Pick $r = \lfloor \log_m |U| \rfloor$ ,so that $m^r \leq |U| \leq m^{r+1}$</li>
<li>For each run, pick a hash function by randomly picking a vector $\vec{a} = \langle a_0, a_1, ..., a_r \rangle$ such that $0 \leq a_i \leq m$, for all $i$, $0 \leq i \leq r$.</li>
<li>During each run use that function on all keys</li>
</ol>
<h2 id="designing-a-perfect-hash-table">Designing a Perfect Hash table</h2>
<h3 id="first-step">First step</h3>
<p><strong>Given $n$ keys we will be constructing hash tables for size $m &lt; 2n^2$ using universal hashing.
The probability that such a table is collision free will be $&gt; 1/2$</strong></p>
<ol>
<li>Pick the smallest prime $m$, such that $n^2 &lt; m &lt; 2n^2$</li>
<li>Pick a random vector $\vec{a}$ and hash all keys using the corresponding hash function $h_{\vec{a}}$</li>
</ol>
<p>Given $n$ keys, there will be $n \choose 2$ pairs of keys.
By universality of the family of hash functions used, for each pair of keys the probability of collision is $1/m$.
Since $m \geq n^2$ we have $\frac{1}{m} \leq \frac{1}{n^2}$.
Thus, the expected total number of collisions in the table is at most</p>
<p>$${n \choose 2} \frac{1}{m} \leq \frac{n(n - 1)}{2} \frac{1}{n^2} &lt; \frac{1}{2}.$$</p>
<p>Let $X$ be the random variable equal to no. collisions. Then by the Markov Inequality with $t=1$ we get</p>
<p>$$P\{X \geq 1\} \leq \frac{E[X]}{1} &lt; \frac{1}{2}.$$</p>
<p>Thus, the chance of a collision after $k$ hashes $&lt; (1/2)^k$, which rapidly tends to 0.
Consequently, after a few random trial-and-error attempts we will obtain a collision free hash table of size $&lt; 2n^2$.</p>
<p>If $p$ is the probability of a collision, then the expected number of trials $E[N]$ before we hit a collision free hash table of size $2n^2$ is</p>
<p>$$
\begin{align*}
E[N]
  &amp;= 1(1 - p) + 2p(1 - p) + 3p^2(1 - p) + ... \\
  &amp;= (1 - p)(1 + 2p + 3p^2 + ...) \\<br>  &amp;= \frac{1}{1 - p} \\
  &amp;&lt; 2.
\end{align*}
$$</p>
<h3 id="second-step">Second step</h3>
<p><strong>Choose $M$ to be the smallest prime number $&gt; n$</strong></p>
<p>Thus, $n \leq m \leq 2n$.
Produce a hash table of size $M$ again by choosing randomly from a universal family of hash funtions.
Assume that a slot $i$ of this table has $n_i$ many elements.
Hash these $n_i$ elements into a secondary hash table of size $m_i &lt; 2n_i^2$.
We have to guarantee that the sum total of sizes of all secondary hash tables, i.e., $\sum_{i=1}^{M}m_i$ is linear in $n$.
Note ${n_i \choose 2}$ is the number of collisions at $n_i$ and</p>
<p>$${n_i \choose 2} = \frac{n_i(n_i - 1)}{2} = \frac{n_i^2}{2} - \frac{n_i}{2}.$$</p>
<p>Therefore the total number of collisions in the hash table is $\sum_{i=1}^{M} {n_i \choose 2}$, and since the expected value of collision with universal hashing is $1/M$,</p>
<p>$$
\begin{align*}
  E\left[ \sum_{i=1}^{M} {n_i \choose 2} \right]
    &amp;= {n \choose 2}\frac{1}{M} \\
    &amp;= \frac{n(n - 1)}{2M} \\
  E\left[ \sum_{i=1}^{M} n_i^2 \right]
    &amp;= 2E\left[ \sum_{i=1}^{M} {n_i \choose 2} \right] + n
\end{align*}
$$</p>
<p>Thus,</p>
<p>$$E\left[ \sum_{i=1}^{M} n^2_i \right] \leq \frac{n(n-1)}{n} + n = 2n - 1 &lt; 2n.$$</p>
<p>Applying the Markov Inequality to find the probability we have more than $4n$ items in our hash table, we obtain</p>
<p>$$P \left\{ \sum_{i=1}^{M} n_i^2 &gt; 4n \right\} \leq \frac{E\left[ \sum_{i=1}^{M} n_i^2 \right]}{4n} &lt; \frac{2n}{4n} = \frac{1}{2}$$</p>
<p>Thus, after a few attempts we will produce a hash table of size $M &lt; 2n$ for which $\sum_{i=1}^{M} &lt; 4n$, and if we choose primes $m_i &lt; 2n_i^2$ then $\sum_{i=1}^{M} m_i &lt; 8n$.
In this way the size of the primary hash table plus the sizes of all secondary hash tables satisfies</p>
<p>$M + \sum_{i=1}^{M}m_i &lt; 2n + 8n = 10n.$</p>
<h1 id="gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas"><a href="#gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas">Gaussian Annulus, Random Projection and Johnson Lindenstrauss Lemmas</a></h1>

<h2 id="generating-random-points-in-d-dimensional-spaces">Generating random points in d-dimensional spaces</h2>
<p>Let us consider a Gaussian random variable $X$ with a zero mean ($E[X] = \mu = 0$) and variance $V[X] = v = 1/2\pi$, then its density is given by</p>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-8892ba7c776356e02f2d0a849330e257.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<p>$$f_X(x) = \frac{1}{\sqrt{2\pi v}}e^{-\frac{x^2}{2v}} = e^{-\pi x^2}$$</p>
<p>Assume that we use such $X$ to generate independently the coordinates $\langle x_1, ..., x_d \rangle$ of a random vector $\vec{x}$ from $\mathbb{R}^d$.
Then $E[X^2] = E[(X - E[X])^2] = V[X]$, and $E\left[ \frac{x_1^2 + ... + x_d^2}{d} \right] = dV[X]/d = V[X]$.</p>
<p>As a result, given the length $|x| = \sqrt{x_1^2 + ... + x_d^2}$, the expected value of the square of a the length of $\vec{x}$ is $E[|x|^2] = d/2\pi$. So, on average, $|x| \approx \frac{\sqrt{d}}{\sqrt{2\pi}} = \Theta(\sqrt{d})$, and if $d$ is large, then this is true with a high probability.</p>
<p>If we choose 2 points independently, then</p>
<p>$$E[\langle x, y \rangle] = E[x_1 y_1 + ... + x_d y_d] = d E[XY] = d E[X] E[Y] = 0.$$</p>
<p>Hence, the expected value of the scalar product of any 2 vector with randomly chosen coordinates is zero.</p>
<p>Hence, vectors with randomly chosen coordinates:</p>
<ul>
<li><strong>have approximate the same length $\Theta(\sqrt{d})$</strong></li>
<li><strong>any 2 such vectors are likely to be almost orthogonal</strong></li>
</ul>
<h2 id="higher-dimensional-balls">Higher Dimensional Balls</h2>
<p>Most of the volume of a high dimensional ball is near:</p>
<ul>
<li>Any of its equators (between two close parallel hyper-planes symmetric with respect to the center).<ul>
<li>(Insert lots of math).
The ratio between a slice of the sphere $A$ that lies above the hyperplane $x_1 = \frac{c}{\sqrt{d - 1}}$ for some constant $c$, and the whole hemisphere satisifies
$$
  \frac{A}{H} &lt; \frac{
V(d - 1)\frac{
  e^{\frac{-c^2}{2}}
}{
  c \sqrt{d - 1}
}
  }{
V(d - 1) \frac{
  1
} {
  2 \sqrt{d - 1}
}
  }
  = \frac{2}{c}e^{-\frac{c^2}{2}}
$$</li>
</ul>
</li>
<li>It&#39;s surface (if we have a ball of radius $r$, and another smaller, of radius $r(1 - \epsilon)$, most of the volume of the bigger ball is in the annulus outside the smaller ball).<ul>
<li>This is because the area of the smaller ball is $(1 - \epsilon)^d$ and $(1 - \epsilon) &lt; 1$.
Hence, for large values of $d$ this tends to 0 quickly.</li>
</ul>
</li>
</ul>
<h2 id="johnson-lindenstrauss-lemma">Johnson-Lindenstrauss Lemma</h2>
<p>For any $\epsilon$, $0 &lt; \epsilon &lt; 1$, and any integer $n$, assume that $k$ satisfies $k &gt; \frac{3}{\gamma \epsilon^2} \ln n$.
Then for any set of $n$ points given by the vectors $v_1, ..., v_n$ in $\mathbb{R}^d$, with the probability of at least $1 - 3/(2n)$, the random projection $f&#39; \mathbb{R}^d \rightarrow \mathbb{R}^k$ has the property that for ALL pairs of points $v_i, v_j$</p>
<p>$$|| f&#39;(v_i - v_j)| - |v_i - v_j|| \leq \epsilon | v_i - v_j|.$$</p>
<p>Thus, $f&#39;(v)$ &quot;almost&quot; preserves distances between points given by vectors $v_i$ despite reduction of dimensionality from $d &gt;&gt; k$ to only $k$.</p>
<h2 id="application-of-johnson-lindenstrauss-lemma">Application of Johnson-Lindenstrauss Lemma</h2>
<ul>
<li>Choose $k$ random vectors by choosing each coordinate of every vector using a unit variance Gaussian</li>
<li>Pre-process data by projecting to $k &lt; &lt; d$ dimensional subspace spanned by the $k$ random vectors</li>
<li>For comparing new data with current data, map the new vector $y \in \mathbb{R}^d$ with its projection $f&#39;(y) = f(y) / \sqrt{k}$.</li>
<li>Then nearest neighbours of $f&#39;(y)$ can be used in hte projected $k$ dimensional space instead of dimension $d &gt; &gt; k$.</li>
</ul>
<h1 id="page-rank"><a href="#page-rank">Page Rank</a></h1>

<p>The Page Rank algorithm aims to solve the problem of how to order webpages.</p>
<h2 id="setup">Setup</h2>
<p>Consider all the webpages $P_i$ on the entire internet as vertices of a directed graph, where a directed edge $P_i \rightarrow P_j$ exists if $P_i$ has a link to $P_j$.</p>
<p>Notation:</p>
<ul>
<li>$\rho(P)$ = the rank of a page (to be assigned)</li>
<li>$\#(P)$ = the number of outgoing links on a web page</li>
</ul>
<p>A web page $P$ should have a high rank only if it is pointed at by many pages $P_i$ which:</p>
<ol>
<li>themselves have a high rank $\rho(P_i)$</li>
<li>and do not point to an excessive number of other web pages, i.e. $\#P(_i)$ is reasonably small.</li>
</ol>
<p>So we want the following system of equations to be satisfied:</p>
<p>$$\left\{\rho(P) = \sum_{P_i \rightarrow P} \frac{\rho(P_i)}{\#(P_i)} \right\}_{P \in WWW}$$</p>
<p>We have a large sparse matrix $G_1$ of size $M \times M$, where $M = |WWW|$</p>
<p>$$
G_1 =
\begin{pmatrix}
    g(1, 1) &amp; \ldots &amp; g(1, j) &amp; \ldots &amp; g(1, M) \\
    \vdots  &amp; \ddots &amp; \vdots  &amp; \ddots &amp; \vdots  \\
    g(i, 1) &amp; \ldots &amp; g(i, j) &amp; \ldots &amp; g(i, M) \\
    \vdots  &amp; \ddots &amp; \vdots  &amp; \ddots &amp; \vdots  \\
    g(M, 1) &amp; \ldots &amp; g(M, j) &amp; \ldots &amp; g(M, M)
\end{pmatrix}
\
$$</p>
<p>$$
g(i, j) = <br>\begin{cases}
    \frac{1}{\#(P_i)} &amp; \text{ if } P_i \rightarrow P_j \\
    0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>However, because $G_1$ is mostly a sparse matrix, it resembles</p>
<p>$$
G_1 =
\begin{pmatrix}
         &amp;   &amp; \vdots      &amp;   &amp;        &amp;   &amp; \vdots      &amp;   &amp;        \\
         &amp;   &amp; \vdots      &amp;   &amp;        &amp;   &amp; \vdots      &amp;   &amp;        \\
  \ldots &amp; 0 &amp; \frac{1}{k} &amp; 0 &amp; \ldots &amp; 0 &amp; \frac{1}{k} &amp; 0 &amp; \ldots \\
         &amp;   &amp; \vdots      &amp;   &amp;        &amp;   &amp; \vdots      &amp;   &amp;        \\
         &amp;   &amp; \vdots      &amp;   &amp;        &amp;   &amp; \vdots      &amp;   &amp;        \\
\end{pmatrix}
\
$$</p>
<p>where $k$ is equal to $\#(P_i)$, the number of pages which the page $P_i$ has links to.
Hence, the system of linear equations can be represented as</p>
<p>$$\mathbf{r}^\intercal = \mathbf{r}^\intercal G_1$$</p>
<p>where</p>
<p>$$\mathbf{r}^\intercal = (\rho(P_1), \rho(P_2), ..., \rho(P_M)).$$</p>
<p>Note that $G_1$ is a markov matrix, and $\mathbf{r}^\intercal$ is a left-hand eigenvector of $G_1$ corresponding to the eigenvalue 1.
Thus, finding ranks of web pages is reduced to finding eigenvectors of $G_1$, which corresponds to the eigenvalue 1.</p>
<p>If we model a random walk on the graph of this matrix, there are 2 issues:</p>
<ol>
<li><p>What should we do when we get to a webpage with no outgoing links?</p>
<p><strong>Solution:</strong>
Jump to a randomly chosen webpage when a node with no outgoing links.</p>
<p>I.e. the first row in $G_1$ is a dangling page, with no outgoing pages.
$G_2$ fixes this by making it point to all pages with equal probability.
Such a matrix is <strong>row stochastic</strong>, meaning that each row sums up to 1.</p>
<p>$$
 G_1 =
 \begin{pmatrix}
      &amp;   &amp; \vdots             &amp;   &amp;        &amp;   &amp; \vdots             &amp;   &amp;        \\
   \ldots &amp; 0 &amp; 0                  &amp; 0 &amp; \ldots &amp; 0 &amp; 0                  &amp; 0 &amp; \ldots \\
      &amp;   &amp; \vdots             &amp;   &amp;        &amp;   &amp; \vdots             &amp;   &amp;        \\
   \ldots &amp; 0 &amp; \frac{1}{\#(P_i)} &amp; 0 &amp; \ldots &amp; 0 &amp; \frac{1}{\#(P_i)} &amp; 0 &amp; \ldots \\
      &amp;   &amp; \vdots             &amp;   &amp;        &amp;   &amp; \vdots             &amp;   &amp;        \\
 \end{pmatrix}
 \\
 \\
 G_2 =
 \begin{pmatrix}
      &amp;             &amp; \vdots             &amp;             &amp;         &amp;             &amp; \vdots             &amp;             &amp;        \\
   \ldots &amp; \frac{1}{M} &amp; \frac{1}{M}        &amp; \frac{1}{M} &amp; \ldots  &amp; \frac{1}{M} &amp; \frac{1}{M}        &amp; \frac{1}{M} &amp; \ldots \\
      &amp;             &amp; \vdots             &amp;             &amp;         &amp;             &amp; \vdots             &amp;             &amp;        \\
   \ldots &amp; 0           &amp; \frac{1}{\#(P_i)} &amp; 0           &amp; \ldots  &amp; 0           &amp; \frac{1}{\#(P_i)} &amp; 0           &amp; \ldots \\
      &amp;             &amp; \vdots             &amp;             &amp;         &amp;             &amp; \vdots             &amp;             &amp;        \\
 \end{pmatrix}
 \
$$</p>
</li>
<li><p>If $T$ is the walk length, then as $T \rightarrow \infty$, the values $N(P)/T$ should converge.
This becomes an issue if there are disconnected graphs or if the graph is bipartite (and the probability of being in one side depends if the path length is odd or even).</p>
<p><strong>Solution:</strong>
Randomly jump to a new page after some time.</p>
<p>This transformation does not change the rows corresponding to dangling webpages: $\alpha / M + (1 - \alpha) / M = 1/M$</p>
<p>$$
 G =
 \begin{pmatrix}
      &amp;                      &amp; \vdots                                    &amp;                      &amp;         &amp;                      &amp; \vdots                                    &amp;                      &amp;        \\
   \ldots &amp; \frac{1}{M}          &amp; \frac{1}{M}                               &amp; \frac{1}{M}          &amp; \ldots  &amp; \frac{1}{M}          &amp; \frac{1}{M}                               &amp; \frac{1}{M}          &amp; \ldots \\
      &amp;                      &amp; \vdots                                    &amp;                      &amp;         &amp;                      &amp; \vdots                                    &amp;                      &amp;        \\
   \ldots &amp; \frac{1 - \alpha}{M} &amp; \frac{\alpha}{\#(P_i)} + \frac{1 - \alpha}{M} &amp; \frac{1 - \alpha}{M} &amp; \ldots  &amp; \frac{1 - \alpha}{M} &amp; \frac{\alpha}{\#(P_i)} + \frac{1 - \alpha}{M} &amp; \frac{1 - \alpha}{M} &amp; \ldots \\
      &amp;                      &amp; \vdots                                    &amp;                      &amp;         &amp;                      &amp; \vdots                                    &amp;                      &amp;        \\
 \end{pmatrix}
 \
$$</p>
</li>
</ol>
<p>These two solutions allow the ranks to converge, because the model works like a well behaved Markov Chain.</p>
<p>To implement the first solution:</p>
<p>Let $d$ be 1 at positions $i$ which correspond to dangling webpages and 0 elsewhere</p>
<p>$$\mathbf{d}^\intercal = (0 \ ... \ 0 \ 1 \ 0 \ ... \ 0 \ 1 \ 0 \ ... \ 0 \ 1 \ 0 \ ... \ 0)$$</p>
<p>Let $\mathbf{e}$ be 1 everywhere: $\mathbf{e}^\intercal = (1 \ 1 \ ... \ 1)$.</p>
<p>Then
$$G_2 = G_1 + \frac{1}{M} \mathbf{d} \mathbf{e}^\intercal$$</p>
<p>To implement the second solution as well, we get $G$ as:</p>
<p>$$G = \alpha G_2 + \frac{1 - \alpha}{M} \mathbf{e} \mathbf{e}^\intercal = \alpha \left( G_1 + \frac{1}{M} \mathbf{d} \mathbf{e}^\intercal \right) + \frac{1 - \alpha}{M} \mathbf{e} \mathbf{e}^\intercal$$</p>
<p>$G$ is no longer sparse, but the vector matrix product $\mathbf{x}^\intercal G$ for vectors $\mathbf{x}$ whose coordinates sum up to 1 can be decomposed as:</p>
<p>$$\mathbf{x}^\intercal = \alpha \mathbf{x}^\intercal G_1 + \frac{1}{M} \left(1 - \alpha (1 - \mathbf{x}^\intercal \mathbf{d}) \right) \mathbf{e}^\intercal$$</p>
<h2 id="markov-chains-discrete-time-markov-processes">Markov Chains (Discrete Time Markov Processes)</h2>
<p>A (finite) Markov Chain is given by a finite set of states $S = \{P_i\}_{i \leq M}$ and by a row stochastic matrix $G$.
The process can start at $t = 0$ in any of its states and continue its evolution by going at every discrete moment of time from its present state to another randomly chosen state.</p>
<h3 id="simple-example-weather-model">Simple Example: Weather Model</h3>
<p>Consider a simple 3-state Markov chain representing weather conditions:</p>
<p><strong>State Transition Diagram:</strong></p>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-58ee9b9d922a50f63d8453209ad43b76.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<p><strong>Transition Matrix G:</strong></p>
<p>$$
G = \begin{pmatrix}
0.7 &amp; 0.2 &amp; 0.1 \\
0.3 &amp; 0.4 &amp; 0.3 \\
0.2 &amp; 0.3 &amp; 0.5
\end{pmatrix}
$$</p>
<p>Each row sums to 1 (row stochastic property), and $G_{ij}$ represents the probability of transitioning from state $i$ to state $j$.</p>
<p>The model of the random walk on the graph is an example of a Markov Chain:</p>
<ul>
<li>States are &quot;being at a webpage $P_i$&quot;, so we have in total $M$ states, one for each web page $P_i$, $1 \leq i \leq M$.</li>
<li>We start from a randomly chosen starting web page.</li>
<li>Thus, $q^{(0)}(i) = \frac{1}{M}$ for all $i$, because all pages are equally likely to be the starting page.</li>
<li>If a webpage $P_i$ is not a dangling webpage, follow a link on the page $P_i$ which points to another webpage $P_j$ with prob $\frac{\alpha}{\#(P_i)}$, or jump directly to a page $P_j$ with probability $\frac{1 - \alpha}{M}$.<ul>
<li>Hence if a page $P_i$ does not point to a page $P_j$ then $g_{i, j} = \frac{1 - \alpha}{M}$.</li>
</ul>
</li>
<li>If $P_i$ is a dangling page then $g_{i, j} = \frac{1}{M}$ for every page $P_j$ (including the same page $P_i$).</li>
</ul>
<h3 id="general-markov-chains">General Markov Chains</h3>
<ul>
<li>The Google matrix induces a strongly connected graph (as there is a directed edge between any two vertices), and so is <strong>irreducible</strong>.</li>
<li>The Google matrix is <strong>aperiodic</strong><ul>
<li>A state $P_i$ in a Markov chain is periodic if there exists integer $K &gt; 1$ s.t. all loops in its underlying graph which contain vertex $P_i$ have length divisible by $K$.</li>
<li>Markov chains which do not have any periodic states are called aperiodic</li>
</ul>
</li>
<li>For any finite, irreducible and aperiodic Markov chain has the following properties:<ol>
<li>For every initial distribution of states $\mathbf{q}^{(0)}$ the value of $\mathbf{q}^{(t)} = \mathbf{q}^{(0)}G^t$ converges as $t \rightarrow \infty$ to a unique stationary distribution $\mathbf{q}$, i.e., converges to a unique distribution $\mathbf{q}$ which is independent of $\mathbf{q}^{(0)}$ and satisfies $\mathbf{q} = \mathbf{q}G$.</li>
<li>Note: in the above $\mathbf{q}$ is a row vector, to avoid having to always transpose it.</li>
<li>Let $N(P_i, T)$ be the number of times the system has ben in state $P_i$ during $T$ many transitions of such a Markov chain, then
$$\lim_{T \rightarrow \infty} \frac{N(P_i, T)}{T}= \mathbf{q_i}$$</li>
</ol>
</li>
</ul>
<h3 id="application-to-pagerank">Application to PageRank</h3>
<p>The general theorem on Markov chains implies that:</p>
<ul>
<li>1 is the left eigenvalue of $G$ of the largest absolute value, and the <em>stationary distribution</em> $\mathbf{q}$ is the corresponding left hand side eigenvector, $\mathbf{q}^\intercal = \mathbf{q}^\intercal G$.</li>
<li>$\mathbf{q}$ is unique, i.e., if $\mathbf{q}_1^\intercal = \mathbf{q}_1^\intercal G$, then $\mathbf{q}_1 = \mathbf{q}$</li>
<li>Distribution $\mathbf{q}$ can be obtained by starting with an arbitrary initial probability distribution $\mathbf{q}_0$ and obtain $\mathbf{q}$ as $\lim_{k \rightarrow \infty} \mathbf{q}_0^\intercal G^k$</li>
<li>An approximation $\mathbf{\tilde{q}}$ of $\mathbf{q}$ can be obtained by taking $\mathbf{q}_o^\intercal = (1/M, 1/M, ..., 1/M)$ and a sufficiently large $K$ and computing $\mathbf{\tilde{q}} = \mathbf{q}_0 G^k$ iteratively</li>
<li>The $i^{th}$ coordinate of such obtained distribution $\mathbf{q}^\intercal = (q_1, ..., q_i, ..., q_M)$ roughly gives the ratio $N(P_i, T)/T$ where $N(P_i, T)$ is the number of times $P_i$ has been visited during a surfing session of length $T$.</li>
</ul>
<h3 id="finding-alpha">Finding alpha</h3>
<p>How close to 1 should $\alpha$ be?</p>
<p>The <em>expected</em> length $lh$ of surfing between two teleportations can be found as:</p>
<p>$$
\begin{align*}
E(lh)
  &amp;= 0(1 - \alpha) + \alpha(1 - \alpha) + ... + k \alpha^k (1 - \alpha) + ... \\
  &amp;= \alpha(1 - \alpha)(1 + 2 \alpha + ... + k \alpha^{k - 1} + ...) \\
  &amp;= \frac{\alpha}{1 - \alpha}.
\end{align*}
$$</p>
<p>Google uses $\alpha = 0.85$ (obtained empirically), giving an expected surfing length $\frac{0.85}{1- 0.85}\approx 5.7$, very close to 6 (coming from the idea of <em>six degrees of separation</em>), and it takes approx $50 - 100$ iterations for convergence.</p>
<p>Larger values produce more accurate representation of &quot;importance&quot; of a webpage, but the convergence slows down fast.</p>
<p>Error of approximation of $\mathbf{q}$ by $\mathbf{q_0}G^k$ decreases approximately as $\alpha^k$.
More importantly, increasing $\alpha$ increases the sensitivity of the resulting PageRank.
This is not effective as internet content and structure changes on a daily basis, but PageRank should change slowly.</p>
<h1 id="hidden-markov-models-and-the-viterbi-algorithm-and-its-applications"><a href="#hidden-markov-models-and-the-viterbi-algorithm-and-its-applications">Hidden Markov Models and the Viterbi Algorithm and its applications</a></h1>

<p>A Hidden Markov Model is a Markov Model that has two states:</p>
<ul>
<li>observations</li>
<li>hidden states</li>
</ul>
<h3 id="hidden-markov-model-structure">Hidden Markov Model Structure</h3>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-db1f7aa9bab5af519444d5807e669af8.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<p>The diagram shows:</p>
<ul>
<li><strong>$\textbf{S}$</strong>: Hidden states that follow Markov transitions</li>
<li><strong>$\textbf{O}$</strong>: Observable outcomes/emissions  </li>
<li><strong>Solid arrows</strong>: State transition probabilities</li>
<li><strong>Dashed arrows</strong>: Emission probabilities from hidden states to observations</li>
</ul>
<p>An example of it&#39;s application, is given a sequence of manifestations, how can we determine what sequence of states caused it?</p>
<p>We do it in a way that maximises the likelihood that we are correct which is what the <strong>Viterbi algorithm</strong> does. It is a dynamic programming algorithm that produces the most likely estimate.</p>
<h2 id="hidden-markov-models">Hidden Markov Models</h2>
<ul>
<li>If we are given a finite Markov chain, consisting of a set of its states $S = \{s_1, s_2, ..., s_K \}$.</li>
<li>We are also given the initial probabilities $\pi_1, \pi_2, ..., \pi_K$ of states.</li>
<li>However, we have no access to the direct states, we only have the set of observables $O = \{o_1, o_2, ..., o_N \}$ which are the possible manifestations of the states $S = \{s_1, s_2, ..., s_K \}$.</li>
<li>We are also given the emission matrix $E$ of size $K \times N$ where entry $e(i, k)$ represents the probability that when the chain is in state $s_i$, the observable outcome will be $o_k$.</li>
</ul>
<!-- ```py
  [a] -> [b] -> [c] -> [d]
   ↓      ↓      ↓      ↓
  [1]    [2]    [3]    [4]
  / \
 l   r
``` -->

<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<p>Likelihood is, in a sense, an inverse of probability.</p>
<ul>
<li>Assume there are $n$ balls labeled 1 to $n$, and you can draw a single ball, look at its value, and have to estimate the value of $n$.</li>
<li>Assume you drew the ball numbered $k$, which has a probability of $1 / n$. Therefore you have the highest chance of picking $k$ when $n$ is as small as possible, meaning it $n = k$ (as you know there are at least $k$ balls).</li>
<li>In this case the MLE estimator is $N(X) = X$, and $E(X)$ is given by
$$\mu = \sum_{i=1}^{n}\left( i \times \frac{1}{n} \right) = \frac{n(n + 1)}{2n} = \frac{n + 1}{2}.$$
Thus, this is biased, because the expected value is half of $n$.</li>
<li>If we ues the estimator $Y(X) = 2X - 1$, then the expected value of $Y$ is
$$\sum_{i = 1}^{n} \frac{2i - 1}{n} = \frac{2\sum_{i=1}^{n}i}{n} - \frac{\sum_{i=1}^n 1}{n} = \frac{2n(n+1)}{2n} - 1 = n$$
and this is unbiased.</li>
<li>As the size of the sample increases, ML estimate approaches the best possible estimate.</li>
</ul>
<h2 id="viterbi-algorithm">Viterbi Algorithm</h2>
<p>Assume we are given a sequence of observable outcomes $(y_1, y_2, y_r)$.
The goal is to find the sequence $(x_1, x_2, ..., x_r)$ of states of the Markov chain for which the likelihood that such a sequence is the cause of the observed sequence of outcomes is the highest.</p>
<ul>
<li>Given $\vec{y}$ observed, we could pick the sequence of states $\vec{x}$ for which the value of $P(\vec{x}, \vec{y})$ is the largest.</li>
<li>This is not feasible, as if the total number of states is $K$ and the observed sequence is of length $T$ then there are $K^T$ sequences to try.</li>
<li>Instead the Viterbi algorithm solves this problem in $O(T \times K^2)$ using dynamic programming.</li>
</ul>
<h3 id="algorithm-1">Algorithm</h3>
<p>We solve all subproblems $S(i, k)$ for every $i \leq i \leq T$ and every $1 \leq k \leq K$:</p>
<p><strong>Subproblem</strong> $S(i, k)$: Find the sequence of states $(x_1, ..., x_i)$ such that $x_i = s_k$ and such that the probability of observing the outcome $(y_1, ..., y_i)$ is maximal.</p>
<p>A <a href="https://www.youtube.com/watch?v=kqSzLo9fenk">good intro to Hidden Markov Models and the Viterbi Algorithm</a>.</p>
<h1 id="recommender-systems">Recommender Systems</h1>
<p>The main purpose of recommender systems is to recommend content / products to users that they may enjoy.</p>
<p>Two major kinds of recommender systems:</p>
<ul>
<li><strong>Content based:</strong> items are recommended by their intrinsic similarity<ul>
<li>This suffers from the problem that content usually has to be done by humans because content is a semantic notion (which computers do not understand well)</li>
</ul>
</li>
<li><strong>Collaborative filtering:</strong> items are recommended based on some similarity measure between users and between items based on rating of items by the community of users<ul>
<li>This tends to be superior in performance and does not rely on human advice</li>
<li>There are two main approaches:</li>
<li><strong>Neighbourhood Method:</strong><ul>
<li>Based on the similarity of users</li>
<li>If $A$ and $B$ gave similar evaluations to movies that they have both seen, if $A$ liked something $b$ has not seen, then $B$ may like it as well</li>
</ul>
</li>
<li><strong>Latent Factor Method:</strong><ul>
<li>Based on the similarity of items</li>
<li>Assume two movies $M_1$ and $M_2$ had similar ratings, then if someone liked $M_1$, then it is reasonable to recommend movie $M_2$ to such a user</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We can construct a sparsely populated table of ratings $R$, the rows correspond to movies, the columns to users.
The entry $r(i, j)$ of the table, if non empty, represents teh rating user $U_i$ gave to some movie $M_j$
Each entry may have some rating in range $0 - 5$ (or a similar relatively small rating range, usually with at most 10 or so levels).</p>
<h2 id="neighbourhood-method">Neighbourhood Method</h2>
<p>We replace these rating integers with more informative numbers.</p>
<ul>
<li>First we compute the mean $\bar{r}$ of all ratings for all movies</li>
<li>Then we obtain $\bar{R}$ by subtracting $\bar{r}$ from all values</li>
<li>Now if a value $r&#39;(i, j) &gt; 0$ then $U_i$ liked $M_j$ above the global average ($r&#39;(i, j)$ can be positive or negative with equal probability).
However, some ratings may be influenced by &quot;hype&quot; or &quot;trendiness&quot; AKA systematic biases&quot;.</li>
<li>To remove this:<ul>
<li>For every user $U_i$, introduce $v_i$, the &quot;individual bias&quot; of user $U_i$, reflecting tendency to give overall higher or lower scores.</li>
<li>For every movie $M_j$, introduce $\mu_j$, the &quot;hype bias&quot; of movie $M_j$ which is how &quot;fashionable&quot; the movie is (which fades with time).</li>
<li>Both systematic biases can be removed by seeking the values of $v_i$ and $\mu_j$ which minimises the expression
$$S(\vec{v}, \vec{\mu}) = \sum_{(i, j) \in R}(r&#39;(i, j) - v_i - \mu_j)^2$$</li>
<li>Note that $\mu$&#39;s are constant shifts of rows (each row corresponding to a movie) and $v$&#39;s are constant shifts of columns (each corresponding to a user).
This is a <strong>Least squares</strong> problem, and $S(\vec{v}, \vec{\mu})$ achieves a minimum when all the partial derivatives are equal to 0:
$$
\begin{align*}
\frac{\partial}{\partial v_i}S(\vec{v}, \vec{\mu}) &amp;= -2 \sum_{j:(i, j) \in R}(r&#39;(j, i) - v_i - \mu_j) = 0 \\<br>\frac{\partial}{\partial \mu_j}S(\vec{v}, \vec{\mu}) &amp;= -2 \sum_{i:(i, j) \in R}(r&#39;(j, i) - v_i - \mu_j) = 0 \\<br>\end{align*}
$$</li>
<li>Unfortunately, Least Squares fits usually suffer from overfitting.
The solution for this is called <strong>regularisation</strong>: where we introduce a term which penalises for large values of the variables.
Thus instead, we minimise the sum:
$$S(\vec{v}, \vec{\mu}, \lambda) = \sum_{(i, j) \in R}(r&#39;(i, j) - v_i - \mu_j)^2 + \lambda \left( \sum_i v_i^2 + \sum_j \mu_j^2 \right)$$
where $\lambda$ is a suitably chosen small positive constant, usually $10^{-10} \leq \lambda \leq 10^{-2}$ (the optimal value of $\lambda$ can also be &quot;learned&quot;)</li>
<li>We now obtain from $\bar{R}$ $\tilde{R}$ and are ready to estimate similarities of users and movies</li>
</ul>
</li>
</ul>
<h3 id="neighbourhood-method---similarity-of-users">Neighbourhood Method - Similarity of users</h3>
<p>One of the most frequently used measure of similarity of users is the <strong>cosine similarity measure</strong>.</p>
<p>If we want to compare 2 users $U_i$ and $U_k$, we find all movies that both users have ranked and delete all other entries.
We obtain two column vectors $\vec{u}_1$ and $\vec{v}_k$, and the similarity of the two users is measured by the cosine of the angle between these two vectors.</p>
<p>$$
\begin{align*}
  \text{sim}(U_i, U_k)
  &amp;= \cos(u_i, u_k) \\
  &amp;= \frac{\langle \vec{u}_i, \vec{u}_k \rangle}{|\vec{u}_i| \cdot |\vec{u}_k|}
\end{align*}
$$</p>
<p>We can now predict the rating a user $U_i$ would give to a movie $M_j$ they have not seen as follows:</p>
<ol>
<li>Among all users who have seen $M_j$, pick $L$ many users $U_{k_l}$ with $L$ largest values of $|\text{sim}(U_i, U_{k_l})|$ (i.e. the $L$ top similar and dissimilar values).</li>
<li>We now predict the rating user $U_i$ would give to movie $M_j$ as
$$\text{pred}(i, j) = \bar{r} + v_i + \mu_j + \frac{\sum_{1 \leq l \leq L} \text{sim}(U_i, U_{k_l}) \tilde{r}(j, k_l)}{\sum_{1 \leq l \leq L} |\text{sim}(U_i, U_{k_l})|}$$</li>
<li>We then recommend to user $U_i$ movie $M_j$ for which the predicted rating $\text{pred}(i, j)$ is the highest<ul>
<li>The &quot;hype factor&quot; $\mu_j$ is brought back when deciding what to recommend).</li>
<li>$v_i$ is constant across movies, so it is insignificant; adding it is mostly for test purposes because it will realistically predict the possible rating of $U_i$ of $M_j$ allowing easy comparison in tests</li>
</ul>
</li>
</ol>
<h3 id="neighbourhood-method---similarity-of-movies">Neighbourhood Method - Similarity of movies</h3>
<p>We can in a similar way estimate similarity of movies, working on the columns of $\tilde{R}$ (instead of rows).
We predict the rating user $U_i$ would give to the move $M_j$ as</p>
<p>$$
\text{pred} =
\bar{r} + v_i + \mu_j + \frac{\sum_{1 \leq l \leq L} \text{sim}(M_j, M_{n_l}) \tilde{r}(n_l, i)}{\sum_{1 \leq l \leq L} |\text{sim}(M_j, M_{n_l})|}
$$</p>
<p>and recommend the movie $M_j$ that has the highest value of $\text{pred}(j, i)$.</p>
<h2 id="latent-factor-method">Latent Factor Method</h2>
<p>This method relies on several heuristics</p>
<ul>
<li><p>There are features movies posses which appeal to different tastes which determine how much a user likes a movie. I.e. &quot;action&quot;, &quot;comedy&quot;, &quot;romance&quot;, &quot;famous actors&quot;, &quot;special effects&quot;</p>
</li>
<li><p>Enumerate these features as $f_1, f_2, ..., f_N$ where $N$ is to the order of a few 10s to a few 100s
A movie can have each of these features, say $f_i$ to an extent $e_i$, where say $0 \leq e_i \leq 10$.</p>
</li>
<li><p>Each movie $M_j$ has a vector $\vec{e}^j$ of length $N$, and we can form a matrix $F$ s.t. rows of $F$ correspond to movies and columns correspond to features. I.e. if we have $M$ movies:</p>
<p>$$
F =
\begin{pmatrix}
  F_{1, 1}  &amp; \ldots &amp; F_{1, N} \\
  \vdots    &amp; \vdots &amp; \vdots   \\
  \vdots    &amp; \vdots &amp; \vdots   \\
  \vdots    &amp; \vdots &amp; \vdots   \\
  \vdots    &amp; \vdots &amp; \vdots   \\
  \vdots    &amp; \vdots &amp; \vdots   \\
  F_{M, 1}  &amp; \ldots &amp; F_{M, N}
\end{pmatrix}
$$</p>
<p>resulting in a very tall matrix</p>
</li>
<li><p>We can associate each user $U_i$ with a column vector $\vec{l}^i$ s.t. its $m^{th}$ coordinate is a number, $0 \leq \vec{l}^i_m \leq 10$ which represents how much a user likes a feature $f_m$ in a movie.</p>
</li>
<li><p>We can now form a matrix $L$ whose rows correspond to features and columns correspond to users.
$$
L =
\begin{pmatrix}
  L_{1, 1}  &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; L_{1, N} \\
  \vdots    &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; \vdots   \\
  L_{M, 1}  &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; \ldots &amp; L_{M, N}
\end{pmatrix}
$$
resulting in a very wide matrix</p>
</li>
</ul>
<p>Assume that we have access to $L$ and $F$.
Then to predict how much $U_i$ likes $M_j$, we evaluate</p>
<p>$$E(j, i) = \sum_{1 \leq m \leq N} (\vec{e}^j)_m (\vec{l}^i)_m = \langle \vec{e}^j, \vec{l}^i \rangle.$$</p>
<p>and as a result can generate $E = F \times L$, resulting in a very large matrix.
However, the issue is that we cannot determine which few dozen features are relevant out of a few hundred features.</p>
<h3 id="finding-relevant-features">Finding relevant features</h3>
<p>Let $N$ be the number of features we want (typically $20 \leq N \leq 200$), $\#M$ by the number of movies, and $\#U$ be the number of users.</p>
<p>Fill matrices $F$ of size $\#M \times N$ and $L$ of size $N \times \#U$ with variables $F(j, m)$ and $L(m, i)$ whose values have yet to be determined.</p>
<p>Then sole the least squares problem in the variables
$$\{ F(j, m): 1 \leq j \leq \#M; 1 \leq m \leq N \} \cup \{ L(m, i) : 1 \leq m \leq N; 1 \leq i \leq \#U \}$$</p>
<p>minimise</p>
<p>$$S(\vec{F}, \vec{L}) = \sum_{(j, i):R(j, i)} \left( \sum_{1 \leq m \leq N} F(j, m) \cdot L(m, i) - R(j, i) \right)^2$$</p>
<p>We can attempt to find the minimum by finding the when the partial derivative of $S(\vec{F}, \vec{L})$ is equal to 0.
However:</p>
<ul>
<li>this results in a huge system of cubic equations that cannot be solved feasibly</li>
<li>an optimisation problem is not convex, so search for the optimal solution can end up in a local minimum</li>
</ul>
<p>Solution:</p>
<ol>
<li><p>Set all variables $F(j, m)$ to the same value $F^{(0)}(j, m)$ as a median value</p>
</li>
<li><p>Now solve the following Least Squares problem for the variables
$$\{ L(m, i) : 1 \leq m \leq N; 1 \leq i \leq \#U \}$$
minimise
$$\sum_{j, i}: R(j, i) \left(\sum{1 \leq m \leq N} F^{(0)}(j, m) \cdot L(m, i) - R(j, i) \right)^2$$
which is now a system of linear equations as after we find the partials $F^{(0)}(j, m)$ is set to 0.</p>
</li>
<li><p>Let $L^{(0)}(m, i)$ be the solutions to such a Least Squares problem.</p>
</li>
<li><p>We now solve the following Least Squares problem in variables
$$\{ F(j, m): 1 \leq j \leq \#M; 1 \leq m \leq N \}$$
minimise
$$\sum_{(j, i):R(j, i)} \left( \sum_{1 \leq m \leq N} F(j, m) \cdot L^{(0)}(m, i) - R(j, i) \right)^2$$</p>
</li>
<li><p>Now, we keep alternating between taking either $\{ L(m, i) : 1 \leq m \leq N; 1 \leq i \leq \#U \}$ or $\{ F(j, m): 1 \leq j \leq \#M; 1 \leq m \leq N \}$ as free variables, fixing the values of the other set from the previously obtained solution to the corresponding Least Squares problem.</p>
<p>This method is sometimes called <strong>Method of Alternating Projections</strong>.</p>
</li>
<li><p>We stop such iterations when
$$\sum_{(j. m)}(F^{(k)}(j. m) - F^{(k - 1)}(j. m))^2 + \sum_{(i. m)}(L^{(k)}(m, i) - L^{(k - 1)}(m, i))^2$$
becomes smaller than an accuracy threshold $\epsilon &gt; 0$.</p>
</li>
<li><p>After we obtain the values $F^{(k)}(j, m)$ and $L^{(k)}(m, i)$ from the last iteration $k$, we form teh corresponding matrices $F$ of size $\#M \times N$ and $L$ of size $N \times \#U$ as
$$
\begin{align*}
  \tilde{F} &amp;= \left( F^{(k)}(j, m) : 1 \leq j \leq \#M; 1 \leq m \leq N \right) \\
  \tilde{L} &amp;= \left( L^{(k)}(m, i) : 1 \leq m \leq \#M; 1 \leq m \leq \#U \right) \\
\end{align*}
$$</p>
</li>
<li><p>We finally set $E = \tilde{F} \times \tilde{L}$ as the final matrix of predicted ratings of all movies by all users, where $E(j, i)$ is the prediction of the rating of movie $M_j$ by user $U_i$.</p>
</li>
</ol>
<ul>
<li>Note we do not know what these latent features actually are</li>
<li>However, the Latent Factor Method performs remarkably well in many domains</li>
<li>Though in a Netflix Challenge competition to design the best recommendation algorithm, the top algorithms were combination of dozens of components / algorithms with empirically tuned parameters</li>
</ul>
<h1 id="clustering-algorithms">Clustering Algorithms</h1>
<p>Clustering algorithms are a type of unsupervised learning used in data science.</p>
<p>There are two kinds of clusters:</p>
<ol>
<li>center - based clusters</li>
<li>high density clusters</li>
</ol>
<p>A good clustering algorithm should be able to handle both kinds.</p>
<h2 id="data-representation">Data Representation</h2>
<p>There are two common representations of points</p>
<h3 id="as-vectors-in-mathbbrd">As vectors in $\mathbb{R}^d$</h3>
<ul>
<li>This is suitable when you have several numerical measurements (and each measurement maps to a dimension).</li>
<li>Note $d$ can be extremely large, corresponding to thousands or more, and can be complex to store and handle such data. This is where Johnson - Lindenstrauss Theorem comes to play.</li>
</ul>
<h3 id="as-a-weighted-graph">As a weighted graph</h3>
<ul>
<li>Data points are represented as vertices of the graph</li>
<li>The weights of the edges reflect the degree of similarity (or dissimilarity) of the data points.
The distance between two data points $x, y \in \mathbb{R}^d$ can be defined as either
$$d(x, y) = \sum_{i=1}^d |x_i - y_i| \quad \text{or} \quad d(x, y) = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}$$</li>
<li>If the scales of the $i^{th}$ and $j^{th}$ coordinates $x_i$ and $x_j$ differ significantly, or if they are not of equal importance, we might consider instead
$$d(x, y)^2 = \sum_{i=1}^d w_i(x_i - y_i)^2$$
where the weights $w_i$ are chosen to normalise different variances of $x_i$ and $x_j$ or to encode their relative significances.</li>
<li>Graph representation of data is often much more compact than vectors, as it does not suffer from problems of high dimensionality.</li>
<li>However, the geometry of the data is lost, so the clustering is based only on mutual distances of pairs of points, which is good when clustering is not center based.</li>
</ul>
<h2 id="center-based-clustering-algorithms">Center-based Clustering Algorithms</h2>
<p>We assume data points are represented as vectors in $\mathbb{R}^d$.</p>
<h3 id="k-center-clustering">k-center clustering</h3>
<p>Find a partition $C = \{C_1, ..., C_k \}$ of a set of data points $A = \{\mathbf{a_1}, ..., \mathbf{a_n} \}$ into $k$ clusters, with the corresponding centers $\mathbf{c_1}, ..., \mathbf{c_k}$ which minimises</p>
<p>$$\Phi(C) = \max_{j=1}^k \max_{a \in C_j} d(\mathbf{a}, \mathbf{c_j})$$</p>
<p>This will minimise the radius of the cluster (as the radius of a cluster is the furthest distance from the cluster center).</p>
<h3 id="k-median-clustering">k-median clustering</h3>
<p>Find a partition $C = \{C_1, ..., C_k \}$ of a set of data points $A = \{\mathbf{a_1}, ..., \mathbf{a_n} \}$ into $k$ clusters, with the corresponding centers $\mathbf{c_1}, ..., \mathbf{c_k}$ which minimises</p>
<p>$$\Phi(C) = \sum_{j=1}^k \sum_{a \in C_j} d(\mathbf{a}, \mathbf{c_j})$$</p>
<p>$d(\mathbf{a}, \mathbf{c}_j)$ can be any distance metric, such as</p>
<ul>
<li>$l_1$: $d(\mathbf{a}, \mathbf{c}_j) = \sum_{k=1}^d| (\mathbf{a})_k - (\mathbf{c}_j)_k |$</li>
<li>$l_2$: $d(\mathbf{a}, \mathbf{c}_j) = \sqrt{\sum_{k=1}^d ((\mathbf{a})_k - (\mathbf{c}_j)_k)^2}$
If the distance is the $l_1$ distance, one can show that the coordinates of the optimal centers are the coordinate-wise medians of points in each cluster.</li>
</ul>
<h3 id="k-means-clustering">k-means clustering</h3>
<p>This is the most frequently used center based clustering algorithm.
It is similar to the k-median clustering, except we want to minimise</p>
<p>$$\Phi(C) = \sum_{j=1}^k \sum_{a \in C_j} d(\mathbf{a}, \mathbf{c_j})^2$$</p>
<ul>
<li>This penalises more for larger distances than the k-median clustering</li>
<li>This has other nice properties; for example, if $d(\mathbf{a}, \mathbf{c_j})^2 = \sum_{j=1}^d (a_i - c_{ji})^2$ then $\mathbf{c_j}$ must be the centroids of the points in their cluster.</li>
</ul>
<h3 id="definitions">Definitions</h3>
<p><strong>Finding the centroid</strong></p>
<p>Since</p>
<p>$$\mathbf{a_i} = (a_{i1}, ..., a_{id})$$</p>
<p>let $\mathbf{c} = (c_1, ..., c_d)$ with for all $1 \leq k \leq d$,</p>
<p>$$c_k = (a_{1k} + ... + a_{nk}) / n$$</p>
<p>As a result, $c_k$ is the arithmetic mean of the $k^{th}$ coordinates of all the points $\{ \mathbf{a}_1, ..., \mathbf{a}_n \}$.
Hence, $\mathbf{c}$ is called the centroid of the set of points $\{ \mathbf{a}_1, ..., \mathbf{a}_n \}$.</p>
<p><strong>Finding the distance</strong></p>
<p>Then $\mathbf{c}$ is called the centroid of the set of points $\{ \mathbf{a_1}, ..., \mathbf{a_n} \}$.</p>
<p>We denote $\mathbf{x} \cdot \mathbf{y}$ the scalar product of vectors $\mathbf{x}$ and $\mathbf{y}$ by $|| \mathbf{x} ||$ the norm of a vector $\mathbf{x}$, i.e.</p>
<p>$$\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^d x_i y_i \quad \text{and} \quad ||x|| = \sqrt{\sum{i=1}^d x_i^2} = \sqrt{\mathbf{x} \cdot \mathbf{x}}$$</p>
<p>Note that $|| \mathbf{x} - \mathbf{y} ||$ is the Euclidean distance of points $\mathbf{x}$ and $\mathbf{y}$:</p>
<p>$$|| \mathbf{x} - \mathbf{y} || = \sqrt{\sum_{i=1}^d(x_i - y_i)^2}$$</p>
<p><strong>Theorem</strong></p>
<p>Let $A = \{\mathbf{a_1}, ..., \mathbf{a_n} \}$ be a set of points and $\mathbf{x}$ be another point, all in $\mathbb{R}^d$. Let also $\mathbf{c}$ be the centroid of $A$.
Then</p>
<p>$$\sum_{i=1}^n || \mathbf{a_i} - \mathbf{x} ||^2 + n||\mathbf{c} - \mathbf{x}||^2$$</p>
<p><strong>Corollary</strong></p>
<p>Let $A = \{\mathbf{a_1}, ..., \mathbf{a_n} \}$ be a set of points and $\mathbf{x}$ be another point, all in $\mathbb{R}^d$.
Then</p>
<p>$$D(x) = \sum_{i=1}^n || \mathbf{a_i} - \mathbf{x} ||^2$$</p>
<p>is minimised when $\mathbf{x}$ is the centroid $\mathbf{c} = \frac{1}{n} \sum_{i=1}^n \mathbf{x_i}$.</p>
<h3 id="implementation">Implementation</h3>
<p>Thus, to find a partition of set of points $A$ into $k$ disjoint components $A = \bigcup_{i=1}^k A_i$ and $k$ points $\mathbf{x_1}, ..., \mathbf{x_k}$ such that the sum</p>
<p>$$\sum{j=1}^k \sum{\mathbf{a_i} \in A_j} || \mathbf{a_i} - \mathbf{x_j} ||^2$$</p>
<p>is as small as possible, then whatever such an optimal partition $\{ A_j : 1 \leq j \leq k \}$ might be, the points $\mathbf{x_j}$ must be the centroids $\mathbf{c_j}$ of sets $A_j$.</p>
<p>Hence, finding the $k$ clusters is equivalent to minimising</p>
<p>$$\sum_{m=1}^k \frac{1}{2|A_m|} \sum_{\mathbf{a_i}, \mathbf{a_j} \in A_m} || \mathbf{a_i} - \mathbf{a_j} ||^2$$</p>
<h3 id="lloyds-algorithm">Lloyd&#39;s Algorithm</h3>
<p>Solving the optimal k-means clustering is NP hard, so we use approximate algorithms.</p>
<p>The best known approximate k-means clustering algorithm is Lloyd&#39;s algorithm.</p>
<ol>
<li>Start with an initial set of cluster centers $\{\mathbf{c}^{(0)}_m : 1 \leq m \leq k\}$</li>
<li>Cluster all points $\mathbf{a} \in A$ into clusters $A_m$ by associating each $\mathbf{a} \in A$ with the nearest cluster centre.</li>
<li>Replace cluster centres with the centroids of thus obtained clusters.</li>
<li>Repeat 2 and 3 until cluster centers (and thus also clusters) stop changing.</li>
</ol>
<p>At every round $p$ of its loop, Lloyd&#39;s algorithm reduces the size of</p>
<p>$$\sum_{m=1}^k \sum_{\mathbf{a}_j \in A_m^{(p)}} || \mathbf{a}_j - \mathbf{c}_m^{(p)} ||^2$$</p>
<p>where $A_m^{(p)}$ are the &quot;temporary&quot; clusters and $\mathbf{c}^{(p)}_m$ is the &quot;temporary&quot; centre of cluster $A_m^{(p)}$ at round $p$ of the loop.</p>
<p>However, the algorithm may stop at a local minimum and not the global minimum.</p>
<ul>
<li>We may choose to run this algorithm multiple times and return the best result</li>
<li>Another algorithm called the <strong>Farthest Traversal k-clustering</strong> picks a random point $a_q$ from $A$ as the first center, and then pick the furthest point in $A$ from $a_q$ as the second point, and continue picking the next center as the one with the largest minimum distance from the already picked centers<ul>
<li>If $A$ has a clustering radius of $r$, then this algorithm produces a radius at most $2r$.</li>
</ul>
</li>
<li>We can randomise the selection by picking a point with probability proportional to the shortest distance to one of already picked points</li>
</ul>
<h3 id="wards-algorithm">Ward&#39;s Algorithm</h3>
<p>Wards algorithm is a greedy k-means algorithm:</p>
<ol>
<li><p>Start with every point $\mathbf{a}_i$ in its own cluster.</p>
</li>
<li><p>While the number of clusters is larger than $k$ repeat:</p>
<p>Find two clusters $C$ and $C&#39;$ such that
$$\text{cost}(C \cup C&#39;) - \text{cost}(C) - \text{cost}(C&#39;)$$
is as small as possible and replace them with a single merged cluster $C \cup C&#39;$ with its centroid as its centre</p>
</li>
</ol>
<h2 id="center-based-clustering-algorithms-1">Center-based Clustering Algorithms</h2>
<p>There are several ways to find non centre based clusters.</p>
<h3 id="similarity-graphs">Similarity Graphs</h3>
<p>Rather than representing a set of data points $A$ with their locations, we represent them as an undirected weighted graph $G = (V, E)$.</p>
<ul>
<li>The weight $w_{ij}$ of an edge $e = (v_i, v_j)$ is equal to a similarity measure of the data points.<ul>
<li>If $w_{ij} = 0$ then the vertices $v_i$ and $v_j$ are completely dissimilar points, and so we do not include this edge</li>
<li>Else the weight can depend on a decreasing function of the Euclidean distance, i.e.
$$e^{-\frac{|| \mathbf{a}_i - \mathbf{a}_j ||^2}{2}}$$</li>
</ul>
</li>
<li>Since the graph is weighted, the degree $d_i$ of a vertex $v_i$ is defined as
$$d_i = \sum_{j=1}^n w_{ij}$$
This degree matrix $D$ is a diagonal matrix with degree $d_i$ of vertex $v_i$ on the $i^{th}$ entry of the diagonal of $D$ and zeroes everywhere else</li>
</ul>
<p>From here there are several ways to cluster the points</p>
<ol>
<li><strong>The $\epsilon$-neighbourhood graph</strong><ul>
<li>We connect all pairs of vertices $v_i$, $v_j$ such that the distances between the data points $&lt; \epsilon$.</li>
<li>This distance is usually the Euclidean distance</li>
</ul>
</li>
<li><strong>The $k$-nearest neighbour graphs</strong><ul>
<li>There are two flavours of this<ol>
<li><strong>Unidirectional k-nearest neighbour graph</strong>
Connect $v_i$ with $v_j$ if either $v_j$ is among $k$ nearest neightours of $v_i$ <strong>or</strong> vice versa</li>
<li><strong>Mutual k-nearest neighbour graph</strong>
Connect $v_i$ with $v_j$ if both $v_j$ is among $k$ nearest neighbours of $v_i$ <strong>and</strong> vice versa</li>
</ol>
</li>
<li>In both cases, the edge is then weighted with the degree of similarity of the vertices $v_i$ and $v_j$</li>
</ul>
</li>
<li><strong>The fully connected graphs</strong><ul>
<li>Connect all pairs of vertices $v_i$ and $v_j$ where the corresponding data points have a strictly positive similarity or similarity higher than some threshold $\epsilon$.</li>
<li>Often we take weights with the formula
$$w_{ij} = e^{-\frac{|| \mathbf{a}_i - \mathbf{a}_j ||^2 }{2 \theta^2}}$$</li>
<li>Here $\theta$ is a parameter which determines &quot;the size&quot; of the neighbourhood, namely how fast the similarity decreases as distance increases</li>
</ul>
</li>
</ol>
<ul>
<li>However, there is no simple way to choose a similarity graph, the best way is to try and determine one empirically</li>
</ul>
<h3 id="spectral-graph-theory">Spectral Graph Theory</h3>
<p>Recall that the $n \times n$ diagonal matrix $D$ has the degrees $d_i$ of vertices $v_i$ on its diagonal, where $d_i = \sum_{j=1}^n w_{ij}$.</p>
<p>The (unnormalised) graph Laplacian matrix $L$ is defined as</p>
<p>$$L = D - W$$</p>
<p>where $W = (w_{ij})^n_{i, j = 1}$.</p>
<p>Clearly $L$ is symmetric and does not depend on $w_{ii}$, $1 \leq i \leq n$.
Graph Laplacians are crucial for spectral clustering.</p>
<p>A matrix $M$ of size $n \times n$ is positive semi-definite if for all vectors $f \in \mathbb{R}^n$ we have</p>
<p>$$f^\intercal M f \geq 0$$</p>
<p>A symmetric matrix is positive semi-definite iff all of its eigvenvalues are real and non-negative.</p>
<p>The matrix $L = D - W$ has the following properties:</p>
<ol>
<li>For every vector $f \in \mathbb{R}^n$,
$$
\begin{align*}
f^\intercal L f
  &amp;= f^\intercal D f - f^\intercal W f \\
  &amp;= \sum_{i=1}^n d_i f_i^2 - \sum_{ij=1}^n w_{ij} f_i f_j \\
  &amp;= \frac{1}{2} \sum_{i, j = 1}^{n} w_{ij} (f_i - f_j)^2
\end{align*}
$$</li>
<li>$L$ is a symmetric positive semi-definite matrix
As shown from (1), since $w_{ij} \geq 0$, L satifies $f^\intercal L f \geq 0$ for all vectors $f$ and is thus positive semi-definite.</li>
<li>The smallest eigenvalue of $L$ is 0 and its corresponding eigenvector is $\mathbb{1} = (1, 1, ..., 1)$</li>
</ol>
<p>(Missing some Spectral Graph Theory)</p>
<p><strong>Spectral Clustering Algorithm:</strong></p>
<ol>
<li>Construct a similarity graph $G$ by one of the way described and let $W$ be its weighted adjacency matrix</li>
<li>Compute the Laplacian $L = D - W$.</li>
<li>Compute the $k$ eigenvectors $\mathbf{e}_1, ..., \mathbf{e}_k$ of $L$ which correspond to $k$ smallest eigenvalues.</li>
<li>Let $E$ be the matrix of size $n \times k$ containing the eigenvectors $\mathbf{e}_1, ..., \mathbf{e}_k$ as columns.</li>
<li>For $i = 1, ..., n$, let $\mathbf{y}_i$ be the vector corresponding to teh $i^{th}$ row of E</li>
<li>Cluster points $\{ \mathbf{y}_1, ..., \mathbf{y}_n \}$ using the k-means algorithm into clusters $C_1, ..., C_k$.</li>
</ol>
<p>(Missing application of Spectral Clustering as graph partioning)</p>
<h1 id="dft-dct-convolution">DFT, DCT, Convolution</h1>
<h2 id="dft">DFT</h2>
<p>FFT is an $O(n \log n)$ DFT conversion and the IFFT can compute the IDFT in the same runtime.
The benefit of finding the DFT or IDFT of something is that it can represent data, in another form which can be more easily manipulated or used.</p>
<p>(Missing a ton of theory)</p>
<h2 id="convolution">Convolution</h2>
<p>Let $A = \langle A_0, A_1, ..., A_{n-1} \rangle$ and $B = \langle B_0, B_1, ..., B_{m-1} \rangle$ be two sequences of real or complex numbers.
We can now form two associated polynomials $P_A(x)$ and $P_B(x)$ with coefficients given by sequences $A$ and $B$.
Finding the multiple of these polynomials $P_C(x) = P_A(x) \cdot P_B(x)$ can be done in $O(m \times n)$.
From here, we can get the sequence of it&#39;s corresponding coefficients.
This sequence of length $m + n - 1$ is the linear convolution of sequences $A$ and $B$.
However, using the FFT algorithm, we can find the linear convolution of these two sequences in time $O((m + n) \log_2(m + n))$.</p>
<p>An example application is application of Gaussian smoothing to a noisy signal.</p>
<h1 id="svd">SVD</h1>
<p>Singular Value Decomposition is a way of representing a very large matrix $A$ as</p>
<p>$$A = \sum_{i=1}^r = \sigma_i u_i v_i^\intercal = UDV^\intercal$$</p>
<p>(Insert some more theory)</p>
<p><strong>To find $\mathbf{v}_1$ and $\mathbf{u}_1$:</strong></p>
<ol>
<li>Start with a random vector $\mathbf{x}$ and normalise it so that $| \mathbf{x}| = 1$.</li>
<li>Compute $\mathbf{z}_1 = A\mathbf{x}$; compute $\mathbf{z}_2 = A^\intercal z_1$; let $\rho = |\mathbf{z}_2|/|\mathbf{x}|$ and $\mathbf{x} = \mathbf{z}_2$</li>
<li>Repeat (2) until the difference between $\rho$&#39;s obtained in two consecutive iterations is smaller than a small threshold $\epsilon$.</li>
<li>Set $\mathbf{v}_1 = \mathbf{z}_2 / |\mathbf{z}_2|$</li>
<li>Finally, let $\sigma_1 = |A\mathbf{v}_1|$ and $\mathbf{u}_1 = A \mathbf{v}_1 / \sigma_1$</li>
</ol>
<p>This method is called the <strong>Power Method</strong> and it is fast if $A$ and thus also $A^\intercal$ are sparse.</p>
<p>A good <a href="https://www.youtube.com/watch?v=gXbThCXjZFM">series on SVD</a>.</p>
<h1 id="power-transmission-in-cellular-networks">Power Transmission in Cellular Networks</h1>
<p>The signal to interference ratio $SIR_i$ at receiver $R_i$ is given by</p>
<p>$$SIR_i = \frac{G_{ii}p_i}{\sum_{j: j \neq i}G_{ii}p_j + \eta_i}$$</p>
<p>where $\eta_i$ is the noise received by the receiver $i$ coming from the environment.
$SIR_i$ determines the capacity of the channel $C_{ii}$.
Each pair of transmitter $T_i$ and a receiver $R_i$ needs at least some channel capacity to carray information.
To achieve this, it needs $SIR_i \geq \gamma_i$.</p>
<p>We will see later how $\gamma_i$ is determined in practice.
However, we are interested in:</p>
<p>$$\text{minimise } \sum_{j=1}^n p_j$$</p>
<p>$$\text{subject to constraints } \frac{G_{ii} p_i}{\sum_{j: j \neq i}G_{ij} + \eta_i} \geq \gamma_i, \quad 1 \leq i \leq n$$</p>
<h2 id="matrix-form">Matrix Form</h2>
<p>Following this, we can simplify our original LP problem into</p>
<p>$$\text{minimise } \sum_{j=1}^n p_j$$</p>
<p>$$\text{subject to constraints } p_i - \gamma_i \sum{j:j \neq i} \frac{G_{ij}}{G_{ii}} p_j \geq \frac{\gamma_i \eta_i}{G_{ii}} \\ 1 \leq i, j \leq n, p_j &gt; 0$$</p>
<p>We can then convert this into a matrix format:</p>
<p>$$\text{minimise } \mathbf{1}^\intercal \mathbf{p}$$</p>
<p>$$\text{subject to constraints } (I - DF)\mathbf{p} \geq \mathbf{v}, \mathbf{p} \leq 0$$</p>
<p>This will have a feasible solution if we are not demanding excessively large $\gamma_i&#39;s$.</p>
<p>This is the case when the spectral radius (largest absolute value of the eigenvalues) of matrix $DF$ $\rho(DF)$, satisfies $\rho(DF) &lt; 1$.</p>
<p>Proof:</p>
<ul>
<li>If $A$ is square matrix and if $\rho(A) &lt; 1$ then $A^m \rightarrow 0$.</li>
<li>This is easy to see if $A$ has $n$ linearly independent eigenvectors, because in this case $A$ can be represented as
$$A = Q \Lambda Q^{-1}$$</li>
<li>Here the $i^{th}$ column of $Q$ is the eigenvector corresponding to eigenvalue $\lambda_i$
$\Lambda$ is a diagonal matrix with $\lambda_i$ in the $i^{th}$ column and row and zeroes elsewhere.
In such a case
$$A^k = Q \Lambda Q^{-1} Q \Lambda Q^{-1} ... Q \Lambda Q^{-1} Q \Lambda Q^{-1} = Q \Lambda^k Q^{-1}$$</li>
<li>Since $\Lambda^k$ has $\lambda_i^k$ on the diagonal, if $\rho(A) &lt; 1$ then clearly $A^k \rightarrow 0$</li>
</ul>
<p>Moreover it is easy to see that</p>
<p>$$(I - A) \sum_{i=0}^k A^i = \sum_{i=0}^k A^i - \sum_{i=1}^{k+1}A^i = I - A^{k+1}$$</p>
<p>Thus,</p>
<p>$$\lim_{k \rightarrow \infty} \left( (I - A) \sum_{i=0}^k A^k \right) \lim_{k \rightarrow \infty}(I - A^{k + 1}),$$</p>
<p>which implies</p>
<p>$$(I - A) \sum_{i=0}^\infty A^i = I.$$</p>
<p>This shows that matrix $I - A$ is invertible and that</p>
<p>$$(I - A)^{-1} = \sum_{i=0}^\infty A^i$$</p>
<p>Applying this to matrix $A = DF$, let $\mathbf{p}^*$ be give by</p>
<p>$$\mathbf{p^*} = (I - DF)^{-1} \mathbf{v} = \sum_{i=0}^\infty (DF)^i \mathbf{v}$$</p>
<p>Then $(I - DF) \mathbf{p^*} = \mathbf{v}$ and the constraint becomes</p>
<p>$$(I - DF) \mathbf{p} \geq (I - DF) \mathbf{p^*}$$</p>
<p>i.e.</p>
<p>$$(I - DF)(\mathbf{p} - \mathbf{p^*}) \geq 0$$</p>
</article></span></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"contents":"\u003ch1 id=\"statistics\"\u003e\u003ca href=\"#statistics\"\u003eStatistics\u003c/a\u003e\u003c/h1\u003e\n\n\u003ch2 id=\"expectation-and-variance-of-a-random-variable\"\u003eExpectation and Variance of a Random Variable\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eexpected value\u003c/strong\u003e of a random variable is its mean. Assuming that the expected value converges, the expected value can be calculated as shown below.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eDiscrete random variable\u003c/th\u003e\n\u003cth\u003eContinuous random variable\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e$E(X) = \\sum_{i = 1}^{\\infty} v_i \\cdot p_i$\u003c/td\u003e\n\u003ctd\u003e$E(X) = \\int_{-\\infty}^{\\infty}x \\cdot f(x) dx$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eThe \u003cstrong\u003evariance\u003c/strong\u003e of a random variable is defined as $V(X) = E(X - E(X))^2$ assuming that both expectations involved are finite; the standard deviation of a random variable $X$ is given by $\\sigma = \\sqrt{V(X)}$.\u003c/p\u003e\n\u003ch2 id=\"simple-inequalities\"\u003eSimple inequalities\u003c/h2\u003e\n\u003cp\u003e$$1 + x \\leq e^x \\text{ for all } x \\in \\mathbb{R}$$\n$$\\left(1 - \\frac{1}{n}\\right)^n \\leq \\frac{1}{e} \\leq \\left(1 - \\frac{1}{n} \\right)^{n - 1} \\text{ for all } n \\in \\mathbb{N}$$\u003c/p\u003e\n\u003ch2 id=\"probability-inequalities\"\u003eProbability Inequalities\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eThe Markov Inequality\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLet $X \u0026gt; 0$ be a non-negative random variable. Then for all $t \u0026gt; 0$, $$P(X \\geq t) \\leq \\frac{E(X)}{t}$$\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eChebyshev Inequality\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLet $X \u0026gt; 0$ be a random variable with the expected value $\\mu = E(X)$ and standard deviation $\\sigma = \\sqrt{E((X - \\mu)^2)}$. Then for all $\\lambda \u0026gt; 0$, $$P(| X - \\mu | \\geq \\lambda \\sigma) \\leq \\frac{1}{\\lambda^2}$$\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eChernoff Bound\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eLet $X = \\sum_{k = 1}^{n} X_k$, where $X_k, 1 \\leq k \\leq n$, are independent Bernoulli trials with the probability of success $P(X_k = 1) = p_k$, where $0 \u0026lt; p_k \u0026lt; 1$. Thus, $\\mu = E(X) = \\sum_{k = 1}^{n} p_k$. Let $\\sigma \u0026gt; 0$ be any positive real. Then, $$P(X \u0026gt; (1 + \\sigma) \\mu) \u0026lt; \\left( \\frac{e^{\\sigma}}{(1 + \\sigma)^{1 + \\sigma}}\\right)^{\\mu}$$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch1 id=\"order-statistics-quickselect\"\u003e\u003ca href=\"#order-statistics-quickselect\"\u003eOrder Statistics (QuickSelect)\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003eCan we find the $i^{th}$ largest or smallest item in linear time?\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"non-deterministic-algorithm\"\u003eNon-Deterministic Algorithm\u003c/h2\u003e\n\u003cp\u003eIn the quicksort algorithm, we note that after a partition, the pivot element ends up being in its correct index. Therefore, we can perform a combination of partitioning and binary search to find the item at the $i^{th}$ index.\u003c/p\u003e\n\u003cp\u003eFor example, we partition first using a random pivot, and as a result we will know the index of our pivot.\nIf the pivot is index $i$, we\u0026#39;ve found the element.\nElse if the pivot is at an index greater than $i$, then we partition the smaller side, else we partition the bigger side and continue until we\u0026#39;ve found the $i^{th}$ element.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Implementation of rand-select\n\ndef partition(A: list[int], lo: int, hi: int) -\u0026gt; int:\n    # Partition the array A[lo..hi] and return partition index\n    i = lo - 1\n    pivot = A[hi]\n    for j in range(lo, hi):\n        if A[j] \u0026lt;= pivot:\n            i += 1\n            A[i], A[j] = A[j], A[i]\n    A[i + 1], A[hi] = A[hi], A[i + 1]\n    return i + 1\n\n\ndef rand_select(A: list[int], i: int, lo: int, hi: int) -\u0026gt; int:\n    # Return the ith largest index in A\n    if lo \u0026lt;= hi:\n        pi = partition(A, lo, hi)\n        if i \u0026lt; pi:\n            return rand_select(A, i, lo, pi - 1)\n        return rand_select(A, i, pi + 1, hi)\n    return A[hi]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"runtime\"\u003eRuntime\u003c/h2\u003e\n\u003cp\u003eThe runtime can be represented through the recurrence relation $T(n) = T\\left( \\frac{n}{2} \\right) + O(n)$ which results in a $O(n)$ runtime. However, it can be analysed more in-depth statistically.\u003c/p\u003e\n\u003ch3 id=\"worst-case\"\u003eWorst Case\u003c/h3\u003e\n\u003cp\u003eThe worst case runtime is $\\Theta(n^2)$ which happens when the smallest or largest element is picked as the pivot - resulting in an \u003cem\u003eunbalanced partition\u003c/em\u003e.\u003c/p\u003e\n\u003ch3 id=\"average-case\"\u003eAverage Case\u003c/h3\u003e\n\u003cp\u003eHowever (assuming all elements are \u003cstrong\u003edistinct\u003c/strong\u003e), let us call a partition a \u003cem\u003ebalanced partition\u003c/em\u003e if the ratio between the smaller side and larger side is less than 9 (9 is arbitrary, any small number \u0026gt; 2 would do).\nThen the probability of having a balanced partition is 1 - (chance of choosing smallest 1/10 or biggest 1/10) = 1 - 2/10 = 8/10.\u003c/p\u003e\n\u003cp\u003eThen let us find the expected number of partitions between two consecutive balanced partitions.\nIn general, the probability that you need $k$ partitions to end up with another balanced partition is $\\left( \\frac{2}{10} \\right)^{k - 1} \\cdot \\frac{8}{10}$.\u003c/p\u003e\n\u003cp\u003eThus, the expected number of partitions between two balanced partitions is\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nE\n  \u0026amp;= 1 \\cdot \\frac{8}{10} + 2 \\cdot \\left( \\frac{2}{10} \\right) \\cdot \\frac{8}{10} + 3 \\cdot \\left( \\frac{2}{10} \\right)^2 \\cdot \\frac{8}{10} + ... \\\\\n  \u0026amp;= \\frac{8}{10} \\cdot \\sum_{k = 0}^{\\infty}(k + 1) \\left( \\frac{2}{10} \\right)^k \\\\\n  \u0026amp;= \\frac{8}{10}S.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eTo find $S$, note that\u003c/p\u003e\n\u003cp\u003e$$\\sum_{k = 0}^{\\infty} q^k = \\frac{1}{1 - q}.$$\u003c/p\u003e\n\u003cp\u003eBy differentiating both sides with respect to $q$ we get\u003c/p\u003e\n\u003cp\u003e$$\\sum_{q = 1}^{\\infty} k q^{k - 1} = \\frac{1}{(1 - q)^2}.$$\u003c/p\u003e\n\u003cp\u003eSubstituting $q = \\frac{2}{10}$ we get that $S = \\left(\\frac{10}{8} \\right)^2$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTherefore,\u003c/p\u003e\n\u003cp\u003e$$E = \\frac{8}{10} \\cdot \\left( \\frac{8}{10} \\right)^2 = \\frac{5}{4}.$$\u003c/p\u003e\n\u003cp\u003eAnd so there are only 5/4 partitions between two balanced partitions.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote after 1 \u003cem\u003ebalanced partition\u003c/em\u003e, the size of the array is $\\leq 9/10 n$, after the second \u003cem\u003ebalanced partition\u003c/em\u003e, it is $\\leq (9/10)^2n$ and so on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTherefore, the total \u003cstrong\u003eaverage\u003c/strong\u003e (expected) run time satisfies\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nT(n)\n  \u0026amp;\u0026lt; 5/4n + 5/4 \\left( \\frac{9}{10} \\right)n + 5/4 \\left( \\frac{9}{10} \\right)^2 n + 5/4 \\left( \\frac{9}{10} \\right)^3 n + ... \\\\\n  \u0026amp;= \\frac{5/4 n}{1 - \\frac{9}{10}} \\\\\n  \u0026amp;= 12.5n.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eOverall, the expected runtime of this algorithm is linear.\u003c/p\u003e\n\u003ch1 id=\"database-access\"\u003e\u003ca href=\"#database-access\"\u003eDatabase access\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eAssume that $n$ processes want to access a database, and that the time $t$ is discrete.\nIf two processes simultaneously request access, there is a conflict and all processes are locked out of access.\u003c/p\u003e\n\u003cp\u003eAssume that processes cannot communicate with each other on when to access.\nOne possible for a process to determine if it should access the database at time $t$ is to \u0026quot;request access\u0026quot; with probability $p$ and \u0026quot;do not request access\u0026quot; with probability $(1 - p)$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat should $p$ be to maximise the probability of a successful access to the database for a process at any instant $t$?\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"efficient-selection-of-p\"\u003eEfficient selection of p\u003c/h2\u003e\n\u003cp\u003eThe probability of success of process $i$ at any instant $t$ is\u003c/p\u003e\n\u003cp\u003e$$P(S(i, t)) = p(1 - p)^{n - 1},$$\u003c/p\u003e\n\u003cp\u003ebecause a process $i$ requests access with probability $p$ and the probability that no other process has requested access is $(1 - p)^{n - 1}$.\u003c/p\u003e\n\u003ch3 id=\"visualization-of-success-probability\"\u003eVisualization of Success Probability\u003c/h3\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-38cfabdfb53946732153ddfb07946ac0.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cp\u003eThe graph clearly shows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHigher n\u003c/strong\u003e → \u003cstrong\u003eLower optimal p\u003c/strong\u003e: As processes increase, optimal request probability decreases\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePeak success probability decreases\u003c/strong\u003e: More processes lead to lower maximum success rates\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSharper curves for large n\u003c/strong\u003e: The optimal region becomes narrower as n increases\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe extreme points of this is found by solving\u003c/p\u003e\n\u003cp\u003e$$\\frac{d}{dp}P(S(i, t)) = (1 - p)^{n - 1} - p(n - 1)(1 - p)^{n - 2} = 0$$\u003c/p\u003e\n\u003cp\u003ewhich gives $p = 1/n$, which gives a probability of success of\u003c/p\u003e\n\u003cp\u003e$$P(S(i, t)) = p(1 - p)^{n - 1} = \\frac{1}{n} \\left( 1 - \\frac{1}{n} \\right)^{n - 1}.$$\u003c/p\u003e\n\u003cp\u003eHowever,\u003c/p\u003e\n\u003cp\u003e$$\\lim_{n \\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)^n = e$$\u003c/p\u003e\n\u003cp\u003eand hence $P(S(i, t)) = \\Theta \\left( \\frac{1}{n} \\right)$.\nThus, the probability of failure after $t$ instances is\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  P(\\text{failure after $t$ instants})\n    \u0026amp;= \\left( 1 - \\frac{1}{n} \\left( 1 - \\frac{1}{n} \\right)^{n - 1} \\right)^t \\\\\n    \u0026amp;\\approx \\left( 1 - \\frac{1}{e n}\\right)^t\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eWe observe that\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eP(failure after $t = en$ instances)\u003c/th\u003e\n\u003cth\u003eP(failure after $t = en \\cdot 2 \\ln n$ instances)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e$$\\left( 1 - \\frac{1}{en}\\right)^{en} \\approx \\frac{1}{e}$$\u003c/td\u003e\n\u003ctd\u003e$$\\left(1 - \\frac{1}{en} \\right)^{en \\cdot 2 \\ln n} \\approx \\frac{1}{n^2}$$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eThus, a small increase in the number of time instants, from $en$ to $en \\cdot 2 \\ln n$ caused a dramatic reduction in the probability of failure.\u003c/p\u003e\n\u003cp\u003eAfter $en \\cdot 2 \\ln n$ instances, the probability of failure of each process $\\leq 1/n^2$ and since there are $n$ processes, then the probability that at least one process failed is $\\leq 1/n$.\nThus after $en \\cdot 2 \\ln n$ instances all processes succeeded to access the database with probability at least $1 - 1/n$.\u003c/p\u003e\n\u003ch2 id=\"comparison-with-centralised-algorithm\"\u003eComparison with centralised algorithm\u003c/h2\u003e\n\u003cp\u003eIf the processes could communicate, then it would take $n$ instances for all of them to access the database.\u003c/p\u003e\n\u003cp\u003eIf they can\u0026#39;t communicate, then the above method will allow them to access the database with probability $1 - 1/n$ time, which is larger only by a relatively small factor of $2e \\ln n$.\u003c/p\u003e\n\u003ch1 id=\"skip-lists\"\u003e\u003ca href=\"#skip-lists\"\u003eSkip Lists\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eA skip list is a probabilistic data structure that functions similarly to a binary search tree.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Example structure of a skip list\n\n[H]--------------------------[35]----------------[T]\n[H]----------------[21]------[35]----------------[T]\n[H]------[12]------[21]-[24]-[35]------[55]------[T]\n[H]-[02]-[12]-[17]-[21]-[24]-[35]-[43]-[55]-[62]-[T]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"operations\"\u003eOperations\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSearch of $k$\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart from the highest level of head H and go as far right without exceeding $k$\u003c/li\u003e\n\u003cli\u003eDrop one level down and repeat the procedure using lower level links until you find $k$\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInsertion of $k$\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSearch for the correct location\u003c/li\u003e\n\u003cli\u003eToss a coin until you get a head, and count the number of tails $t$ you got\u003c/li\u003e\n\u003cli\u003eInsert $k$ and link it at levels $0 - t$ from the bottom up\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDeletion\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDeleting an element is just like in a standard doubly linked list\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"analysis\"\u003eAnalysis\u003c/h2\u003e\n\u003ch3 id=\"expected-levels\"\u003eExpected Levels\u003c/h3\u003e\n\u003cp\u003eThe probability of getting $i$ consecutive tails when flipping a coin $i$ times is $1/2^i$.\nThus, an $n$ element Skip List has on average $n/2^i$ elements with links on level $i$.\nSince an element has links only on levels $0 - i$ with probability $1/2^{i + 1}$, the total \u003cstrong\u003eexpected\u003c/strong\u003e number of link levels per element is\u003c/p\u003e\n\u003cp\u003e$$\\sum_{i = 0}^{\\infty} \\frac{i + 1}{2^{i + 1}} = \\sum_{i = 1}^{\\infty} \\frac{i}{2^i} = 2.$$\u003c/p\u003e\n\u003cp\u003eLet $\\#(i)$ be the number of elements on level $i$.\u003c/p\u003e\n\u003cp\u003eThen, $E[\\#(i)] = \\frac{n}{2^i}$, and by the Markov inequality, the probability of having at least one element at level $i$ satisfies\u003c/p\u003e\n\u003cp\u003e$$P(\\#(i) \\geq 1) \\leq \\frac{E[\\#(i)]}{1} = \\frac{n}{2^i}.$$\u003c/p\u003e\n\u003cp\u003eThus, the probability to have an element on level $2 \\log n$ is smaller than $n/2^{2 \\log n} = 1/n.$\u003c/p\u003e\n\u003cp\u003eMore generally, the probability to have an element (be nonempty) on level $k \\log n$ $\u0026lt; n /2^{k \\log n} = 1/n^{k - 1}$.\nThe expected value $E(k)$ such that $k$ is the lowest integer so that the number of levels is $\\leq k \\log n$ is\u003c/p\u003e\n\u003cp\u003e$$E(k) \\leq \\sum_{k = 1}^{\\infty} \\frac{k}{n^{k - 1}} = \\left( \\frac{n}{n - 1} \\right)^2.$$\u003c/p\u003e\n\u003cp\u003eThus, the expected number of levels is barely larger than $\\log n$.\u003c/p\u003e\n\u003ch3 id=\"expected-search-per-level\"\u003eExpected search per level\u003c/h3\u003e\n\u003cp\u003eThus, the expected number of elements between any two consecutive elements with a link on level $i + 1$ which have links only up to level $i$ is smaller than\u003c/p\u003e\n\u003cp\u003e$$\\frac{0}{2} + \\frac{1}{2^2} + \\frac{2}{2^3} + \\frac{3}{2^4} + ... = 1.$$\u003c/p\u003e\n\u003cp\u003eSo once on level $i$, on average we will have to inspect only two elements on that level before going to a lower level.\u003c/p\u003e\n\u003ch3 id=\"search-time-complexity\"\u003eSearch Time Complexity\u003c/h3\u003e\n\u003cp\u003eOn average, levels $\u0026lt; 2 \\log n$, and 2 elements are visited per level.\nTherefore, on average, the search will be in time $O(4 \\log n) = O(\\log n)$.\u003c/p\u003e\n\u003ch3 id=\"space-complexity\"\u003eSpace Complexity\u003c/h3\u003e\n\u003cp\u003eFor an element on levels $0 - i$, we store $O(i + 1)$ pointers, and expected number of elements with highest link on level $i$ is $O(n/2^{i + 1})$. Thus, total expected space for is\u003c/p\u003e\n\u003cp\u003e$$O \\left( \\sum_{i = 0}^{\\infty}2(i + 1) \\frac{n}{2^{i + 1}}\\right) = O \\left( 2n \\sum_{i = 0}^{\\infty} \\frac{i + 1}{2^{i + 1}} \\right) = O(4n) = O(n).$$\u003c/p\u003e\n\u003ch1 id=\"kargers-min-cut-algorithm\"\u003e\u003ca href=\"#kargers-min-cut-algorithm\"\u003eKarger's Min Cut Algorithm\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eGiven a graph $G = (V, E)$, Karger\u0026#39;s Min Cut algorithm finds a cut $T$ that partitions vertices $V$ into two non empty disjoint subsets $X$ and $Y$, with the lowest capacity of edges which have one edge in $X$ and the other in $Y$.\u003c/p\u003e\n\u003cp\u003eA deterministic way to solve this is through max flow from one vertex to all other vertices, however this runs in $O(|V|^4)$.\u003c/p\u003e\n\u003ch2 id=\"procedure\"\u003eProcedure\u003c/h2\u003e\n\u003ch3 id=\"contraction\"\u003eContraction\u003c/h3\u003e\n\u003cp\u003eThe algorithm makes use of contracting edges in a graph.\nTo contract an edge $e(u, v)$, fuse $u$ and $v$ into a single vertex $[uv]$ and replace edges $e(u, x)$ and $e(v, x)$ with a single edge $e([uv], x)$ of weight $w([uv], x) = w(u, x) + w(v, x)$.\nThe obtained graph after this is called $G_{uv}$.\u003c/p\u003e\n\u003ch2 id=\"claims\"\u003eClaims\u003c/h2\u003e\n\u003cp\u003eAfter collapsing $u$ and $v$ into a single vertex\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf $u$ and $v$ belong to the \u003cstrong\u003esame side of a min cut\u003c/strong\u003e, the capacity of the min cut in $G_{uv}$ is the same as that of $G$.\u003c/li\u003e\n\u003cli\u003eIf $u$ and $v$ belong to \u003cstrong\u003eopposite sides of a min cut\u003c/strong\u003e, the capacity of the min cut in $G_{uv}$ is larger or equal to the capacity of the min cut in $G$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTherefore,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eLet $T_1 = (X_1, Y_1)$ be a min cut in $G_{uv}$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSplit $[uv]$ back into $u$ and $v$, but keep them on the same side of the cut $T_1$. This produces a cut $T_2$ in $G$ of the same capacity as the min cut $T_1$ in $G_{uv}$.\u003c/p\u003e\n\u003cp\u003eThus, the capacity of the min cut in $G$ can only be smaller than the min cut $T_1$ in $G_{uv}$\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"algorithm\"\u003eAlgorithm\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWhile there are more than 2 vertices\u003col\u003e\n\u003cli\u003ePick an edge to with probability proportional to the weight of that edge\n$$P(e(u, v)) = \\frac{w(u, v)}{\\sum_{e(p, q) \\in E} w(p, q)}$$\u003c/li\u003e\n\u003cli\u003eContract the edge, and remove self loops\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTake the capacity of that last edge to be the estimate of the capacity of the min cut in $G$\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"theorems\"\u003eTheorems\u003c/h2\u003e\n\u003cp\u003eLet $M(G)$ represent the min cut capacity of $G$.\u003c/p\u003e\n\u003ch3 id=\"theorem-1\"\u003eTheorem 1\u003c/h3\u003e\n\u003cp\u003eThe probability that the capacity of a min cut in $G_{uv}$ is larger than the capacity of a min cut in $G$ is smaller than $2/n$ where $n = |V|$.\u003c/p\u003e\n\u003cp\u003e$$P(M(G_{uv}) \u0026gt; M(G)) \u0026lt; \\frac{2}{n}.$$\u003c/p\u003e\n\u003cp\u003eThis is because this probability is less than or equal to the probability that the edge $e(u, v)$ belonged in the set of edges along the min cut $M$.\nThe probability of the $e(u, v)$ in $M$ is less than or equal to the final min cut capacity divided by the total capacity of the graph (which is equal to $\\frac{n}{2}$), resulting in the final probability of $\\frac{2}{n}$.\u003c/p\u003e\n\u003ch3 id=\"theorem-2\"\u003eTheorem 2\u003c/h3\u003e\n\u003cp\u003eIf we run edge contraction until there is 1 edge, then the probability $\\pi$ that the capacity of that edge is equal to the capacity of the min cut in G is $\\Omega \\left( \\frac{1}{n^2} \\right)$.\u003c/p\u003e\n\u003cp\u003eFrom the first theorem\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  \\pi\n    \u0026amp;= P(M(G) = M(G_{n-2})) \\\\\n    \u0026amp;= \\prod_{i = 1}^{n - 2} P(M(G_i) = M(G_{i - 1})) \\\\\n    \u0026amp;\\leq \\left(1 - \\frac{2}{n} \\right) \\left(1-\\frac{2}{n-1}\\right) \\left(1-\\frac{2}{n-2}\\right) ... \\left(1-\\frac{2}{3} \\right) \\\\\n    \u0026amp;= \\frac{n - 2}{n} \\times \\frac{n - 3}{n - 1} \\times \\frac{n - 4}{n - 2} \\times ... \\times \\frac{1}{3} \\\\\n    \u0026amp;= \\frac{2}{n(n-1)}.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eSince the contraction runs in $O(n^2)$, and has a $\\Omega \\left( \\frac{1}{n^2} \\right)$ chance of being correct, it needs to be run $\\Theta(n^2)$ times, resulting in a final runtime of $O(n^4)$ to find the min cut.\u003c/p\u003e\n\u003ch2 id=\"kargers-min-cut-refinement\"\u003eKarger\u0026#39;s Min Cut Refinement\u003c/h2\u003e\n\u003cp\u003eThe algorithm can be improved to run in $O(n^2 \\log^3 n)$ with the high probability of being correct through a divide and conquer approach.\u003c/p\u003e\n\u003cp\u003eIf after running the contraction algorithm until there is $\\lfloor \\frac{n}{2} \\rfloor$ vertices runs in $O(n^2)$ and the chance of being correct is $\\approx 1/4$.\nHence, by running the algorithm until there are $\\lfloor \\frac{n}{2} \\rfloor$ vertices 4 times, and then recursing on those smaller graphs, we have a runtime of\u003c/p\u003e\n\u003cp\u003e$$T(n) = 4T\\left(\\frac{n}{2}\\right) + O\\left(n^2\\right) = O(n^2 \\log n).$$\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Python flavoured pseudo code of the refined algorithm\n\ndef karger_refined(G: Graph) -\u0026gt; int:\n    V, E = G\n    # Base case, return the last and only edge weight\n    if len(V) == 2:\n        return E[V[0], V[1]]\n\n    # Run contraction 4 times and recurse on those 4 new graphs\n    min_cuts = [karger_refined(contract(G)) for _ in range(4)]\n    return min(min_cuts)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet $p(n) = P(\\text{success for a graph of size $n$})$, then\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  p(n)\n    \u0026amp;= 1 - P(\\text{failure on one branch})^4 \\\\\n    \u0026amp;= 1 - (1 - P(\\text{success on one branch}))^4 \\\\\n    \u0026amp;= 1 - \\left( 1 - \\frac{1}{4}p \\left(\\frac{n}{2}\\right)\\right)^4 \\\\\n    \u0026amp;\u0026gt; p\\left(\\frac{n}{2}\\right) - \\frac{3}{8}p\\left(\\frac{n}{2}\\right)^2 \\\\\n    \u0026amp;\u0026gt; \\frac{1}{log(n)}.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eIf we run our algorithm $(\\log n)^2$ times, the probability we are correct $\\pi$ is\u003c/p\u003e\n\u003cp\u003e$$\\pi = 1 - \\left(1 - \\frac{1}{\\log n} \\right)^{(\\log n)^2}$$\u003c/p\u003e\n\u003cp\u003eHowever, since for all reasonably large $k$, $(1 - 1/k)^k \\approx e^{-1}$.\nAs a result,\u003c/p\u003e\n\u003cp\u003e$$\\pi \\approx 1 - e^{-\\log n} = 1 - 1/n.$$\u003c/p\u003e\n\u003cp\u003eHence, if we run \u003ccode\u003ekarger_refined\u003c/code\u003e $(\\log n)^2$ times, we have a probability of being correct $1 - 1/n$, with a runtime of\u003c/p\u003e\n\u003cp\u003e$$O\\left(n^2 \\log^3 n \\right) \\lt\\lt O(\\left(n^4\\right).$$\u003c/p\u003e\n\u003ch1 id=\"randomised-hashing\"\u003e\u003ca href=\"#randomised-hashing\"\u003eRandomised Hashing\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eIf the hash function can be analysed, and a sequence of worst keys is chosen, then a lookup in a hash table can take $O(n)$, though ideally we want to have $O(1)$.\u003c/p\u003e\n\u003ch2 id=\"universal-hashing\"\u003eUniversal Hashing\u003c/h2\u003e\n\u003cp\u003eLet $H$ be a finite collection of hash functions that map a given universe $U$ of keys into the smaller range {0 .. m - 1}.\n$H$ is said to be \u003cstrong\u003euniversal\u003c/strong\u003e if for each pair of distinct keys $x, y \\in U$, the number of hash functions $h \\in H$ for which $h(x) = h(y)$ is $\\frac{|H|}{m}$.\u003c/p\u003e\n\u003cp\u003eAssume that a family of hash functions $H$ is universal, and we are hashing $n$ keys into a hash table of size $m$.\nLet $C_x$ be the total number of collisions involving key $x$, then the expected value $E[C_x]$ satisifies\u003c/p\u003e\n\u003cp\u003e$$E[C_x] = \\frac{n - 1}{m}.$$\u003c/p\u003e\n\u003ch3 id=\"designing-a-universal-family-of-hash-functions\"\u003eDesigning a universal family of hash functions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eChoose the size of the hash table $m$ to be a prime number\u003c/li\u003e\n\u003cli\u003eLet $r$ be such that the size $|U|$ of the universe of all keys satisifies $m^r \\leq |U| \\leq m^{r+1}$, i.e. $r = \\lfloor \\log_m |U|\\rfloor$\u003c/li\u003e\n\u003cli\u003eHence, we can represent each key $x$ in base $m$ where\n$$\\vec{x} = \\langle x_0, x_1, ..., x_r\\rangle \\text{ and } x = \\sum_{i=0}^{r}x_i m^i$$\u003c/li\u003e\n\u003cli\u003eLet $\\vec{a} = \\langle a_0, a_1, ..., a_r \\rangle$ be a vector of $r + 1$ \u003cstrong\u003erandomly chosen\u003c/strong\u003e elements from the set $\\{0, 1, ..., m - 1 \\}$\u003c/li\u003e\n\u003cli\u003eDefine a corresponding hash function $h_{\\vec{a}}(x) = \\left( \\sum_{i=0}^{r} x_i a_i \\right) \\mod m$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"proving-universality\"\u003eProving universality\u003c/h3\u003e\n\u003cp\u003eAssume $x, y$ are two distinct keys. Then for there to be a hash collision,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nh_{\\vec{a}}(x) = h_{\\vec{a}}(y)\n    \u0026amp;\\Leftrightarrow \\sum_{i=0}^{r}x_ia_i = \\sum_{i=0}^{r}y_ia_i \\mod m \\\\\n    \u0026amp;\\Leftrightarrow \\sum_{i=0}^{r}(x_i - y_ia_i) = 0 \\mod m\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eSince $x \\neq y$ there exits $0 \\leq k \\leq r$ such that $x_k \\neq y_k$.\nAssume that $x_0 \\neq y_0$, then\u003c/p\u003e\n\u003cp\u003e$$(x_0 - y_0)a_0 = - \\sum_{i=1}^{r}(x_i - y_i)a_i \\mod m$$\u003c/p\u003e\n\u003cp\u003eSince $m$ is a prime, every non-zero element $z \\in \\{0, 1, ..., m - 1\\}$ has a multiplicative inverse $z^{-1}$, such that $z \\cdot z^{-1} = 1 \\mod m$ and so\u003c/p\u003e\n\u003cp\u003e$$a_0 = \\left( - \\sum_{i=0}^{r} (x_i - y_i)a_i \\right) (x_0 - y_0)^{-1} \\mod m$$\u003c/p\u003e\n\u003cp\u003ethis implies that\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efor any 2 keys $x, y$ such that $x_0 \\neq y_0$ and\u003c/li\u003e\n\u003cli\u003efor any randomly chosen $r$ numbers $a_1, a_2, ..., a_r$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ethere exists \u003cstrong\u003eexactly one\u003c/strong\u003e $a_0$ such that $h_{\\vec{a}}(x) = h_{\\vec{a}}(y)$.\u003c/p\u003e\n\u003cp\u003eSince there are\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$m^r$ sequences of the form $\\langle a_{1}, ..., a_r \\rangle$\u003c/li\u003e\n\u003cli\u003e$m^{r+1}$ sequences of the form $\\langle a_0, a_1, ..., a_r \\rangle$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eas a result, the probability to choose a sequence $\\vec{a}$ such that $h_{\\vec{a}}(x) = h_{\\vec{a}}(y)$ is equal to\u003c/p\u003e\n\u003cp\u003e$$\\frac{m^r}{m^{r+1}} = \\frac{1}{m}.$$\u003c/p\u003e\n\u003cp\u003eThus, the family $H$ is a universal collection of hash functions.\u003c/p\u003e\n\u003ch3 id=\"using-universal-family-of-hash-functions\"\u003eUsing universal family of hash functions\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003ePick $r = \\lfloor \\log_m |U| \\rfloor$ ,so that $m^r \\leq |U| \\leq m^{r+1}$\u003c/li\u003e\n\u003cli\u003eFor each run, pick a hash function by randomly picking a vector $\\vec{a} = \\langle a_0, a_1, ..., a_r \\rangle$ such that $0 \\leq a_i \\leq m$, for all $i$, $0 \\leq i \\leq r$.\u003c/li\u003e\n\u003cli\u003eDuring each run use that function on all keys\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"designing-a-perfect-hash-table\"\u003eDesigning a Perfect Hash table\u003c/h2\u003e\n\u003ch3 id=\"first-step\"\u003eFirst step\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGiven $n$ keys we will be constructing hash tables for size $m \u0026lt; 2n^2$ using universal hashing.\nThe probability that such a table is collision free will be $\u0026gt; 1/2$\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePick the smallest prime $m$, such that $n^2 \u0026lt; m \u0026lt; 2n^2$\u003c/li\u003e\n\u003cli\u003ePick a random vector $\\vec{a}$ and hash all keys using the corresponding hash function $h_{\\vec{a}}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eGiven $n$ keys, there will be $n \\choose 2$ pairs of keys.\nBy universality of the family of hash functions used, for each pair of keys the probability of collision is $1/m$.\nSince $m \\geq n^2$ we have $\\frac{1}{m} \\leq \\frac{1}{n^2}$.\nThus, the expected total number of collisions in the table is at most\u003c/p\u003e\n\u003cp\u003e$${n \\choose 2} \\frac{1}{m} \\leq \\frac{n(n - 1)}{2} \\frac{1}{n^2} \u0026lt; \\frac{1}{2}.$$\u003c/p\u003e\n\u003cp\u003eLet $X$ be the random variable equal to no. collisions. Then by the Markov Inequality with $t=1$ we get\u003c/p\u003e\n\u003cp\u003e$$P\\{X \\geq 1\\} \\leq \\frac{E[X]}{1} \u0026lt; \\frac{1}{2}.$$\u003c/p\u003e\n\u003cp\u003eThus, the chance of a collision after $k$ hashes $\u0026lt; (1/2)^k$, which rapidly tends to 0.\nConsequently, after a few random trial-and-error attempts we will obtain a collision free hash table of size $\u0026lt; 2n^2$.\u003c/p\u003e\n\u003cp\u003eIf $p$ is the probability of a collision, then the expected number of trials $E[N]$ before we hit a collision free hash table of size $2n^2$ is\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nE[N]\n  \u0026amp;= 1(1 - p) + 2p(1 - p) + 3p^2(1 - p) + ... \\\\\n  \u0026amp;= (1 - p)(1 + 2p + 3p^2 + ...) \\\\\u003cbr\u003e  \u0026amp;= \\frac{1}{1 - p} \\\\\n  \u0026amp;\u0026lt; 2.\n\\end{align*}\n$$\u003c/p\u003e\n\u003ch3 id=\"second-step\"\u003eSecond step\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eChoose $M$ to be the smallest prime number $\u0026gt; n$\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThus, $n \\leq m \\leq 2n$.\nProduce a hash table of size $M$ again by choosing randomly from a universal family of hash funtions.\nAssume that a slot $i$ of this table has $n_i$ many elements.\nHash these $n_i$ elements into a secondary hash table of size $m_i \u0026lt; 2n_i^2$.\nWe have to guarantee that the sum total of sizes of all secondary hash tables, i.e., $\\sum_{i=1}^{M}m_i$ is linear in $n$.\nNote ${n_i \\choose 2}$ is the number of collisions at $n_i$ and\u003c/p\u003e\n\u003cp\u003e$${n_i \\choose 2} = \\frac{n_i(n_i - 1)}{2} = \\frac{n_i^2}{2} - \\frac{n_i}{2}.$$\u003c/p\u003e\n\u003cp\u003eTherefore the total number of collisions in the hash table is $\\sum_{i=1}^{M} {n_i \\choose 2}$, and since the expected value of collision with universal hashing is $1/M$,\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  E\\left[ \\sum_{i=1}^{M} {n_i \\choose 2} \\right]\n    \u0026amp;= {n \\choose 2}\\frac{1}{M} \\\\\n    \u0026amp;= \\frac{n(n - 1)}{2M} \\\\\n  E\\left[ \\sum_{i=1}^{M} n_i^2 \\right]\n    \u0026amp;= 2E\\left[ \\sum_{i=1}^{M} {n_i \\choose 2} \\right] + n\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eThus,\u003c/p\u003e\n\u003cp\u003e$$E\\left[ \\sum_{i=1}^{M} n^2_i \\right] \\leq \\frac{n(n-1)}{n} + n = 2n - 1 \u0026lt; 2n.$$\u003c/p\u003e\n\u003cp\u003eApplying the Markov Inequality to find the probability we have more than $4n$ items in our hash table, we obtain\u003c/p\u003e\n\u003cp\u003e$$P \\left\\{ \\sum_{i=1}^{M} n_i^2 \u0026gt; 4n \\right\\} \\leq \\frac{E\\left[ \\sum_{i=1}^{M} n_i^2 \\right]}{4n} \u0026lt; \\frac{2n}{4n} = \\frac{1}{2}$$\u003c/p\u003e\n\u003cp\u003eThus, after a few attempts we will produce a hash table of size $M \u0026lt; 2n$ for which $\\sum_{i=1}^{M} \u0026lt; 4n$, and if we choose primes $m_i \u0026lt; 2n_i^2$ then $\\sum_{i=1}^{M} m_i \u0026lt; 8n$.\nIn this way the size of the primary hash table plus the sizes of all secondary hash tables satisfies\u003c/p\u003e\n\u003cp\u003e$M + \\sum_{i=1}^{M}m_i \u0026lt; 2n + 8n = 10n.$\u003c/p\u003e\n\u003ch1 id=\"gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas\"\u003e\u003ca href=\"#gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas\"\u003eGaussian Annulus, Random Projection and Johnson Lindenstrauss Lemmas\u003c/a\u003e\u003c/h1\u003e\n\n\u003ch2 id=\"generating-random-points-in-d-dimensional-spaces\"\u003eGenerating random points in d-dimensional spaces\u003c/h2\u003e\n\u003cp\u003eLet us consider a Gaussian random variable $X$ with a zero mean ($E[X] = \\mu = 0$) and variance $V[X] = v = 1/2\\pi$, then its density is given by\u003c/p\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-8892ba7c776356e02f2d0a849330e257.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cp\u003e$$f_X(x) = \\frac{1}{\\sqrt{2\\pi v}}e^{-\\frac{x^2}{2v}} = e^{-\\pi x^2}$$\u003c/p\u003e\n\u003cp\u003eAssume that we use such $X$ to generate independently the coordinates $\\langle x_1, ..., x_d \\rangle$ of a random vector $\\vec{x}$ from $\\mathbb{R}^d$.\nThen $E[X^2] = E[(X - E[X])^2] = V[X]$, and $E\\left[ \\frac{x_1^2 + ... + x_d^2}{d} \\right] = dV[X]/d = V[X]$.\u003c/p\u003e\n\u003cp\u003eAs a result, given the length $|x| = \\sqrt{x_1^2 + ... + x_d^2}$, the expected value of the square of a the length of $\\vec{x}$ is $E[|x|^2] = d/2\\pi$. So, on average, $|x| \\approx \\frac{\\sqrt{d}}{\\sqrt{2\\pi}} = \\Theta(\\sqrt{d})$, and if $d$ is large, then this is true with a high probability.\u003c/p\u003e\n\u003cp\u003eIf we choose 2 points independently, then\u003c/p\u003e\n\u003cp\u003e$$E[\\langle x, y \\rangle] = E[x_1 y_1 + ... + x_d y_d] = d E[XY] = d E[X] E[Y] = 0.$$\u003c/p\u003e\n\u003cp\u003eHence, the expected value of the scalar product of any 2 vector with randomly chosen coordinates is zero.\u003c/p\u003e\n\u003cp\u003eHence, vectors with randomly chosen coordinates:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ehave approximate the same length $\\Theta(\\sqrt{d})$\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eany 2 such vectors are likely to be almost orthogonal\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"higher-dimensional-balls\"\u003eHigher Dimensional Balls\u003c/h2\u003e\n\u003cp\u003eMost of the volume of a high dimensional ball is near:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAny of its equators (between two close parallel hyper-planes symmetric with respect to the center).\u003cul\u003e\n\u003cli\u003e(Insert lots of math).\nThe ratio between a slice of the sphere $A$ that lies above the hyperplane $x_1 = \\frac{c}{\\sqrt{d - 1}}$ for some constant $c$, and the whole hemisphere satisifies\n$$\n  \\frac{A}{H} \u0026lt; \\frac{\nV(d - 1)\\frac{\n  e^{\\frac{-c^2}{2}}\n}{\n  c \\sqrt{d - 1}\n}\n  }{\nV(d - 1) \\frac{\n  1\n} {\n  2 \\sqrt{d - 1}\n}\n  }\n  = \\frac{2}{c}e^{-\\frac{c^2}{2}}\n$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIt\u0026#39;s surface (if we have a ball of radius $r$, and another smaller, of radius $r(1 - \\epsilon)$, most of the volume of the bigger ball is in the annulus outside the smaller ball).\u003cul\u003e\n\u003cli\u003eThis is because the area of the smaller ball is $(1 - \\epsilon)^d$ and $(1 - \\epsilon) \u0026lt; 1$.\nHence, for large values of $d$ this tends to 0 quickly.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"johnson-lindenstrauss-lemma\"\u003eJohnson-Lindenstrauss Lemma\u003c/h2\u003e\n\u003cp\u003eFor any $\\epsilon$, $0 \u0026lt; \\epsilon \u0026lt; 1$, and any integer $n$, assume that $k$ satisfies $k \u0026gt; \\frac{3}{\\gamma \\epsilon^2} \\ln n$.\nThen for any set of $n$ points given by the vectors $v_1, ..., v_n$ in $\\mathbb{R}^d$, with the probability of at least $1 - 3/(2n)$, the random projection $f\u0026#39; \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ has the property that for ALL pairs of points $v_i, v_j$\u003c/p\u003e\n\u003cp\u003e$$|| f\u0026#39;(v_i - v_j)| - |v_i - v_j|| \\leq \\epsilon | v_i - v_j|.$$\u003c/p\u003e\n\u003cp\u003eThus, $f\u0026#39;(v)$ \u0026quot;almost\u0026quot; preserves distances between points given by vectors $v_i$ despite reduction of dimensionality from $d \u0026gt;\u0026gt; k$ to only $k$.\u003c/p\u003e\n\u003ch2 id=\"application-of-johnson-lindenstrauss-lemma\"\u003eApplication of Johnson-Lindenstrauss Lemma\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eChoose $k$ random vectors by choosing each coordinate of every vector using a unit variance Gaussian\u003c/li\u003e\n\u003cli\u003ePre-process data by projecting to $k \u0026lt; \u0026lt; d$ dimensional subspace spanned by the $k$ random vectors\u003c/li\u003e\n\u003cli\u003eFor comparing new data with current data, map the new vector $y \\in \\mathbb{R}^d$ with its projection $f\u0026#39;(y) = f(y) / \\sqrt{k}$.\u003c/li\u003e\n\u003cli\u003eThen nearest neighbours of $f\u0026#39;(y)$ can be used in hte projected $k$ dimensional space instead of dimension $d \u0026gt; \u0026gt; k$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"page-rank\"\u003e\u003ca href=\"#page-rank\"\u003ePage Rank\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eThe Page Rank algorithm aims to solve the problem of how to order webpages.\u003c/p\u003e\n\u003ch2 id=\"setup\"\u003eSetup\u003c/h2\u003e\n\u003cp\u003eConsider all the webpages $P_i$ on the entire internet as vertices of a directed graph, where a directed edge $P_i \\rightarrow P_j$ exists if $P_i$ has a link to $P_j$.\u003c/p\u003e\n\u003cp\u003eNotation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\rho(P)$ = the rank of a page (to be assigned)\u003c/li\u003e\n\u003cli\u003e$\\#(P)$ = the number of outgoing links on a web page\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA web page $P$ should have a high rank only if it is pointed at by many pages $P_i$ which:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethemselves have a high rank $\\rho(P_i)$\u003c/li\u003e\n\u003cli\u003eand do not point to an excessive number of other web pages, i.e. $\\#P(_i)$ is reasonably small.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo we want the following system of equations to be satisfied:\u003c/p\u003e\n\u003cp\u003e$$\\left\\{\\rho(P) = \\sum_{P_i \\rightarrow P} \\frac{\\rho(P_i)}{\\#(P_i)} \\right\\}_{P \\in WWW}$$\u003c/p\u003e\n\u003cp\u003eWe have a large sparse matrix $G_1$ of size $M \\times M$, where $M = |WWW|$\u003c/p\u003e\n\u003cp\u003e$$\nG_1 =\n\\begin{pmatrix}\n    g(1, 1) \u0026amp; \\ldots \u0026amp; g(1, j) \u0026amp; \\ldots \u0026amp; g(1, M) \\\\\n    \\vdots  \u0026amp; \\ddots \u0026amp; \\vdots  \u0026amp; \\ddots \u0026amp; \\vdots  \\\\\n    g(i, 1) \u0026amp; \\ldots \u0026amp; g(i, j) \u0026amp; \\ldots \u0026amp; g(i, M) \\\\\n    \\vdots  \u0026amp; \\ddots \u0026amp; \\vdots  \u0026amp; \\ddots \u0026amp; \\vdots  \\\\\n    g(M, 1) \u0026amp; \\ldots \u0026amp; g(M, j) \u0026amp; \\ldots \u0026amp; g(M, M)\n\\end{pmatrix}\n\\\n$$\u003c/p\u003e\n\u003cp\u003e$$\ng(i, j) = \u003cbr\u003e\\begin{cases}\n    \\frac{1}{\\#(P_i)} \u0026amp; \\text{ if } P_i \\rightarrow P_j \\\\\n    0 \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eHowever, because $G_1$ is mostly a sparse matrix, it resembles\u003c/p\u003e\n\u003cp\u003e$$\nG_1 =\n\\begin{pmatrix}\n         \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \\\\\n         \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \\\\\n  \\ldots \u0026amp; 0 \u0026amp; \\frac{1}{k} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; \\frac{1}{k} \u0026amp; 0 \u0026amp; \\ldots \\\\\n         \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \\\\\n         \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots      \u0026amp;   \u0026amp;        \\\\\n\\end{pmatrix}\n\\\n$$\u003c/p\u003e\n\u003cp\u003ewhere $k$ is equal to $\\#(P_i)$, the number of pages which the page $P_i$ has links to.\nHence, the system of linear equations can be represented as\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{r}^\\intercal = \\mathbf{r}^\\intercal G_1$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{r}^\\intercal = (\\rho(P_1), \\rho(P_2), ..., \\rho(P_M)).$$\u003c/p\u003e\n\u003cp\u003eNote that $G_1$ is a markov matrix, and $\\mathbf{r}^\\intercal$ is a left-hand eigenvector of $G_1$ corresponding to the eigenvalue 1.\nThus, finding ranks of web pages is reduced to finding eigenvectors of $G_1$, which corresponds to the eigenvalue 1.\u003c/p\u003e\n\u003cp\u003eIf we model a random walk on the graph of this matrix, there are 2 issues:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eWhat should we do when we get to a webpage with no outgoing links?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSolution:\u003c/strong\u003e\nJump to a randomly chosen webpage when a node with no outgoing links.\u003c/p\u003e\n\u003cp\u003eI.e. the first row in $G_1$ is a dangling page, with no outgoing pages.\n$G_2$ fixes this by making it point to all pages with equal probability.\nSuch a matrix is \u003cstrong\u003erow stochastic\u003c/strong\u003e, meaning that each row sums up to 1.\u003c/p\u003e\n\u003cp\u003e$$\n G_1 =\n \\begin{pmatrix}\n      \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \\\\\n   \\ldots \u0026amp; 0 \u0026amp; 0                  \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0                  \u0026amp; 0 \u0026amp; \\ldots \\\\\n      \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \\\\\n   \\ldots \u0026amp; 0 \u0026amp; \\frac{1}{\\#(P_i)} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; \\frac{1}{\\#(P_i)} \u0026amp; 0 \u0026amp; \\ldots \\\\\n      \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \u0026amp;   \u0026amp; \\vdots             \u0026amp;   \u0026amp;        \\\\\n \\end{pmatrix}\n \\\\\n \\\\\n G_2 =\n \\begin{pmatrix}\n      \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;         \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;        \\\\\n   \\ldots \u0026amp; \\frac{1}{M} \u0026amp; \\frac{1}{M}        \u0026amp; \\frac{1}{M} \u0026amp; \\ldots  \u0026amp; \\frac{1}{M} \u0026amp; \\frac{1}{M}        \u0026amp; \\frac{1}{M} \u0026amp; \\ldots \\\\\n      \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;         \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;        \\\\\n   \\ldots \u0026amp; 0           \u0026amp; \\frac{1}{\\#(P_i)} \u0026amp; 0           \u0026amp; \\ldots  \u0026amp; 0           \u0026amp; \\frac{1}{\\#(P_i)} \u0026amp; 0           \u0026amp; \\ldots \\\\\n      \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;         \u0026amp;             \u0026amp; \\vdots             \u0026amp;             \u0026amp;        \\\\\n \\end{pmatrix}\n \\\n$$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf $T$ is the walk length, then as $T \\rightarrow \\infty$, the values $N(P)/T$ should converge.\nThis becomes an issue if there are disconnected graphs or if the graph is bipartite (and the probability of being in one side depends if the path length is odd or even).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSolution:\u003c/strong\u003e\nRandomly jump to a new page after some time.\u003c/p\u003e\n\u003cp\u003eThis transformation does not change the rows corresponding to dangling webpages: $\\alpha / M + (1 - \\alpha) / M = 1/M$\u003c/p\u003e\n\u003cp\u003e$$\n G =\n \\begin{pmatrix}\n      \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;         \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;        \\\\\n   \\ldots \u0026amp; \\frac{1}{M}          \u0026amp; \\frac{1}{M}                               \u0026amp; \\frac{1}{M}          \u0026amp; \\ldots  \u0026amp; \\frac{1}{M}          \u0026amp; \\frac{1}{M}                               \u0026amp; \\frac{1}{M}          \u0026amp; \\ldots \\\\\n      \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;         \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;        \\\\\n   \\ldots \u0026amp; \\frac{1 - \\alpha}{M} \u0026amp; \\frac{\\alpha}{\\#(P_i)} + \\frac{1 - \\alpha}{M} \u0026amp; \\frac{1 - \\alpha}{M} \u0026amp; \\ldots  \u0026amp; \\frac{1 - \\alpha}{M} \u0026amp; \\frac{\\alpha}{\\#(P_i)} + \\frac{1 - \\alpha}{M} \u0026amp; \\frac{1 - \\alpha}{M} \u0026amp; \\ldots \\\\\n      \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;         \u0026amp;                      \u0026amp; \\vdots                                    \u0026amp;                      \u0026amp;        \\\\\n \\end{pmatrix}\n \\\n$$\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese two solutions allow the ranks to converge, because the model works like a well behaved Markov Chain.\u003c/p\u003e\n\u003cp\u003eTo implement the first solution:\u003c/p\u003e\n\u003cp\u003eLet $d$ be 1 at positions $i$ which correspond to dangling webpages and 0 elsewhere\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{d}^\\intercal = (0 \\ ... \\ 0 \\ 1 \\ 0 \\ ... \\ 0 \\ 1 \\ 0 \\ ... \\ 0 \\ 1 \\ 0 \\ ... \\ 0)$$\u003c/p\u003e\n\u003cp\u003eLet $\\mathbf{e}$ be 1 everywhere: $\\mathbf{e}^\\intercal = (1 \\ 1 \\ ... \\ 1)$.\u003c/p\u003e\n\u003cp\u003eThen\n$$G_2 = G_1 + \\frac{1}{M} \\mathbf{d} \\mathbf{e}^\\intercal$$\u003c/p\u003e\n\u003cp\u003eTo implement the second solution as well, we get $G$ as:\u003c/p\u003e\n\u003cp\u003e$$G = \\alpha G_2 + \\frac{1 - \\alpha}{M} \\mathbf{e} \\mathbf{e}^\\intercal = \\alpha \\left( G_1 + \\frac{1}{M} \\mathbf{d} \\mathbf{e}^\\intercal \\right) + \\frac{1 - \\alpha}{M} \\mathbf{e} \\mathbf{e}^\\intercal$$\u003c/p\u003e\n\u003cp\u003e$G$ is no longer sparse, but the vector matrix product $\\mathbf{x}^\\intercal G$ for vectors $\\mathbf{x}$ whose coordinates sum up to 1 can be decomposed as:\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{x}^\\intercal = \\alpha \\mathbf{x}^\\intercal G_1 + \\frac{1}{M} \\left(1 - \\alpha (1 - \\mathbf{x}^\\intercal \\mathbf{d}) \\right) \\mathbf{e}^\\intercal$$\u003c/p\u003e\n\u003ch2 id=\"markov-chains-discrete-time-markov-processes\"\u003eMarkov Chains (Discrete Time Markov Processes)\u003c/h2\u003e\n\u003cp\u003eA (finite) Markov Chain is given by a finite set of states $S = \\{P_i\\}_{i \\leq M}$ and by a row stochastic matrix $G$.\nThe process can start at $t = 0$ in any of its states and continue its evolution by going at every discrete moment of time from its present state to another randomly chosen state.\u003c/p\u003e\n\u003ch3 id=\"simple-example-weather-model\"\u003eSimple Example: Weather Model\u003c/h3\u003e\n\u003cp\u003eConsider a simple 3-state Markov chain representing weather conditions:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eState Transition Diagram:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-58ee9b9d922a50f63d8453209ad43b76.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eTransition Matrix G:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$$\nG = \\begin{pmatrix}\n0.7 \u0026amp; 0.2 \u0026amp; 0.1 \\\\\n0.3 \u0026amp; 0.4 \u0026amp; 0.3 \\\\\n0.2 \u0026amp; 0.3 \u0026amp; 0.5\n\\end{pmatrix}\n$$\u003c/p\u003e\n\u003cp\u003eEach row sums to 1 (row stochastic property), and $G_{ij}$ represents the probability of transitioning from state $i$ to state $j$.\u003c/p\u003e\n\u003cp\u003eThe model of the random walk on the graph is an example of a Markov Chain:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStates are \u0026quot;being at a webpage $P_i$\u0026quot;, so we have in total $M$ states, one for each web page $P_i$, $1 \\leq i \\leq M$.\u003c/li\u003e\n\u003cli\u003eWe start from a randomly chosen starting web page.\u003c/li\u003e\n\u003cli\u003eThus, $q^{(0)}(i) = \\frac{1}{M}$ for all $i$, because all pages are equally likely to be the starting page.\u003c/li\u003e\n\u003cli\u003eIf a webpage $P_i$ is not a dangling webpage, follow a link on the page $P_i$ which points to another webpage $P_j$ with prob $\\frac{\\alpha}{\\#(P_i)}$, or jump directly to a page $P_j$ with probability $\\frac{1 - \\alpha}{M}$.\u003cul\u003e\n\u003cli\u003eHence if a page $P_i$ does not point to a page $P_j$ then $g_{i, j} = \\frac{1 - \\alpha}{M}$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIf $P_i$ is a dangling page then $g_{i, j} = \\frac{1}{M}$ for every page $P_j$ (including the same page $P_i$).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"general-markov-chains\"\u003eGeneral Markov Chains\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe Google matrix induces a strongly connected graph (as there is a directed edge between any two vertices), and so is \u003cstrong\u003eirreducible\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThe Google matrix is \u003cstrong\u003eaperiodic\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eA state $P_i$ in a Markov chain is periodic if there exists integer $K \u0026gt; 1$ s.t. all loops in its underlying graph which contain vertex $P_i$ have length divisible by $K$.\u003c/li\u003e\n\u003cli\u003eMarkov chains which do not have any periodic states are called aperiodic\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor any finite, irreducible and aperiodic Markov chain has the following properties:\u003col\u003e\n\u003cli\u003eFor every initial distribution of states $\\mathbf{q}^{(0)}$ the value of $\\mathbf{q}^{(t)} = \\mathbf{q}^{(0)}G^t$ converges as $t \\rightarrow \\infty$ to a unique stationary distribution $\\mathbf{q}$, i.e., converges to a unique distribution $\\mathbf{q}$ which is independent of $\\mathbf{q}^{(0)}$ and satisfies $\\mathbf{q} = \\mathbf{q}G$.\u003c/li\u003e\n\u003cli\u003eNote: in the above $\\mathbf{q}$ is a row vector, to avoid having to always transpose it.\u003c/li\u003e\n\u003cli\u003eLet $N(P_i, T)$ be the number of times the system has ben in state $P_i$ during $T$ many transitions of such a Markov chain, then\n$$\\lim_{T \\rightarrow \\infty} \\frac{N(P_i, T)}{T}= \\mathbf{q_i}$$\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"application-to-pagerank\"\u003eApplication to PageRank\u003c/h3\u003e\n\u003cp\u003eThe general theorem on Markov chains implies that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1 is the left eigenvalue of $G$ of the largest absolute value, and the \u003cem\u003estationary distribution\u003c/em\u003e $\\mathbf{q}$ is the corresponding left hand side eigenvector, $\\mathbf{q}^\\intercal = \\mathbf{q}^\\intercal G$.\u003c/li\u003e\n\u003cli\u003e$\\mathbf{q}$ is unique, i.e., if $\\mathbf{q}_1^\\intercal = \\mathbf{q}_1^\\intercal G$, then $\\mathbf{q}_1 = \\mathbf{q}$\u003c/li\u003e\n\u003cli\u003eDistribution $\\mathbf{q}$ can be obtained by starting with an arbitrary initial probability distribution $\\mathbf{q}_0$ and obtain $\\mathbf{q}$ as $\\lim_{k \\rightarrow \\infty} \\mathbf{q}_0^\\intercal G^k$\u003c/li\u003e\n\u003cli\u003eAn approximation $\\mathbf{\\tilde{q}}$ of $\\mathbf{q}$ can be obtained by taking $\\mathbf{q}_o^\\intercal = (1/M, 1/M, ..., 1/M)$ and a sufficiently large $K$ and computing $\\mathbf{\\tilde{q}} = \\mathbf{q}_0 G^k$ iteratively\u003c/li\u003e\n\u003cli\u003eThe $i^{th}$ coordinate of such obtained distribution $\\mathbf{q}^\\intercal = (q_1, ..., q_i, ..., q_M)$ roughly gives the ratio $N(P_i, T)/T$ where $N(P_i, T)$ is the number of times $P_i$ has been visited during a surfing session of length $T$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"finding-alpha\"\u003eFinding alpha\u003c/h3\u003e\n\u003cp\u003eHow close to 1 should $\\alpha$ be?\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eexpected\u003c/em\u003e length $lh$ of surfing between two teleportations can be found as:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nE(lh)\n  \u0026amp;= 0(1 - \\alpha) + \\alpha(1 - \\alpha) + ... + k \\alpha^k (1 - \\alpha) + ... \\\\\n  \u0026amp;= \\alpha(1 - \\alpha)(1 + 2 \\alpha + ... + k \\alpha^{k - 1} + ...) \\\\\n  \u0026amp;= \\frac{\\alpha}{1 - \\alpha}.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eGoogle uses $\\alpha = 0.85$ (obtained empirically), giving an expected surfing length $\\frac{0.85}{1- 0.85}\\approx 5.7$, very close to 6 (coming from the idea of \u003cem\u003esix degrees of separation\u003c/em\u003e), and it takes approx $50 - 100$ iterations for convergence.\u003c/p\u003e\n\u003cp\u003eLarger values produce more accurate representation of \u0026quot;importance\u0026quot; of a webpage, but the convergence slows down fast.\u003c/p\u003e\n\u003cp\u003eError of approximation of $\\mathbf{q}$ by $\\mathbf{q_0}G^k$ decreases approximately as $\\alpha^k$.\nMore importantly, increasing $\\alpha$ increases the sensitivity of the resulting PageRank.\nThis is not effective as internet content and structure changes on a daily basis, but PageRank should change slowly.\u003c/p\u003e\n\u003ch1 id=\"hidden-markov-models-and-the-viterbi-algorithm-and-its-applications\"\u003e\u003ca href=\"#hidden-markov-models-and-the-viterbi-algorithm-and-its-applications\"\u003eHidden Markov Models and the Viterbi Algorithm and its applications\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eA Hidden Markov Model is a Markov Model that has two states:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eobservations\u003c/li\u003e\n\u003cli\u003ehidden states\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hidden-markov-model-structure\"\u003eHidden Markov Model Structure\u003c/h3\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-db1f7aa9bab5af519444d5807e669af8.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cp\u003eThe diagram shows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e$\\textbf{S}$\u003c/strong\u003e: Hidden states that follow Markov transitions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e$\\textbf{O}$\u003c/strong\u003e: Observable outcomes/emissions  \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSolid arrows\u003c/strong\u003e: State transition probabilities\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDashed arrows\u003c/strong\u003e: Emission probabilities from hidden states to observations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn example of it\u0026#39;s application, is given a sequence of manifestations, how can we determine what sequence of states caused it?\u003c/p\u003e\n\u003cp\u003eWe do it in a way that maximises the likelihood that we are correct which is what the \u003cstrong\u003eViterbi algorithm\u003c/strong\u003e does. It is a dynamic programming algorithm that produces the most likely estimate.\u003c/p\u003e\n\u003ch2 id=\"hidden-markov-models\"\u003eHidden Markov Models\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIf we are given a finite Markov chain, consisting of a set of its states $S = \\{s_1, s_2, ..., s_K \\}$.\u003c/li\u003e\n\u003cli\u003eWe are also given the initial probabilities $\\pi_1, \\pi_2, ..., \\pi_K$ of states.\u003c/li\u003e\n\u003cli\u003eHowever, we have no access to the direct states, we only have the set of observables $O = \\{o_1, o_2, ..., o_N \\}$ which are the possible manifestations of the states $S = \\{s_1, s_2, ..., s_K \\}$.\u003c/li\u003e\n\u003cli\u003eWe are also given the emission matrix $E$ of size $K \\times N$ where entry $e(i, k)$ represents the probability that when the chain is in state $s_i$, the observable outcome will be $o_k$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ```py\n  [a] -\u003e [b] -\u003e [c] -\u003e [d]\n   ↓      ↓      ↓      ↓\n  [1]    [2]    [3]    [4]\n  / \\\n l   r\n``` --\u003e\n\n\u003ch2 id=\"maximum-likelihood-estimation\"\u003eMaximum Likelihood Estimation\u003c/h2\u003e\n\u003cp\u003eLikelihood is, in a sense, an inverse of probability.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAssume there are $n$ balls labeled 1 to $n$, and you can draw a single ball, look at its value, and have to estimate the value of $n$.\u003c/li\u003e\n\u003cli\u003eAssume you drew the ball numbered $k$, which has a probability of $1 / n$. Therefore you have the highest chance of picking $k$ when $n$ is as small as possible, meaning it $n = k$ (as you know there are at least $k$ balls).\u003c/li\u003e\n\u003cli\u003eIn this case the MLE estimator is $N(X) = X$, and $E(X)$ is given by\n$$\\mu = \\sum_{i=1}^{n}\\left( i \\times \\frac{1}{n} \\right) = \\frac{n(n + 1)}{2n} = \\frac{n + 1}{2}.$$\nThus, this is biased, because the expected value is half of $n$.\u003c/li\u003e\n\u003cli\u003eIf we ues the estimator $Y(X) = 2X - 1$, then the expected value of $Y$ is\n$$\\sum_{i = 1}^{n} \\frac{2i - 1}{n} = \\frac{2\\sum_{i=1}^{n}i}{n} - \\frac{\\sum_{i=1}^n 1}{n} = \\frac{2n(n+1)}{2n} - 1 = n$$\nand this is unbiased.\u003c/li\u003e\n\u003cli\u003eAs the size of the sample increases, ML estimate approaches the best possible estimate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"viterbi-algorithm\"\u003eViterbi Algorithm\u003c/h2\u003e\n\u003cp\u003eAssume we are given a sequence of observable outcomes $(y_1, y_2, y_r)$.\nThe goal is to find the sequence $(x_1, x_2, ..., x_r)$ of states of the Markov chain for which the likelihood that such a sequence is the cause of the observed sequence of outcomes is the highest.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGiven $\\vec{y}$ observed, we could pick the sequence of states $\\vec{x}$ for which the value of $P(\\vec{x}, \\vec{y})$ is the largest.\u003c/li\u003e\n\u003cli\u003eThis is not feasible, as if the total number of states is $K$ and the observed sequence is of length $T$ then there are $K^T$ sequences to try.\u003c/li\u003e\n\u003cli\u003eInstead the Viterbi algorithm solves this problem in $O(T \\times K^2)$ using dynamic programming.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"algorithm-1\"\u003eAlgorithm\u003c/h3\u003e\n\u003cp\u003eWe solve all subproblems $S(i, k)$ for every $i \\leq i \\leq T$ and every $1 \\leq k \\leq K$:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSubproblem\u003c/strong\u003e $S(i, k)$: Find the sequence of states $(x_1, ..., x_i)$ such that $x_i = s_k$ and such that the probability of observing the outcome $(y_1, ..., y_i)$ is maximal.\u003c/p\u003e\n\u003cp\u003eA \u003ca href=\"https://www.youtube.com/watch?v=kqSzLo9fenk\"\u003egood intro to Hidden Markov Models and the Viterbi Algorithm\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"recommender-systems\"\u003eRecommender Systems\u003c/h1\u003e\n\u003cp\u003eThe main purpose of recommender systems is to recommend content / products to users that they may enjoy.\u003c/p\u003e\n\u003cp\u003eTwo major kinds of recommender systems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eContent based:\u003c/strong\u003e items are recommended by their intrinsic similarity\u003cul\u003e\n\u003cli\u003eThis suffers from the problem that content usually has to be done by humans because content is a semantic notion (which computers do not understand well)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCollaborative filtering:\u003c/strong\u003e items are recommended based on some similarity measure between users and between items based on rating of items by the community of users\u003cul\u003e\n\u003cli\u003eThis tends to be superior in performance and does not rely on human advice\u003c/li\u003e\n\u003cli\u003eThere are two main approaches:\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNeighbourhood Method:\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eBased on the similarity of users\u003c/li\u003e\n\u003cli\u003eIf $A$ and $B$ gave similar evaluations to movies that they have both seen, if $A$ liked something $b$ has not seen, then $B$ may like it as well\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLatent Factor Method:\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eBased on the similarity of items\u003c/li\u003e\n\u003cli\u003eAssume two movies $M_1$ and $M_2$ had similar ratings, then if someone liked $M_1$, then it is reasonable to recommend movie $M_2$ to such a user\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe can construct a sparsely populated table of ratings $R$, the rows correspond to movies, the columns to users.\nThe entry $r(i, j)$ of the table, if non empty, represents teh rating user $U_i$ gave to some movie $M_j$\nEach entry may have some rating in range $0 - 5$ (or a similar relatively small rating range, usually with at most 10 or so levels).\u003c/p\u003e\n\u003ch2 id=\"neighbourhood-method\"\u003eNeighbourhood Method\u003c/h2\u003e\n\u003cp\u003eWe replace these rating integers with more informative numbers.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst we compute the mean $\\bar{r}$ of all ratings for all movies\u003c/li\u003e\n\u003cli\u003eThen we obtain $\\bar{R}$ by subtracting $\\bar{r}$ from all values\u003c/li\u003e\n\u003cli\u003eNow if a value $r\u0026#39;(i, j) \u0026gt; 0$ then $U_i$ liked $M_j$ above the global average ($r\u0026#39;(i, j)$ can be positive or negative with equal probability).\nHowever, some ratings may be influenced by \u0026quot;hype\u0026quot; or \u0026quot;trendiness\u0026quot; AKA systematic biases\u0026quot;.\u003c/li\u003e\n\u003cli\u003eTo remove this:\u003cul\u003e\n\u003cli\u003eFor every user $U_i$, introduce $v_i$, the \u0026quot;individual bias\u0026quot; of user $U_i$, reflecting tendency to give overall higher or lower scores.\u003c/li\u003e\n\u003cli\u003eFor every movie $M_j$, introduce $\\mu_j$, the \u0026quot;hype bias\u0026quot; of movie $M_j$ which is how \u0026quot;fashionable\u0026quot; the movie is (which fades with time).\u003c/li\u003e\n\u003cli\u003eBoth systematic biases can be removed by seeking the values of $v_i$ and $\\mu_j$ which minimises the expression\n$$S(\\vec{v}, \\vec{\\mu}) = \\sum_{(i, j) \\in R}(r\u0026#39;(i, j) - v_i - \\mu_j)^2$$\u003c/li\u003e\n\u003cli\u003eNote that $\\mu$\u0026#39;s are constant shifts of rows (each row corresponding to a movie) and $v$\u0026#39;s are constant shifts of columns (each corresponding to a user).\nThis is a \u003cstrong\u003eLeast squares\u003c/strong\u003e problem, and $S(\\vec{v}, \\vec{\\mu})$ achieves a minimum when all the partial derivatives are equal to 0:\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial v_i}S(\\vec{v}, \\vec{\\mu}) \u0026amp;= -2 \\sum_{j:(i, j) \\in R}(r\u0026#39;(j, i) - v_i - \\mu_j) = 0 \\\\\u003cbr\u003e\\frac{\\partial}{\\partial \\mu_j}S(\\vec{v}, \\vec{\\mu}) \u0026amp;= -2 \\sum_{i:(i, j) \\in R}(r\u0026#39;(j, i) - v_i - \\mu_j) = 0 \\\\\u003cbr\u003e\\end{align*}\n$$\u003c/li\u003e\n\u003cli\u003eUnfortunately, Least Squares fits usually suffer from overfitting.\nThe solution for this is called \u003cstrong\u003eregularisation\u003c/strong\u003e: where we introduce a term which penalises for large values of the variables.\nThus instead, we minimise the sum:\n$$S(\\vec{v}, \\vec{\\mu}, \\lambda) = \\sum_{(i, j) \\in R}(r\u0026#39;(i, j) - v_i - \\mu_j)^2 + \\lambda \\left( \\sum_i v_i^2 + \\sum_j \\mu_j^2 \\right)$$\nwhere $\\lambda$ is a suitably chosen small positive constant, usually $10^{-10} \\leq \\lambda \\leq 10^{-2}$ (the optimal value of $\\lambda$ can also be \u0026quot;learned\u0026quot;)\u003c/li\u003e\n\u003cli\u003eWe now obtain from $\\bar{R}$ $\\tilde{R}$ and are ready to estimate similarities of users and movies\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"neighbourhood-method---similarity-of-users\"\u003eNeighbourhood Method - Similarity of users\u003c/h3\u003e\n\u003cp\u003eOne of the most frequently used measure of similarity of users is the \u003cstrong\u003ecosine similarity measure\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIf we want to compare 2 users $U_i$ and $U_k$, we find all movies that both users have ranked and delete all other entries.\nWe obtain two column vectors $\\vec{u}_1$ and $\\vec{v}_k$, and the similarity of the two users is measured by the cosine of the angle between these two vectors.\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  \\text{sim}(U_i, U_k)\n  \u0026amp;= \\cos(u_i, u_k) \\\\\n  \u0026amp;= \\frac{\\langle \\vec{u}_i, \\vec{u}_k \\rangle}{|\\vec{u}_i| \\cdot |\\vec{u}_k|}\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eWe can now predict the rating a user $U_i$ would give to a movie $M_j$ they have not seen as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAmong all users who have seen $M_j$, pick $L$ many users $U_{k_l}$ with $L$ largest values of $|\\text{sim}(U_i, U_{k_l})|$ (i.e. the $L$ top similar and dissimilar values).\u003c/li\u003e\n\u003cli\u003eWe now predict the rating user $U_i$ would give to movie $M_j$ as\n$$\\text{pred}(i, j) = \\bar{r} + v_i + \\mu_j + \\frac{\\sum_{1 \\leq l \\leq L} \\text{sim}(U_i, U_{k_l}) \\tilde{r}(j, k_l)}{\\sum_{1 \\leq l \\leq L} |\\text{sim}(U_i, U_{k_l})|}$$\u003c/li\u003e\n\u003cli\u003eWe then recommend to user $U_i$ movie $M_j$ for which the predicted rating $\\text{pred}(i, j)$ is the highest\u003cul\u003e\n\u003cli\u003eThe \u0026quot;hype factor\u0026quot; $\\mu_j$ is brought back when deciding what to recommend).\u003c/li\u003e\n\u003cli\u003e$v_i$ is constant across movies, so it is insignificant; adding it is mostly for test purposes because it will realistically predict the possible rating of $U_i$ of $M_j$ allowing easy comparison in tests\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"neighbourhood-method---similarity-of-movies\"\u003eNeighbourhood Method - Similarity of movies\u003c/h3\u003e\n\u003cp\u003eWe can in a similar way estimate similarity of movies, working on the columns of $\\tilde{R}$ (instead of rows).\nWe predict the rating user $U_i$ would give to the move $M_j$ as\u003c/p\u003e\n\u003cp\u003e$$\n\\text{pred} =\n\\bar{r} + v_i + \\mu_j + \\frac{\\sum_{1 \\leq l \\leq L} \\text{sim}(M_j, M_{n_l}) \\tilde{r}(n_l, i)}{\\sum_{1 \\leq l \\leq L} |\\text{sim}(M_j, M_{n_l})|}\n$$\u003c/p\u003e\n\u003cp\u003eand recommend the movie $M_j$ that has the highest value of $\\text{pred}(j, i)$.\u003c/p\u003e\n\u003ch2 id=\"latent-factor-method\"\u003eLatent Factor Method\u003c/h2\u003e\n\u003cp\u003eThis method relies on several heuristics\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eThere are features movies posses which appeal to different tastes which determine how much a user likes a movie. I.e. \u0026quot;action\u0026quot;, \u0026quot;comedy\u0026quot;, \u0026quot;romance\u0026quot;, \u0026quot;famous actors\u0026quot;, \u0026quot;special effects\u0026quot;\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eEnumerate these features as $f_1, f_2, ..., f_N$ where $N$ is to the order of a few 10s to a few 100s\nA movie can have each of these features, say $f_i$ to an extent $e_i$, where say $0 \\leq e_i \\leq 10$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eEach movie $M_j$ has a vector $\\vec{e}^j$ of length $N$, and we can form a matrix $F$ s.t. rows of $F$ correspond to movies and columns correspond to features. I.e. if we have $M$ movies:\u003c/p\u003e\n\u003cp\u003e$$\nF =\n\\begin{pmatrix}\n  F_{1, 1}  \u0026amp; \\ldots \u0026amp; F_{1, N} \\\\\n  \\vdots    \u0026amp; \\vdots \u0026amp; \\vdots   \\\\\n  \\vdots    \u0026amp; \\vdots \u0026amp; \\vdots   \\\\\n  \\vdots    \u0026amp; \\vdots \u0026amp; \\vdots   \\\\\n  \\vdots    \u0026amp; \\vdots \u0026amp; \\vdots   \\\\\n  \\vdots    \u0026amp; \\vdots \u0026amp; \\vdots   \\\\\n  F_{M, 1}  \u0026amp; \\ldots \u0026amp; F_{M, N}\n\\end{pmatrix}\n$$\u003c/p\u003e\n\u003cp\u003eresulting in a very tall matrix\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe can associate each user $U_i$ with a column vector $\\vec{l}^i$ s.t. its $m^{th}$ coordinate is a number, $0 \\leq \\vec{l}^i_m \\leq 10$ which represents how much a user likes a feature $f_m$ in a movie.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe can now form a matrix $L$ whose rows correspond to features and columns correspond to users.\n$$\nL =\n\\begin{pmatrix}\n  L_{1, 1}  \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; L_{1, N} \\\\\n  \\vdots    \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\vdots   \\\\\n  L_{M, 1}  \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; L_{M, N}\n\\end{pmatrix}\n$$\nresulting in a very wide matrix\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAssume that we have access to $L$ and $F$.\nThen to predict how much $U_i$ likes $M_j$, we evaluate\u003c/p\u003e\n\u003cp\u003e$$E(j, i) = \\sum_{1 \\leq m \\leq N} (\\vec{e}^j)_m (\\vec{l}^i)_m = \\langle \\vec{e}^j, \\vec{l}^i \\rangle.$$\u003c/p\u003e\n\u003cp\u003eand as a result can generate $E = F \\times L$, resulting in a very large matrix.\nHowever, the issue is that we cannot determine which few dozen features are relevant out of a few hundred features.\u003c/p\u003e\n\u003ch3 id=\"finding-relevant-features\"\u003eFinding relevant features\u003c/h3\u003e\n\u003cp\u003eLet $N$ be the number of features we want (typically $20 \\leq N \\leq 200$), $\\#M$ by the number of movies, and $\\#U$ be the number of users.\u003c/p\u003e\n\u003cp\u003eFill matrices $F$ of size $\\#M \\times N$ and $L$ of size $N \\times \\#U$ with variables $F(j, m)$ and $L(m, i)$ whose values have yet to be determined.\u003c/p\u003e\n\u003cp\u003eThen sole the least squares problem in the variables\n$$\\{ F(j, m): 1 \\leq j \\leq \\#M; 1 \\leq m \\leq N \\} \\cup \\{ L(m, i) : 1 \\leq m \\leq N; 1 \\leq i \\leq \\#U \\}$$\u003c/p\u003e\n\u003cp\u003eminimise\u003c/p\u003e\n\u003cp\u003e$$S(\\vec{F}, \\vec{L}) = \\sum_{(j, i):R(j, i)} \\left( \\sum_{1 \\leq m \\leq N} F(j, m) \\cdot L(m, i) - R(j, i) \\right)^2$$\u003c/p\u003e\n\u003cp\u003eWe can attempt to find the minimum by finding the when the partial derivative of $S(\\vec{F}, \\vec{L})$ is equal to 0.\nHowever:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis results in a huge system of cubic equations that cannot be solved feasibly\u003c/li\u003e\n\u003cli\u003ean optimisation problem is not convex, so search for the optimal solution can end up in a local minimum\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSolution:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eSet all variables $F(j, m)$ to the same value $F^{(0)}(j, m)$ as a median value\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNow solve the following Least Squares problem for the variables\n$$\\{ L(m, i) : 1 \\leq m \\leq N; 1 \\leq i \\leq \\#U \\}$$\nminimise\n$$\\sum_{j, i}: R(j, i) \\left(\\sum{1 \\leq m \\leq N} F^{(0)}(j, m) \\cdot L(m, i) - R(j, i) \\right)^2$$\nwhich is now a system of linear equations as after we find the partials $F^{(0)}(j, m)$ is set to 0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eLet $L^{(0)}(m, i)$ be the solutions to such a Least Squares problem.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe now solve the following Least Squares problem in variables\n$$\\{ F(j, m): 1 \\leq j \\leq \\#M; 1 \\leq m \\leq N \\}$$\nminimise\n$$\\sum_{(j, i):R(j, i)} \\left( \\sum_{1 \\leq m \\leq N} F(j, m) \\cdot L^{(0)}(m, i) - R(j, i) \\right)^2$$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNow, we keep alternating between taking either $\\{ L(m, i) : 1 \\leq m \\leq N; 1 \\leq i \\leq \\#U \\}$ or $\\{ F(j, m): 1 \\leq j \\leq \\#M; 1 \\leq m \\leq N \\}$ as free variables, fixing the values of the other set from the previously obtained solution to the corresponding Least Squares problem.\u003c/p\u003e\n\u003cp\u003eThis method is sometimes called \u003cstrong\u003eMethod of Alternating Projections\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe stop such iterations when\n$$\\sum_{(j. m)}(F^{(k)}(j. m) - F^{(k - 1)}(j. m))^2 + \\sum_{(i. m)}(L^{(k)}(m, i) - L^{(k - 1)}(m, i))^2$$\nbecomes smaller than an accuracy threshold $\\epsilon \u0026gt; 0$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eAfter we obtain the values $F^{(k)}(j, m)$ and $L^{(k)}(m, i)$ from the last iteration $k$, we form teh corresponding matrices $F$ of size $\\#M \\times N$ and $L$ of size $N \\times \\#U$ as\n$$\n\\begin{align*}\n  \\tilde{F} \u0026amp;= \\left( F^{(k)}(j, m) : 1 \\leq j \\leq \\#M; 1 \\leq m \\leq N \\right) \\\\\n  \\tilde{L} \u0026amp;= \\left( L^{(k)}(m, i) : 1 \\leq m \\leq \\#M; 1 \\leq m \\leq \\#U \\right) \\\\\n\\end{align*}\n$$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe finally set $E = \\tilde{F} \\times \\tilde{L}$ as the final matrix of predicted ratings of all movies by all users, where $E(j, i)$ is the prediction of the rating of movie $M_j$ by user $U_i$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eNote we do not know what these latent features actually are\u003c/li\u003e\n\u003cli\u003eHowever, the Latent Factor Method performs remarkably well in many domains\u003c/li\u003e\n\u003cli\u003eThough in a Netflix Challenge competition to design the best recommendation algorithm, the top algorithms were combination of dozens of components / algorithms with empirically tuned parameters\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"clustering-algorithms\"\u003eClustering Algorithms\u003c/h1\u003e\n\u003cp\u003eClustering algorithms are a type of unsupervised learning used in data science.\u003c/p\u003e\n\u003cp\u003eThere are two kinds of clusters:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ecenter - based clusters\u003c/li\u003e\n\u003cli\u003ehigh density clusters\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA good clustering algorithm should be able to handle both kinds.\u003c/p\u003e\n\u003ch2 id=\"data-representation\"\u003eData Representation\u003c/h2\u003e\n\u003cp\u003eThere are two common representations of points\u003c/p\u003e\n\u003ch3 id=\"as-vectors-in-mathbbrd\"\u003eAs vectors in $\\mathbb{R}^d$\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThis is suitable when you have several numerical measurements (and each measurement maps to a dimension).\u003c/li\u003e\n\u003cli\u003eNote $d$ can be extremely large, corresponding to thousands or more, and can be complex to store and handle such data. This is where Johnson - Lindenstrauss Theorem comes to play.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"as-a-weighted-graph\"\u003eAs a weighted graph\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eData points are represented as vertices of the graph\u003c/li\u003e\n\u003cli\u003eThe weights of the edges reflect the degree of similarity (or dissimilarity) of the data points.\nThe distance between two data points $x, y \\in \\mathbb{R}^d$ can be defined as either\n$$d(x, y) = \\sum_{i=1}^d |x_i - y_i| \\quad \\text{or} \\quad d(x, y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}$$\u003c/li\u003e\n\u003cli\u003eIf the scales of the $i^{th}$ and $j^{th}$ coordinates $x_i$ and $x_j$ differ significantly, or if they are not of equal importance, we might consider instead\n$$d(x, y)^2 = \\sum_{i=1}^d w_i(x_i - y_i)^2$$\nwhere the weights $w_i$ are chosen to normalise different variances of $x_i$ and $x_j$ or to encode their relative significances.\u003c/li\u003e\n\u003cli\u003eGraph representation of data is often much more compact than vectors, as it does not suffer from problems of high dimensionality.\u003c/li\u003e\n\u003cli\u003eHowever, the geometry of the data is lost, so the clustering is based only on mutual distances of pairs of points, which is good when clustering is not center based.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"center-based-clustering-algorithms\"\u003eCenter-based Clustering Algorithms\u003c/h2\u003e\n\u003cp\u003eWe assume data points are represented as vectors in $\\mathbb{R}^d$.\u003c/p\u003e\n\u003ch3 id=\"k-center-clustering\"\u003ek-center clustering\u003c/h3\u003e\n\u003cp\u003eFind a partition $C = \\{C_1, ..., C_k \\}$ of a set of data points $A = \\{\\mathbf{a_1}, ..., \\mathbf{a_n} \\}$ into $k$ clusters, with the corresponding centers $\\mathbf{c_1}, ..., \\mathbf{c_k}$ which minimises\u003c/p\u003e\n\u003cp\u003e$$\\Phi(C) = \\max_{j=1}^k \\max_{a \\in C_j} d(\\mathbf{a}, \\mathbf{c_j})$$\u003c/p\u003e\n\u003cp\u003eThis will minimise the radius of the cluster (as the radius of a cluster is the furthest distance from the cluster center).\u003c/p\u003e\n\u003ch3 id=\"k-median-clustering\"\u003ek-median clustering\u003c/h3\u003e\n\u003cp\u003eFind a partition $C = \\{C_1, ..., C_k \\}$ of a set of data points $A = \\{\\mathbf{a_1}, ..., \\mathbf{a_n} \\}$ into $k$ clusters, with the corresponding centers $\\mathbf{c_1}, ..., \\mathbf{c_k}$ which minimises\u003c/p\u003e\n\u003cp\u003e$$\\Phi(C) = \\sum_{j=1}^k \\sum_{a \\in C_j} d(\\mathbf{a}, \\mathbf{c_j})$$\u003c/p\u003e\n\u003cp\u003e$d(\\mathbf{a}, \\mathbf{c}_j)$ can be any distance metric, such as\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$l_1$: $d(\\mathbf{a}, \\mathbf{c}_j) = \\sum_{k=1}^d| (\\mathbf{a})_k - (\\mathbf{c}_j)_k |$\u003c/li\u003e\n\u003cli\u003e$l_2$: $d(\\mathbf{a}, \\mathbf{c}_j) = \\sqrt{\\sum_{k=1}^d ((\\mathbf{a})_k - (\\mathbf{c}_j)_k)^2}$\nIf the distance is the $l_1$ distance, one can show that the coordinates of the optimal centers are the coordinate-wise medians of points in each cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"k-means-clustering\"\u003ek-means clustering\u003c/h3\u003e\n\u003cp\u003eThis is the most frequently used center based clustering algorithm.\nIt is similar to the k-median clustering, except we want to minimise\u003c/p\u003e\n\u003cp\u003e$$\\Phi(C) = \\sum_{j=1}^k \\sum_{a \\in C_j} d(\\mathbf{a}, \\mathbf{c_j})^2$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThis penalises more for larger distances than the k-median clustering\u003c/li\u003e\n\u003cli\u003eThis has other nice properties; for example, if $d(\\mathbf{a}, \\mathbf{c_j})^2 = \\sum_{j=1}^d (a_i - c_{ji})^2$ then $\\mathbf{c_j}$ must be the centroids of the points in their cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"definitions\"\u003eDefinitions\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eFinding the centroid\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSince\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{a_i} = (a_{i1}, ..., a_{id})$$\u003c/p\u003e\n\u003cp\u003elet $\\mathbf{c} = (c_1, ..., c_d)$ with for all $1 \\leq k \\leq d$,\u003c/p\u003e\n\u003cp\u003e$$c_k = (a_{1k} + ... + a_{nk}) / n$$\u003c/p\u003e\n\u003cp\u003eAs a result, $c_k$ is the arithmetic mean of the $k^{th}$ coordinates of all the points $\\{ \\mathbf{a}_1, ..., \\mathbf{a}_n \\}$.\nHence, $\\mathbf{c}$ is called the centroid of the set of points $\\{ \\mathbf{a}_1, ..., \\mathbf{a}_n \\}$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFinding the distance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThen $\\mathbf{c}$ is called the centroid of the set of points $\\{ \\mathbf{a_1}, ..., \\mathbf{a_n} \\}$.\u003c/p\u003e\n\u003cp\u003eWe denote $\\mathbf{x} \\cdot \\mathbf{y}$ the scalar product of vectors $\\mathbf{x}$ and $\\mathbf{y}$ by $|| \\mathbf{x} ||$ the norm of a vector $\\mathbf{x}$, i.e.\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^d x_i y_i \\quad \\text{and} \\quad ||x|| = \\sqrt{\\sum{i=1}^d x_i^2} = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}}$$\u003c/p\u003e\n\u003cp\u003eNote that $|| \\mathbf{x} - \\mathbf{y} ||$ is the Euclidean distance of points $\\mathbf{x}$ and $\\mathbf{y}$:\u003c/p\u003e\n\u003cp\u003e$$|| \\mathbf{x} - \\mathbf{y} || = \\sqrt{\\sum_{i=1}^d(x_i - y_i)^2}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheorem\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet $A = \\{\\mathbf{a_1}, ..., \\mathbf{a_n} \\}$ be a set of points and $\\mathbf{x}$ be another point, all in $\\mathbb{R}^d$. Let also $\\mathbf{c}$ be the centroid of $A$.\nThen\u003c/p\u003e\n\u003cp\u003e$$\\sum_{i=1}^n || \\mathbf{a_i} - \\mathbf{x} ||^2 + n||\\mathbf{c} - \\mathbf{x}||^2$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCorollary\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLet $A = \\{\\mathbf{a_1}, ..., \\mathbf{a_n} \\}$ be a set of points and $\\mathbf{x}$ be another point, all in $\\mathbb{R}^d$.\nThen\u003c/p\u003e\n\u003cp\u003e$$D(x) = \\sum_{i=1}^n || \\mathbf{a_i} - \\mathbf{x} ||^2$$\u003c/p\u003e\n\u003cp\u003eis minimised when $\\mathbf{x}$ is the centroid $\\mathbf{c} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x_i}$.\u003c/p\u003e\n\u003ch3 id=\"implementation\"\u003eImplementation\u003c/h3\u003e\n\u003cp\u003eThus, to find a partition of set of points $A$ into $k$ disjoint components $A = \\bigcup_{i=1}^k A_i$ and $k$ points $\\mathbf{x_1}, ..., \\mathbf{x_k}$ such that the sum\u003c/p\u003e\n\u003cp\u003e$$\\sum{j=1}^k \\sum{\\mathbf{a_i} \\in A_j} || \\mathbf{a_i} - \\mathbf{x_j} ||^2$$\u003c/p\u003e\n\u003cp\u003eis as small as possible, then whatever such an optimal partition $\\{ A_j : 1 \\leq j \\leq k \\}$ might be, the points $\\mathbf{x_j}$ must be the centroids $\\mathbf{c_j}$ of sets $A_j$.\u003c/p\u003e\n\u003cp\u003eHence, finding the $k$ clusters is equivalent to minimising\u003c/p\u003e\n\u003cp\u003e$$\\sum_{m=1}^k \\frac{1}{2|A_m|} \\sum_{\\mathbf{a_i}, \\mathbf{a_j} \\in A_m} || \\mathbf{a_i} - \\mathbf{a_j} ||^2$$\u003c/p\u003e\n\u003ch3 id=\"lloyds-algorithm\"\u003eLloyd\u0026#39;s Algorithm\u003c/h3\u003e\n\u003cp\u003eSolving the optimal k-means clustering is NP hard, so we use approximate algorithms.\u003c/p\u003e\n\u003cp\u003eThe best known approximate k-means clustering algorithm is Lloyd\u0026#39;s algorithm.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart with an initial set of cluster centers $\\{\\mathbf{c}^{(0)}_m : 1 \\leq m \\leq k\\}$\u003c/li\u003e\n\u003cli\u003eCluster all points $\\mathbf{a} \\in A$ into clusters $A_m$ by associating each $\\mathbf{a} \\in A$ with the nearest cluster centre.\u003c/li\u003e\n\u003cli\u003eReplace cluster centres with the centroids of thus obtained clusters.\u003c/li\u003e\n\u003cli\u003eRepeat 2 and 3 until cluster centers (and thus also clusters) stop changing.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAt every round $p$ of its loop, Lloyd\u0026#39;s algorithm reduces the size of\u003c/p\u003e\n\u003cp\u003e$$\\sum_{m=1}^k \\sum_{\\mathbf{a}_j \\in A_m^{(p)}} || \\mathbf{a}_j - \\mathbf{c}_m^{(p)} ||^2$$\u003c/p\u003e\n\u003cp\u003ewhere $A_m^{(p)}$ are the \u0026quot;temporary\u0026quot; clusters and $\\mathbf{c}^{(p)}_m$ is the \u0026quot;temporary\u0026quot; centre of cluster $A_m^{(p)}$ at round $p$ of the loop.\u003c/p\u003e\n\u003cp\u003eHowever, the algorithm may stop at a local minimum and not the global minimum.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe may choose to run this algorithm multiple times and return the best result\u003c/li\u003e\n\u003cli\u003eAnother algorithm called the \u003cstrong\u003eFarthest Traversal k-clustering\u003c/strong\u003e picks a random point $a_q$ from $A$ as the first center, and then pick the furthest point in $A$ from $a_q$ as the second point, and continue picking the next center as the one with the largest minimum distance from the already picked centers\u003cul\u003e\n\u003cli\u003eIf $A$ has a clustering radius of $r$, then this algorithm produces a radius at most $2r$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWe can randomise the selection by picking a point with probability proportional to the shortest distance to one of already picked points\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"wards-algorithm\"\u003eWard\u0026#39;s Algorithm\u003c/h3\u003e\n\u003cp\u003eWards algorithm is a greedy k-means algorithm:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eStart with every point $\\mathbf{a}_i$ in its own cluster.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhile the number of clusters is larger than $k$ repeat:\u003c/p\u003e\n\u003cp\u003eFind two clusters $C$ and $C\u0026#39;$ such that\n$$\\text{cost}(C \\cup C\u0026#39;) - \\text{cost}(C) - \\text{cost}(C\u0026#39;)$$\nis as small as possible and replace them with a single merged cluster $C \\cup C\u0026#39;$ with its centroid as its centre\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"center-based-clustering-algorithms-1\"\u003eCenter-based Clustering Algorithms\u003c/h2\u003e\n\u003cp\u003eThere are several ways to find non centre based clusters.\u003c/p\u003e\n\u003ch3 id=\"similarity-graphs\"\u003eSimilarity Graphs\u003c/h3\u003e\n\u003cp\u003eRather than representing a set of data points $A$ with their locations, we represent them as an undirected weighted graph $G = (V, E)$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe weight $w_{ij}$ of an edge $e = (v_i, v_j)$ is equal to a similarity measure of the data points.\u003cul\u003e\n\u003cli\u003eIf $w_{ij} = 0$ then the vertices $v_i$ and $v_j$ are completely dissimilar points, and so we do not include this edge\u003c/li\u003e\n\u003cli\u003eElse the weight can depend on a decreasing function of the Euclidean distance, i.e.\n$$e^{-\\frac{|| \\mathbf{a}_i - \\mathbf{a}_j ||^2}{2}}$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSince the graph is weighted, the degree $d_i$ of a vertex $v_i$ is defined as\n$$d_i = \\sum_{j=1}^n w_{ij}$$\nThis degree matrix $D$ is a diagonal matrix with degree $d_i$ of vertex $v_i$ on the $i^{th}$ entry of the diagonal of $D$ and zeroes everywhere else\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFrom here there are several ways to cluster the points\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eThe $\\epsilon$-neighbourhood graph\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eWe connect all pairs of vertices $v_i$, $v_j$ such that the distances between the data points $\u0026lt; \\epsilon$.\u003c/li\u003e\n\u003cli\u003eThis distance is usually the Euclidean distance\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe $k$-nearest neighbour graphs\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eThere are two flavours of this\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eUnidirectional k-nearest neighbour graph\u003c/strong\u003e\nConnect $v_i$ with $v_j$ if either $v_j$ is among $k$ nearest neightours of $v_i$ \u003cstrong\u003eor\u003c/strong\u003e vice versa\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMutual k-nearest neighbour graph\u003c/strong\u003e\nConnect $v_i$ with $v_j$ if both $v_j$ is among $k$ nearest neighbours of $v_i$ \u003cstrong\u003eand\u003c/strong\u003e vice versa\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eIn both cases, the edge is then weighted with the degree of similarity of the vertices $v_i$ and $v_j$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe fully connected graphs\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eConnect all pairs of vertices $v_i$ and $v_j$ where the corresponding data points have a strictly positive similarity or similarity higher than some threshold $\\epsilon$.\u003c/li\u003e\n\u003cli\u003eOften we take weights with the formula\n$$w_{ij} = e^{-\\frac{|| \\mathbf{a}_i - \\mathbf{a}_j ||^2 }{2 \\theta^2}}$$\u003c/li\u003e\n\u003cli\u003eHere $\\theta$ is a parameter which determines \u0026quot;the size\u0026quot; of the neighbourhood, namely how fast the similarity decreases as distance increases\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eHowever, there is no simple way to choose a similarity graph, the best way is to try and determine one empirically\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spectral-graph-theory\"\u003eSpectral Graph Theory\u003c/h3\u003e\n\u003cp\u003eRecall that the $n \\times n$ diagonal matrix $D$ has the degrees $d_i$ of vertices $v_i$ on its diagonal, where $d_i = \\sum_{j=1}^n w_{ij}$.\u003c/p\u003e\n\u003cp\u003eThe (unnormalised) graph Laplacian matrix $L$ is defined as\u003c/p\u003e\n\u003cp\u003e$$L = D - W$$\u003c/p\u003e\n\u003cp\u003ewhere $W = (w_{ij})^n_{i, j = 1}$.\u003c/p\u003e\n\u003cp\u003eClearly $L$ is symmetric and does not depend on $w_{ii}$, $1 \\leq i \\leq n$.\nGraph Laplacians are crucial for spectral clustering.\u003c/p\u003e\n\u003cp\u003eA matrix $M$ of size $n \\times n$ is positive semi-definite if for all vectors $f \\in \\mathbb{R}^n$ we have\u003c/p\u003e\n\u003cp\u003e$$f^\\intercal M f \\geq 0$$\u003c/p\u003e\n\u003cp\u003eA symmetric matrix is positive semi-definite iff all of its eigvenvalues are real and non-negative.\u003c/p\u003e\n\u003cp\u003eThe matrix $L = D - W$ has the following properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFor every vector $f \\in \\mathbb{R}^n$,\n$$\n\\begin{align*}\nf^\\intercal L f\n  \u0026amp;= f^\\intercal D f - f^\\intercal W f \\\\\n  \u0026amp;= \\sum_{i=1}^n d_i f_i^2 - \\sum_{ij=1}^n w_{ij} f_i f_j \\\\\n  \u0026amp;= \\frac{1}{2} \\sum_{i, j = 1}^{n} w_{ij} (f_i - f_j)^2\n\\end{align*}\n$$\u003c/li\u003e\n\u003cli\u003e$L$ is a symmetric positive semi-definite matrix\nAs shown from (1), since $w_{ij} \\geq 0$, L satifies $f^\\intercal L f \\geq 0$ for all vectors $f$ and is thus positive semi-definite.\u003c/li\u003e\n\u003cli\u003eThe smallest eigenvalue of $L$ is 0 and its corresponding eigenvector is $\\mathbb{1} = (1, 1, ..., 1)$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e(Missing some Spectral Graph Theory)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSpectral Clustering Algorithm:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eConstruct a similarity graph $G$ by one of the way described and let $W$ be its weighted adjacency matrix\u003c/li\u003e\n\u003cli\u003eCompute the Laplacian $L = D - W$.\u003c/li\u003e\n\u003cli\u003eCompute the $k$ eigenvectors $\\mathbf{e}_1, ..., \\mathbf{e}_k$ of $L$ which correspond to $k$ smallest eigenvalues.\u003c/li\u003e\n\u003cli\u003eLet $E$ be the matrix of size $n \\times k$ containing the eigenvectors $\\mathbf{e}_1, ..., \\mathbf{e}_k$ as columns.\u003c/li\u003e\n\u003cli\u003eFor $i = 1, ..., n$, let $\\mathbf{y}_i$ be the vector corresponding to teh $i^{th}$ row of E\u003c/li\u003e\n\u003cli\u003eCluster points $\\{ \\mathbf{y}_1, ..., \\mathbf{y}_n \\}$ using the k-means algorithm into clusters $C_1, ..., C_k$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e(Missing application of Spectral Clustering as graph partioning)\u003c/p\u003e\n\u003ch1 id=\"dft-dct-convolution\"\u003eDFT, DCT, Convolution\u003c/h1\u003e\n\u003ch2 id=\"dft\"\u003eDFT\u003c/h2\u003e\n\u003cp\u003eFFT is an $O(n \\log n)$ DFT conversion and the IFFT can compute the IDFT in the same runtime.\nThe benefit of finding the DFT or IDFT of something is that it can represent data, in another form which can be more easily manipulated or used.\u003c/p\u003e\n\u003cp\u003e(Missing a ton of theory)\u003c/p\u003e\n\u003ch2 id=\"convolution\"\u003eConvolution\u003c/h2\u003e\n\u003cp\u003eLet $A = \\langle A_0, A_1, ..., A_{n-1} \\rangle$ and $B = \\langle B_0, B_1, ..., B_{m-1} \\rangle$ be two sequences of real or complex numbers.\nWe can now form two associated polynomials $P_A(x)$ and $P_B(x)$ with coefficients given by sequences $A$ and $B$.\nFinding the multiple of these polynomials $P_C(x) = P_A(x) \\cdot P_B(x)$ can be done in $O(m \\times n)$.\nFrom here, we can get the sequence of it\u0026#39;s corresponding coefficients.\nThis sequence of length $m + n - 1$ is the linear convolution of sequences $A$ and $B$.\nHowever, using the FFT algorithm, we can find the linear convolution of these two sequences in time $O((m + n) \\log_2(m + n))$.\u003c/p\u003e\n\u003cp\u003eAn example application is application of Gaussian smoothing to a noisy signal.\u003c/p\u003e\n\u003ch1 id=\"svd\"\u003eSVD\u003c/h1\u003e\n\u003cp\u003eSingular Value Decomposition is a way of representing a very large matrix $A$ as\u003c/p\u003e\n\u003cp\u003e$$A = \\sum_{i=1}^r = \\sigma_i u_i v_i^\\intercal = UDV^\\intercal$$\u003c/p\u003e\n\u003cp\u003e(Insert some more theory)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTo find $\\mathbf{v}_1$ and $\\mathbf{u}_1$:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart with a random vector $\\mathbf{x}$ and normalise it so that $| \\mathbf{x}| = 1$.\u003c/li\u003e\n\u003cli\u003eCompute $\\mathbf{z}_1 = A\\mathbf{x}$; compute $\\mathbf{z}_2 = A^\\intercal z_1$; let $\\rho = |\\mathbf{z}_2|/|\\mathbf{x}|$ and $\\mathbf{x} = \\mathbf{z}_2$\u003c/li\u003e\n\u003cli\u003eRepeat (2) until the difference between $\\rho$\u0026#39;s obtained in two consecutive iterations is smaller than a small threshold $\\epsilon$.\u003c/li\u003e\n\u003cli\u003eSet $\\mathbf{v}_1 = \\mathbf{z}_2 / |\\mathbf{z}_2|$\u003c/li\u003e\n\u003cli\u003eFinally, let $\\sigma_1 = |A\\mathbf{v}_1|$ and $\\mathbf{u}_1 = A \\mathbf{v}_1 / \\sigma_1$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis method is called the \u003cstrong\u003ePower Method\u003c/strong\u003e and it is fast if $A$ and thus also $A^\\intercal$ are sparse.\u003c/p\u003e\n\u003cp\u003eA good \u003ca href=\"https://www.youtube.com/watch?v=gXbThCXjZFM\"\u003eseries on SVD\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"power-transmission-in-cellular-networks\"\u003ePower Transmission in Cellular Networks\u003c/h1\u003e\n\u003cp\u003eThe signal to interference ratio $SIR_i$ at receiver $R_i$ is given by\u003c/p\u003e\n\u003cp\u003e$$SIR_i = \\frac{G_{ii}p_i}{\\sum_{j: j \\neq i}G_{ii}p_j + \\eta_i}$$\u003c/p\u003e\n\u003cp\u003ewhere $\\eta_i$ is the noise received by the receiver $i$ coming from the environment.\n$SIR_i$ determines the capacity of the channel $C_{ii}$.\nEach pair of transmitter $T_i$ and a receiver $R_i$ needs at least some channel capacity to carray information.\nTo achieve this, it needs $SIR_i \\geq \\gamma_i$.\u003c/p\u003e\n\u003cp\u003eWe will see later how $\\gamma_i$ is determined in practice.\nHowever, we are interested in:\u003c/p\u003e\n\u003cp\u003e$$\\text{minimise } \\sum_{j=1}^n p_j$$\u003c/p\u003e\n\u003cp\u003e$$\\text{subject to constraints } \\frac{G_{ii} p_i}{\\sum_{j: j \\neq i}G_{ij} + \\eta_i} \\geq \\gamma_i, \\quad 1 \\leq i \\leq n$$\u003c/p\u003e\n\u003ch2 id=\"matrix-form\"\u003eMatrix Form\u003c/h2\u003e\n\u003cp\u003eFollowing this, we can simplify our original LP problem into\u003c/p\u003e\n\u003cp\u003e$$\\text{minimise } \\sum_{j=1}^n p_j$$\u003c/p\u003e\n\u003cp\u003e$$\\text{subject to constraints } p_i - \\gamma_i \\sum{j:j \\neq i} \\frac{G_{ij}}{G_{ii}} p_j \\geq \\frac{\\gamma_i \\eta_i}{G_{ii}} \\\\ 1 \\leq i, j \\leq n, p_j \u0026gt; 0$$\u003c/p\u003e\n\u003cp\u003eWe can then convert this into a matrix format:\u003c/p\u003e\n\u003cp\u003e$$\\text{minimise } \\mathbf{1}^\\intercal \\mathbf{p}$$\u003c/p\u003e\n\u003cp\u003e$$\\text{subject to constraints } (I - DF)\\mathbf{p} \\geq \\mathbf{v}, \\mathbf{p} \\leq 0$$\u003c/p\u003e\n\u003cp\u003eThis will have a feasible solution if we are not demanding excessively large $\\gamma_i\u0026#39;s$.\u003c/p\u003e\n\u003cp\u003eThis is the case when the spectral radius (largest absolute value of the eigenvalues) of matrix $DF$ $\\rho(DF)$, satisfies $\\rho(DF) \u0026lt; 1$.\u003c/p\u003e\n\u003cp\u003eProof:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf $A$ is square matrix and if $\\rho(A) \u0026lt; 1$ then $A^m \\rightarrow 0$.\u003c/li\u003e\n\u003cli\u003eThis is easy to see if $A$ has $n$ linearly independent eigenvectors, because in this case $A$ can be represented as\n$$A = Q \\Lambda Q^{-1}$$\u003c/li\u003e\n\u003cli\u003eHere the $i^{th}$ column of $Q$ is the eigenvector corresponding to eigenvalue $\\lambda_i$\n$\\Lambda$ is a diagonal matrix with $\\lambda_i$ in the $i^{th}$ column and row and zeroes elsewhere.\nIn such a case\n$$A^k = Q \\Lambda Q^{-1} Q \\Lambda Q^{-1} ... Q \\Lambda Q^{-1} Q \\Lambda Q^{-1} = Q \\Lambda^k Q^{-1}$$\u003c/li\u003e\n\u003cli\u003eSince $\\Lambda^k$ has $\\lambda_i^k$ on the diagonal, if $\\rho(A) \u0026lt; 1$ then clearly $A^k \\rightarrow 0$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMoreover it is easy to see that\u003c/p\u003e\n\u003cp\u003e$$(I - A) \\sum_{i=0}^k A^i = \\sum_{i=0}^k A^i - \\sum_{i=1}^{k+1}A^i = I - A^{k+1}$$\u003c/p\u003e\n\u003cp\u003eThus,\u003c/p\u003e\n\u003cp\u003e$$\\lim_{k \\rightarrow \\infty} \\left( (I - A) \\sum_{i=0}^k A^k \\right) \\lim_{k \\rightarrow \\infty}(I - A^{k + 1}),$$\u003c/p\u003e\n\u003cp\u003ewhich implies\u003c/p\u003e\n\u003cp\u003e$$(I - A) \\sum_{i=0}^\\infty A^i = I.$$\u003c/p\u003e\n\u003cp\u003eThis shows that matrix $I - A$ is invertible and that\u003c/p\u003e\n\u003cp\u003e$$(I - A)^{-1} = \\sum_{i=0}^\\infty A^i$$\u003c/p\u003e\n\u003cp\u003eApplying this to matrix $A = DF$, let $\\mathbf{p}^*$ be give by\u003c/p\u003e\n\u003cp\u003e$$\\mathbf{p^*} = (I - DF)^{-1} \\mathbf{v} = \\sum_{i=0}^\\infty (DF)^i \\mathbf{v}$$\u003c/p\u003e\n\u003cp\u003eThen $(I - DF) \\mathbf{p^*} = \\mathbf{v}$ and the constraint becomes\u003c/p\u003e\n\u003cp\u003e$$(I - DF) \\mathbf{p} \\geq (I - DF) \\mathbf{p^*}$$\u003c/p\u003e\n\u003cp\u003ei.e.\u003c/p\u003e\n\u003cp\u003e$$(I - DF)(\\mathbf{p} - \\mathbf{p^*}) \\geq 0$$\u003c/p\u003e\n","metadata":{"title":"COMP4121","description":"Advanced Algorithms","date":"2021-11-26","mathjax":true,"hljs":true},"toc":[{"id":"statistics","title":"Statistics"},{"id":"order-statistics-quickselect","title":"Order Statistics (QuickSelect)"},{"id":"database-access","title":"Database access"},{"id":"skip-lists","title":"Skip Lists"},{"id":"kargers-min-cut-algorithm","title":"Karger's Min Cut Algorithm"},{"id":"randomised-hashing","title":"Randomised Hashing"},{"id":"gaussian-annulus-random-projection-and-johnson-lindenstrauss-lemmas","title":"Gaussian Annulus, Random Projection and Johnson Lindenstrauss Lemmas"},{"id":"page-rank","title":"Page Rank"},{"id":"hidden-markov-models-and-the-viterbi-algorithm-and-its-applications","title":"Hidden Markov Models and the Viterbi Algorithm and its applications"}]},"__N_SSG":true},"page":"/writings/[slug]","query":{"slug":"comp4121"},"buildId":"ILkuclJ7g-R-hn-4-ve6L","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>