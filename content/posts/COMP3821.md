+++
title = "COMP3821"
description = "Extended Algorithms and Programming Techniques"
type = ["posts","post"]
tags = [
]
date = "2021-04-25"
categories = [
]
series = ["UNSW"]
[ author ]
  name = "Gary Sun"
+++

# Work in progress

The course is split into four topics
1. Divide and Conquer
2. Greedy Algorithms
3. Dynamic Programming
4. Linear Programming and Reductions

However for the sake of organising notes, I've included an extra section to cover knowledge not covered in the course's prerequisite (COMP2521), and split Linear Programming and Reductions into two sections.


# Assumed Knowledge
## Asymptotic Runtime
### $O(n)$, Big O
- Denotes the upper bound of the runtime of an algorithm
- If $f(n) = O(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq f(n) \leq cg(n),  \forall n \geq n_0$
- $f(n) = O(g(n))$ means that $f(n)$ does not grow substantially faster than $g(n)$ because a multiple of $g(n)$ eventually dominates $f(n)$
- Most commonly used as we are concerned with the worst runtime

### $\Theta(n)$, Big Theta
- Denotes a tight bound of the runtime of an algorithm
- $f(n) = \Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$
- Less commonly used than $O(n)$, but more common than $\Omega(n)$

### $\Omega(n)$, Big Omega
- Denotes the lower bound of the runtime of an algorithm
- If $f(n) = \Omega(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq cg(n) \leq f(n),  \forall n \geq n_0$
- $f(n) = \Omega(g(n))$ means that $f(n)$ grows at least as fast as $g(n)$, because $f(n)$ eventually dominates a multiple of $g(n)$
- Least often used as we usually aren't concerned with the best runtime

## Math
<!-- ### Log Rules and Proofs -->

### Polynomial Interpolation (Vandemonde Matrix)
#### From Coefficient to Value Representation
Every polynomial $A(x)$ of degree $n$ is uniquely determined by its values at any $n + 1$ distinct input values $x_0, x_1, \ldots, x_n$:
$$A(x) \leftrightarrow \{(x_0, A(x_0)), (x_1, A(x_1)), \ldots, (x_n, A(x_n))\}$$

For $A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$, these values can be obtained via a matrix multiplication:
$$
\begin{pmatrix}
1      & x_0    & x_0^2  & \ldots & x_0^n  \\\\
1      & x_1    & x_1^2  & \ldots & x_1^n  \\\\
\vdots & \ldots & \ldots & \ddots & \vdots \\\\
1      & x_n    & x_n^2  & \ldots & x_n^n  \\\\
\end{pmatrix}
\begin{pmatrix}
a_0    \\\\
a_1    \\\\
\vdots \\\\
a_n    \\\\
\end{pmatrix} =
\begin{pmatrix}
A(x_0)    \\\\
A(x_1)    \\\\
\vdots    \\\\
A(x_n)    \\\\
\end{pmatrix}
$$
Such a matrix is called the Vandermonde matrix.

#### From Value to Coefficient Representation
It can be shown that if $x_i$ are all distinct, then this matrix is invertible.
Thus, if all $x_i$ are all distinct, given any values $A(x_0), A(x_1), \ldots, A(x_n)$ the coefficients $a_0, a_1, \ldots, a_n$ of the polynomial $A(x)$ are uniquely determined:
$$
\begin{pmatrix}
1      & x_0    & x_0^2  & \ldots & x_0^n  \\\\
1      & x_1    & x_1^2  & \ldots & x_1^n  \\\\
\vdots & \ldots & \ldots & \ddots & \vdots \\\\
1      & x_n    & x_n^2  & \ldots & x_n^n  \\\\
\end{pmatrix}^{-1}
\begin{pmatrix}
A(x_0)    \\\\
A(x_1)    \\\\
\vdots    \\\\
A(x_n)    \\\\
\end{pmatrix} =
\begin{pmatrix}
a_0    \\\\
a_1    \\\\
\vdots \\\\
a_n    \\\\
\end{pmatrix}
$$
#### Complexity of Swapping Representation 
If we fix the inputs $x_0, x_1, \ldots, x_n$ then commuting between a representation of a polynomial $A(x)$ via its coefficients and a representation via its values at these points is done via the following two matrix multiplications, with matrices made from constants, and thus, for fixed inputs $x_0, x_1, \ldots, x_n$, this switch between the two kinds of representations is done in linear time.

### Roots of Unity
#### Representation
Complex numbers $z = a + ib$ can be represented using 
- $modulus$ $|z| = \sqrt{a^2 + b^2}$
- $argument$ $\arg(z)$, which is an angle taking values in $(-\pi, \pi]$ 

and satisfying:
$$z = |z|e^{i \arg(z)} = |z|(\cos(\arg(z)) + i \sin(\arg(z))).$$

and

$$z^n = \left(|z|e^{i \arg(z)}\right)^n = |z|^ne^{in\arg(z)} = |z|^n(n\cos(\arg(z)) + i \sin(n\arg(z))).$$

#### Properties
Roots of unity of order $n$ are complex numbers which satisfy $z^n = 1$.
- If $z^n = |z|^n(\cos(n \arg(z)) + i\sin(n\arg(z))) = 1$ then $|z| = 1$ and $n\arg(z)$ is a multiple of $2\pi$
- Thus, $n\arg( z) = 2\pi k$, i.e. $arg(z) = \frac{2\pi k}{n}$
- We denote $\omega_n = e^{i \frac{2\pi}{n}}$, such that $\omega_n$ is called a primitive root of unity order $n$
- A root of unity $\omega$ of order $n$ is primitive if all other roots of unity of the same order can be obtained as its powers $\omega^k$.

For $\omega_n = e^{i \frac{2\pi}{n}}$ and for all $k$ such that $0 \leq k \leq n - 1$,
$$ (\omega_n^k)^n = ((\omega_n)^k)^n = (\omega_n)^{nk} = ((\omega_n)^n)^k = 1^k = 1.$$

If $k + m \geq n$ then $k + m = n + l$ for $l = (k + m) \mod(n)$ and we have 
$$\omega_n^k \omega_n^m = \omega_n^{k+m} = \omega_n^{n+l} = \omega_n^n \omega_n^l = 1 \cdot \omega_n^l = \omega_n^l \text{ where } 0 \leq l \leq n.$$

Therefore, the product of any two roots of unity of the same order is just another root of unity of the same order (this is not the case for addition, as the sum of two roots of unity is usually not another root of unity).

The Cancellation Lemma: $\omega_{kn}^{km} = \omega_n^m$, and its proof is below
$$\omega_{kn}^{km} = (\omega_{kn})^{km} = \left(e^{i \frac{2\pi}{kn}}\right)^{km} = e^{i \frac{2\pi k m}{k n}} = e^{i \frac{2\pi m}{n}} = \left(e^{i \frac{2\pi}{n}}\right)^m = \omega_n^m.$$

# Divide and Conquer
Divide and conquer algorithms recursively break down a problem into two or more non overlapping subproblems, before merging them together to become easy to solve.

Common examples of Divide and Conquer algorithms include
- Binary Search
- Merge Sort
- Fast Fourier Transform

## Master theorem
### Representation
The time complexity of a divide and conquer algorithm can be represented in the form 
$$T(n) = aT \left( \frac{n}{b} \right) + f(n)$$
where, 
- $T(n)$
  - The divide and conquer algorithm, with an input size of $n$
- $a \geq 1$
  - $a$ is the number of recursive calls per function
  - The constraint implies we make at least 1 recursive call, otherwise there would be no recursion
- $b > 1$
  - $b$ is the decrease in input size per recursive call, the constraint implying that the input size must decrease in each recursive call
  - The constraint implies that the input size must decrease at each recursive call, otherwise we would loop infinitely, never reaching the base case
- $f(n)$
  - The runtime within each function call

### Runtime cases
Using this information, we can find the runtime of most DAQ algorithms through one of the 3 cases where if $\epsilon > 0$ is a constant, then 
1. If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$
2. If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$
3. If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$

### Examples

#### Case 1
If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$ \
Work per subproblem is dwarfed by number of subproblems
- Binary tree DFS | $T(n) = 2T \left( \frac{n}{2} \right) + c$
  - Proof:
    - $n^{log_b a} = n^{\lg 2} = n^{1} = n$
    - $f(n) = c = \Theta(1)$
  - Therefore $T(n) = \Theta(n^{\lg 2}) = \Theta(n)$
- Karatsuba's integer multiplication | $T(n) = 3T \left( \frac{n}{2} \right) + n$
  - Proof
    - $n^{\log_2 3} = n^{\lg 3} \approx n^{1.58}$ 
    - $f(n) = n = \Theta(n)$
  - Therefore $T(n) = \Theta(n^{1.58})$

#### Case 2
If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$ \
Work per subproblem is comparable to nunmber of subproblems
- Binary search | $T(n) = T \left( \frac{n}{2} \right) + c$
  - Proof:
    - $n^{\log_b a} = n^{\lg 1} = n^{0} = 1$
    - $f(n) = c = \Theta(1)$
  - Therefore $T(n) = \Theta(n^{\lg 1} \cdot \Theta(1)) = \Theta(\log n)$
- Merge sort | $T(n) = 2T \left( \frac{n}{2} \right) + cn$
  - Proof:
    - $n^{\log_b a} = n^{\lg 2} = n^{1} = n$
    - $f(n) = cn = \Theta(n)$
  - Therefore $T(n) = \Theta(n^{\lg 2} \cdot \Theta(n)) = \Theta(n \log n)$

#### Case 3
If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$ \
Work per subproblem dominates number of subproblems

## Sample algorithms
### Karatsuba's Fast Integer Multiplication
#### Naive method
Given 2 'big integers', i.e. a number, stored as an array, where the values at an the $i^{th}$ index is the $i^{th}$ digit of the integer, and there are $n$ digits, a sample solution would look like
```python
# Naive solution
def mul(A, B, n, m):
    C = [0] * (n + m)
    for i in range(n):
        for j in range(m):
            A[i + j] += B[i] * Y[j]
    for i in range(n + m - 1):
        C[i + 1] += C[i] // 10
        C[i] %= 10
    return C
```
and hence have a runtime of $\Theta(n^{2})$, where $n$ is the greater number of digits.

#### Karatsuba's method
If we want to multiply $A$ with $B$, we can represent them as 
$$
\begin{align*}
A &= 10^{\frac{n}{2}}A_1 + A_0 \\\\
B &= 10^{\frac{n}{2}}B_1 + B_0.
\end{align*}
$$

With this, we can apply Karatsuba's method, where
$$
\begin{align*}
  x &= A_1B_1 \\\\
  z &= A_0B_0 \\\\
  y &= (A_1 + A_0)(B_1 + B_0) - x - z \\\\
  AB 
    &= 10^{n}x + 10^{\frac{n}{2}}y + z.
\end{align*}
$$
At each function call, there are 3 multiplications being performed (each of which is multiplied by the same algorithm), where the input size is halved in each recursive call, and the addition runs in $O(n)$. Therefore Karatsuba's method has the recurrence 
$$ T(n) = 3T\left(\frac{n}{2}\right) + O(n) = O(n^{\lg 3}). $$

#### Karatsuba's method for polynomials
We can view this representation of integers similar to that of polynomials. Say we are given polynomials $A(x)$, $B(x)$, where
$$A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$$
$$B(x) = b_nx^n + b_{n-1}x^{n-1} + \ldots + b_0.$$
We have $C(x) = A(x) \cdots B(x)$ of degree $2n$
$$C(x) = \sum_{j = 0}^{2n}c_jx^j = A(x)B(x) = \sum_{j=0}^{2n} \left( \sum_{i + k = j}a_ib_k \right)x^j$$
however, we want to find the coefficients of $c_j = \sum_{i+k=j}a_ib_k$ without performing $(n+1)^2$ many multiplications necessary to get all products of the form $a_ib_k$.

### Fast Fourier Transform
The fast Fourier transform (FFT) is an algorithm to calculate the discrete Fourier transform (DFT) of a sequence, or the inverse (IDFT) in $O(n \log n)$.

#### Discrete Fourier Transform
For $\mathbf{a} = \langle a_0, a_1, \ldots a_{n-1} \rangle$ a sequence of $n$ real or complex numbers, we can form the corresponding polynomial $P_A(x) = \sum_{j=0}^{n-1}a_jx^j$, and evaluate it at all complex roots of unity of order n,
$$\forall \ 0 \leq k \leq n - 1, \quad P_A(\omega_n^k) = A_k = \sum_{j=0}^{n-1}a_j\omega_n^{jk}.$$

The DFT of a sequence $\mathbf{a}$ is a sequence $\mathbf{A}$ of the same length.

#### Inverse Discrete Fourier Transform
The IDFT of a sequence $\mathbf{A} = \langle A_0, A_1, \ldots, A_{n-1} \rangle$ is the sequence of values $\mathbf{a} = \langle a_0, a_1, \ldots, a_{n-1} \rangle = \langle \frac{P_a(1)}{n}, \frac{P_a(\omega_n^{-1})}{n}, \frac{P_a(\omega_n^{-2})}{n}, \ldots, \frac{P_a(\omega_n^{1-n})}{n} \rangle$.

We can show that IDFT(DFT($\mathbf{a}$)) = $\mathbf{a}$ and DFT(IDFT($\mathbf{A}$)) = $\mathbf{A}$.

#### Computation of DFT/ IDFT
Brute force computation of the DFT takes $\Theta(n^2)$, same for IDFT. The DFT of a sequence can be computed in $\Theta(n \lg n)$ using the Fast Fourier Transform (as can be the IDFT).


# Greedy Algorithms
Greedy algorithms make the optimal choice at each step to find the overall optimal way to solve the entire problem.

Common examples of Greedy Algorithms include
- Kruksal's algorithm/ Prim's algorithm
- A* path finding algorithm
- Huffman encoding



# Dynamic Programming
Dynamic programming is an optimisation method to solve problems by breaking them into overlapping subproblems.

Common examples of Dynamic Programming include
- Dijkstra's algorithm
- 0 - 1 Knapsack
- Optimal matrix chain multiplication



# Linear Programming
Linear programming is an optimisation method to maximise or minimise a linear function given linear constraints.  x


# Reductions
Reductions are the conversion of one problem to another. An efficient reduction from A to B may be used to show that B is at least as difficult as A.

