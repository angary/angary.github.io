+++
title = "COMP4121"
description = "Advanced Algorithms"
type = ["posts","post"]
tags = []
date = "2021-09-05"
categories = []
series = ["UNSW"]
toc = false
[ author ]
  name = "Gary Sun"
+++

# Stats

## Definitions
- The **probability of an event** is given by a probability distribution function $P$ which assigns numbers to events such that the assigned values are all in the range [0, 1] and, in particular, the probability assigned to the entires set $\Omega$ of all outcomes is 1. 
  
  More over, if $A = \bigcup^{\infty}\_{i = 1} A_i$ and if all $A_i$ are pairwise disjoint sets, then $P (A) = \sum^{\infty}_{i=1} P (A_i)$ must hold.
- **Events $\\{A_i\\}_{1 \leq i \leq n}$ are independent** if

  $$P \left( \bigcap^n_{i = 1} A_i \right) = \prod^n_{i = 1} P (A_i).$$

- **Conditional probability of $A$ given $B$** 

  $$P (A | B) = \frac{P (A \cap B)}{P(B)} = \frac{P(B|A) P(A)}{P(B)}.$$

  $P(A)$ is the **prior probability** of $A$, whilst $P(A|B)$ is the **posterior probability** of $A$.

- The **probability density function (PDF)** of a continuous random variable is a function $f_X(x) \geq 0$ satisfying:
  $$\int^a_{-\infty}f_X(x)dx = P(X \leq a).$$

  This implies that for all $a, b$ such that $b > a$,

  $$P(a \leq X \leq b) = \int^b_a f_X(x)dx.$$
- Two random variables $X, Y$ are **equally distributed** ($X \sim Y$) if they have the same probability distribution function.

## Expectation and Variance of a Random Variable

The **expected value** of a random variable is its mean. Assuming that the expected value converges, the expected value can be calculated as shown below.

| Discrete random variable | Continuous random variable |
| --- | --- |
| $E(X) = \sum_{i = 1}^{\infty} v_i \cdot p_i$ | $E(X) = \int_{-\infty}^{\infty}x \cdot f(x) dx$ | 

The **variance** of a random variable is defined as $V(X) = E(X - E(X))^2$ assuming that both expectations involved are finite; the standard deviation of a random variable $X$ is given by $\sigma = \sqrt{V(X)}$. 

## A few most common random variables

| | |
| --- | --- |
| **Discrete Uniform Distribution** | All $n$ possible outcomes have a probability $\frac{1}{n}$. |
| **Bernoulli Distribution** | Probability of getting a head when tossing a biased coin. Thus, set of all outcomes is $\\{0, 1\\}$, and $P(X = 1) = p$; $P(X = 0) = 1 = p$, for some $0 \leq p \leq 1$. |
| **Binomial Distribution** | Let the experiment be "tossing a biased coin $n$ times", and the random variable $X$ be "number of heads obtained". Then the values of $X$ are $\\{0, 1, ..., n\\}$, $P(X = k) = {n \choose k} p^k (1 - p)^{n - k}$. We denote such a random variable by $\text{Binomial}(n, p)$. Note $\text{Binomial}(n, p) + \text{Binomial}(m, p) = \text{Binomial}(n + m, p)$. | 
| **Poisson Trials** | This generalises the Binomial Distribution by allowing different probabilities at each trial. Thus, the random variable $X = \sum_{i = 1}^n X_i$ where $X_i$ are independent Bernoulli Trials, each with probability of success of $p_i$. |
| **Geometric Distribution**| The distribution of a random variable corresponding to the number of tossing a biased coin you need to perform to get a head. Its values are all positive integers, and $P(X = k) = p(1 - p)^{k - 1}$. | 
| **Poisson Distribution** | A discrete random variable $X$ has the Poisson Distribution with parameter $\lambda$ if $P(X = n) = e^{-\lambda} \frac{\lambda^n}{n!}$ for all integers $n \geq 0$. |
| **Uniform Distribution** | The uniform distribution on an interval $[a, b]$ is given by a constant density on $[a, b]$, i.e., if $X$ has the following density function $f(x)$: $$f(x) = \begin{cases} \frac{1}{b - a} & \text{if } a \leq x \leq b  \\\\ 0 & \text{otherwise} \end{cases} $$ |
| **Normal (Gaussian) Distribution** | $X$ has a normal distribution with the mean $\mu$ and standard deviation $\sigma$, denoted by $X \sim \mathcal{N}(\mu, \sigma^2)$ if $$f(x) = e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$$ |
| **Exponential Distribution** | $X$ has an exponential distribution with a parameter $\beta$, denoted by $X \sim \text{Exp}(\beta)$, if $$f(x) = \frac{1}{\beta}e^{-\frac{x}{\beta}}, \quad \text{for all } x \geq 0.$$ |

## Simple inequalities
$$1 + x \leq e^x \text{ for all } x \in \mathbb{R}$$
$$\left(1 - \frac{1}{n}\right)^n \leq \frac{1}{e} \leq \left(1 - \frac{1}{n} \right)^{n - 1} \text{ for all } n \in \mathbb{N}$$

## Probability Inequalities

| | |
| --- | --- |
| **The Markov Inequality** | Let $X > 0$ be a non-negative random variable. Then for all $t > 0$, $$P(X \geq t) \leq \frac{E(X)}{t}$$ |
| **Chebyshev Inequality** | Let $X > 0$ be a random variable with the expected value $\mu = E(X)$ and standard deviation $\sigma = \sqrt{E((X - \mu)^2)}$. Then for all $\lambda > 0$, $$P(\| X - \mu \| \geq \lambda \sigma) \leq \frac{1}{\lambda^2}$$ | 
| **Chernoff Bound** | Let $X = \sum_{k = 1}^{n} X_k$, where $X_k, 1 \leq k \leq n$, are independent Bernoulli trials with the probability of success $P(X_k = 1) = p_k$, where $0 < p_k < 1$. Thus, $\mu = E(X) = \sum_{k = 1}^{n} p_k$. Let $\sigma > 0$ be any positive real. Then, $$P(X > (1 + \sigma) \mu) < \left( \frac{e^{\sigma}}{(1 + \sigma)^{1 + \sigma}}\right)^{\mu}$$ |

## Order Statistics
Can we find the $i^{th}$ largest or smallest item in linear time?

### Non-Deterministic Algorithm
In the quicksort algorithm, we note that after a partition, the pivot element ends up being in its correct index. Therefore, we can perform a combination of partitioning and binary search to find the item at the ith index.

For example, we partition first using a random pivot, and as a result we will know the index of our pivot.
If the pivot is index $i$, we've found the element.
Else if the pivot is at an index greater than $i$, then we partition the smaller side, else we partition the bigger side and continue until we've found the $i$th element.

```py
# Implementation of rand-select

def partition(A, lo, hi):
    # Partition the array A[lo..hi] and return partition index
    i = lo - 1
    pivot = A[hi]
    for j in range(lo, hi):
        if A[j] <= pivot:
            i += 1
            A[i], A[j] = A[j], A[i]
    A[i + 1], A[hi] = A[hi], A[i + 1]
    return i + 1


def rand_select(A, i, lo, hi):
    # Return the ith largest index in A
    if lo <= hi:
        pi = partition(A, lo, hi)
        if i < pi:
            return rand_select(A, i, lo, pi - 1)
        return rand_select(A, i, pi + 1, hi)
    return A[hi]
```

### Runtime

The runtime can be represented through the recurrence relation $T(n) = T\left( \frac{n}{2} \right) + O(n)$ which results in a $O(n)$ runtime. However, it can be analysed more in-depth statistically.

#### Worst Case

The worst case runtime is $\Theta(n^2)$ which happens when the smallest or largest element is picked as the pivot - resulting in an *unbalanced partition*.

#### Average Case

However (assuming all elements are **distinct**), let us call a partition a *balanced partition* if the ratio between the ratio smaller side and larger side is less than 9 (9 is arbitrary, any small number > 2 would do).

Then the probability of having a balanced partition is 1 - (chance of choosing smallest 1/10 or biggest 1/10) = 1 - 2/10 = 8/10.

Then let us find the expected number of partitions between two consecutive balanced partitions. In general, the probability that you need $k$ partitions to end up with another balanced partition is $\left( \frac{2}{10} \right)^{k - 1} \cdot \frac{8}{10}$.

Thus, the expected number of partitions between two balanced partitions is

$$
\begin{align*}
E 
  &= 1 \cdot \frac{8}{10} + 2 \cdot \left( \frac{2}{10} \right) \cdot \frac{8}{10} + 3 \cdot \left( \frac{2}{10} \right)^2 \cdot \frac{8}{10} + ... \\\\
  &= \frac{8}{10} \cdot \sum_{k = 0}^{\infty}(k + 1) \left( \frac{2}{10} \right)^k \\\\
  &= \frac{8}{10}S.
\end{align*}
$$

- To find $S$, note that

  $$\sum_{k = 0}^{\infty} q^k = \frac{1}{1 - q}.$$

  By differentiating both sides with respect to $q$ we get

  $$\sum_{q = 1}^{\infty} k q^{k - 1} = \frac{1}{(1 - q)^2}.$$

  Substituting $q = \frac{2}{10}$ we get that $S = \left(\frac{10}{8} \right)^2$.

Therefore,

$$E = \frac{8}{10} \cdot \left( \frac{8}{10} \right)^2 = \frac{5}{4}.$$

And so there are only 5/4 partitions between two balanced partitions.

- Note after 1 *balanced partition*, the size of the array is $\leq 9/10 n$, after the second *balanced partition*, it is $\leq (9/10)^2n$ and so on.

Therefore, the total **average** (expected) run time satisfies

$$
\begin{align*}
T(n) 
  &< 5/4n + 5/4 \left( \frac{9}{10} \right)n + 5/4 \left( \frac{9}{10} \right)^2 n + 5/4 \left( \frac{9}{10} \right)^3 n + ... \\\\
  &= \frac{5/4 n}{1 - \frac{9}{10}} \\\\
  &= 12.5n.
\end{align*}
$$

## Database access

Assume that $n$ processes want to access a database, and that the time $t$ is discrete.
If two processes simultaneously request access, there is a conflict and all processes are locked out of access.

Assume that processes cannot communicate with each other on when to access.
One possible for a process to determine if it should access the database at time $t$ is to toss a coin which produces outcome "request access" with probability $p$ and "do not request access" with probability $(1 - p)$.

What should $p$ be to maximise the probability of a successful access to the database for a process at any instant $t$?

### Efficient selection of p

The probability of success of process $i$ at any instant $t$ is

$$P(S(i, t)) = p(1 - p)^{n - 1},$$

because a process $i$ requests access with probability $p$ and the probability that no other process has requested access is $(1 - p)^{n - 1}$.

The extreme points of this is found by solving

$$\frac{d}{dp}P(S(i, t)) = (1 - p)^{n - 1} - p(n - 1)(1 - p)^{n - 2} = 0$$

which gives $p = 1/n$, which gives a probability of success of

$$P(S(i, t)) = p(1 - p)^{n - 1} = \frac{1}{n} \left( 1 - \frac{1}{n} \right)^{n - 1}.$$

To determine the asymptotic behaviour of $P(S(i, t))$, the following two facts are useful; both can be proved by establishing the signs of the relevant first derivatives

1. $\left( 1 - \frac{1}{n} \right)^n$ increases monotonically from $1/4$ to $1/e$ as $n$ increases from 2 to $\infty$.
2. $\left( 1 - \frac{1}{n} \right)^{n - 1}$ decreases monotonically from $1/2$ to $1/e$ as $n$ increases from 2 to $\infty$.

Therefore,

$$\frac{1}{n} \cdot \frac{1}{e} \leq P(S(i, t)) \leq \frac{1}{n} \cdot \frac{1}{2},$$

and hence $P(S(i, t)) = \Theta \left( \frac{1}{n} \right)$.

Thus, the probability of failure after $t$ instances is

$$P(\text{failure after $t$ instants}) = \left( 1 - \frac{1}{n} \left( 1 - \frac{1}{n} \right)^{n - 1} \right)^t$$

and using the second of our two inequalities we get

$$P(\text{failure after $t$ instants}) \approx \left( 1 - \frac{1}{e n}\right)^t.$$

We observe that

| P(failure after $t = en$ instances | P(failure after $t = en \cdot 2\text{ln}(n)$ instances| 
| --- | --- |
| $$\left( 1 - \frac{1}{en}\right)^{en} \approx \frac{1}{e}$$ | $$\left(1 - \frac{1}{en} \right)^{en \cdot 2\text{ln}(n)} \approx \frac{1}{n^2}$$ |

Thus, a small increase in the number of time instants, from $en$ to $en \cdot 2\text{ln}(n)$ caused a dramatic reduction in the probability of failure.

After $en \cdot 2\text{ln}(n)$ instances, the probability of failure of each process is less than $1/n^2$ and since there are $n$ processes, then the probability that at least one process failed is $\leq 1/n$.

Thus after $en \cdot 2 \text{ln}(n)$ instances all processes succeeded to access the database with probability at least $1 - 1/n$.

### Comparison with centralised algorithm

If the processes could communicate, then it would take $n$ instances for all of them to access the database.

If they can't communicate, then the above method will allow them to access the database with probability $1 - 1/n$ time, which is larger only by a relatively small factor of $2e \text{ln}n$.

## Skip Lists
