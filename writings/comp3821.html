<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>COMP3821</title><meta name="next-head-count" content="3"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#ffffff"/><link rel="preload" href="/_next/static/css/f553854d468f6c8c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f553854d468f6c8c.css" data-n-g=""/><link rel="preload" href="/_next/static/css/33bcc2031ebe3241.css" as="style"/><link rel="stylesheet" href="/_next/static/css/33bcc2031ebe3241.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-0ecb9ccfcb6c9b24.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7db202e315b716c1.js" defer=""></script><script src="/_next/static/chunks/996-502a5e68cb1e9d91.js" defer=""></script><script src="/_next/static/chunks/207-81e7acd450235f3f.js" defer=""></script><script src="/_next/static/chunks/pages/writings/%5Bslug%5D-7e316cc9cfef0534.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_buildManifest.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,n='data-theme',s='setAttribute';var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';d[s](n,'dark')}else{d.style.colorScheme = 'light';d[s](n,'light')}}else if(e){d[s](n,e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="Writing_body__d5Mqz"><div class="Writing_layout__X8ptu"><aside class="Writing_toc__C4kbO" aria-label="Table of contents"><h2 class="Writing_tocTitle__8Z0ek">Contents</h2><ul class="Writing_tocList__2Nnoy"><li class="Writing_tocItem__rjAOC"><a href="#misc-knowledge" class="Writing_tocLinkActive__0kigr">Misc Knowledge</a></li><li class="Writing_tocItem__rjAOC"><a href="#divide-and-conquer" class="Writing_tocLink__ztBHG">Divide and Conquer</a></li><li class="Writing_tocItem__rjAOC"><a href="#greedy-algorithms" class="Writing_tocLink__ztBHG">Greedy Algorithms</a></li><li class="Writing_tocItem__rjAOC"><a href="#dynamic-programming" class="Writing_tocLink__ztBHG">Dynamic Programming</a></li><li class="Writing_tocItem__rjAOC"><a href="#linear-programming" class="Writing_tocLink__ztBHG">Linear Programming</a></li><li class="Writing_tocItem__rjAOC"><a href="#reductions" class="Writing_tocLink__ztBHG">Reductions</a></li></ul></aside><aside class="Writing_balanceNav__vSGQh" aria-label="Site navigation"><a class="Writing_siteTitle__O17hB" href="/">Gary Sun // <span class="cn">孫健</span></a><ul class="Writing_siteNavList__zzxAq"><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#about">About</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="https://github.com/angary/">Projects</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#writings">Writings</a></li></ul></aside><div class="Writing_articleHeader__rl9TS"><h1>COMP3821</h1><p class="Writing_description__Rk8wD">Extended Algorithms and Programming Techniques</p><p class="Writing_date__BYvoK">25 April 2021</p></div><div class="Writing_articleBody__A1uFB"><span style="display:block"><article><p>The course is split into four topics</p>
<ol>
<li>Divide and Conquer</li>
<li>Greedy Algorithms</li>
<li>Dynamic Programming</li>
<li>Linear Programming and Reductions</li>
</ol>
<p>However for the sake of organising notes, I&#39;ve included an extra section to cover knowledge not covered in the course&#39;s prerequisite (COMP2521), and split Linear Programming and Reductions into two sections.</p>
<h1 id="misc-knowledge"><a href="#misc-knowledge">Misc Knowledge</a></h1>

<hr>
<h2 id="asymptotic-runtime">Asymptotic Runtime</h2>
<h3 id="on-big-o">$O(n)$, Big O</h3>
<ul>
<li>Denotes the upper bound of the runtime of an algorithm</li>
<li>If $f(n) = O(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq f(n) \leq cg(n),  \forall n \geq n_0$</li>
<li>$f(n) = O(g(n))$ means that $f(n)$ does not grow substantially faster than $g(n)$ because a multiple of $g(n)$ eventually dominates $f(n)$</li>
<li>Most commonly used as we are concerned with the worst runtime</li>
</ul>
<h3 id="thetan-big-theta">$\Theta(n)$, Big Theta</h3>
<ul>
<li>Denotes a tight bound of the runtime of an algorithm</li>
<li>$f(n) = \Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$</li>
<li>Less commonly used than $O(n)$, but more common than $\Omega(n)$</li>
</ul>
<h3 id="omegan-big-omega">$\Omega(n)$, Big Omega</h3>
<ul>
<li>Denotes the lower bound of the runtime of an algorithm</li>
<li>If $f(n) = \Omega(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \leq cg(n) \leq f(n),  \forall n \geq n_0$</li>
<li>$f(n) = \Omega(g(n))$ means that $f(n)$ grows at least as fast as $g(n)$, because $f(n)$ eventually dominates a multiple of $g(n)$</li>
<li>Least often used as we usually aren&#39;t concerned with the best runtime</li>
</ul>
<h2 id="math">Math</h2>
<h3 id="log-identity">Log Identity</h3>
<p>If $a, b, c &gt; 0$ then
$$a^{\log_{b}c} = c^{\log_{b}a}$$
Proof</p>
<p>$$
\begin{align*}
\log_{b}c \cdot \log_{b}a &amp;= log_{b}a \cdot \log_{b}c \\
\log_{b}(a^{\log_{b}c}) &amp;= \log_{b}(c^{\log_{b}a}) \\
a^{log_{b}c} &amp;= c^{\log_{b}a}
\end{align*}
$$</p>
<h3 id="roots-of-unity">Roots of Unity</h3>
<h4 id="representation">Representation</h4>
<p>Complex numbers $z = a + ib$ can be represented using</p>
<ul>
<li>$modulus$ $|z| = \sqrt{a^2 + b^2}$</li>
<li>$argument$ $\arg(z)$, which is an angle taking values in $(-\pi, \pi]$</li>
</ul>
<p>and satisfying:
$$z = |z|e^{i \arg(z)} = |z|(\cos(\arg(z)) + i \sin(\arg(z))).$$</p>
<p>and</p>
<p>$$z^n = \left(|z|e^{i \arg(z)}\right)^n = |z|^ne^{in\arg(z)} = |z|^n(n\cos(\arg(z)) + i \sin(n\arg(z))).$$</p>
<h4 id="properties">Properties</h4>
<p>Roots of unity of order $n$ are complex numbers which satisfy $z^n = 1$.</p>
<ul>
<li>If $z^n = |z|^n(\cos(n \arg(z)) + i\sin(n\arg(z))) = 1$ then $|z| = 1$ and $n\arg(z)$ is a multiple of $2\pi$</li>
<li>Thus, $n\arg( z) = 2\pi k$, i.e. $arg(z) = \frac{2\pi k}{n}$</li>
<li>We denote $\omega_n = e^{i \frac{2\pi}{n}}$, such that $\omega_n$ is called a primitive root of unity order $n$</li>
<li>A root of unity $\omega$ of order $n$ is primitive if all other roots of unity of the same order can be obtained as its powers $\omega^k$.</li>
</ul>
<p>For $\omega_n = e^{i \frac{2\pi}{n}}$ and for all $k$ such that $0 \leq k \leq n - 1$,
$$ (\omega_n^k)^n = ((\omega_n)^k)^n = (\omega_n)^{nk} = ((\omega_n)^n)^k = 1^k = 1.$$</p>
<p>If $k + m \geq n$ then $k + m = n + l$ for $l = (k + m) \mod(n)$ and we have
$$\omega_n^k \omega_n^m = \omega_n^{k+m} = \omega_n^{n+l} = \omega_n^n \omega_n^l = 1 \cdot \omega_n^l = \omega_n^l \text{ where } 0 \leq l \leq n.$$</p>
<p>Therefore, the product of any two roots of unity of the same order is just another root of unity of the same order (this is not the case for addition, as the sum of two roots of unity is usually not another root of unity).</p>
<p>The Cancellation Lemma: $\omega_{kn}^{km} = \omega_n^m$, and its proof is below
$$\omega_{kn}^{km} = (\omega_{kn})^{km} = \left(e^{i \frac{2\pi}{kn}}\right)^{km} = e^{i \frac{2\pi k m}{k n}} = e^{i \frac{2\pi m}{n}} = \left(e^{i \frac{2\pi}{n}}\right)^m = \omega_n^m.$$</p>
<h1 id="divide-and-conquer"><a href="#divide-and-conquer">Divide and Conquer</a></h1>

<hr>
<p>Divide and conquer algorithms recursively break down a problem into two or more non overlapping subproblems, before merging them together to become easy to solve.</p>
<p>Common examples of Divide and Conquer algorithms include</p>
<ul>
<li>Binary Search</li>
<li>Merge Sort</li>
<li>Fast Fourier Transform</li>
</ul>
<h2 id="sample-algorithms">Sample Algorithms</h2>
<h3 id="karatsubas-fast-integer-multiplication">Karatsuba&#39;s Fast Integer Multiplication</h3>
<h4 id="naive-method">Naive method</h4>
<p>Given 2 &#39;big integers&#39;, i.e. a number, stored as an array, where the values at an the $i^{th}$ index is the $i^{th}$ digit of the integer, and there are $n$ digits, a sample solution would look like</p>
<pre><code class="language-py"># Naive method for big integer multiplication

def mul(A: list[int], B: list[int]) -&gt; list[int]:
    n, m = len(A), len(B)
    C = [0] * (n + m)

    # Apply multiplication
    for i in range(n):
        for j in range(m):
            C[i + j] += A[i] * B[j]

    # Add carry amount to next digit and take mod of 10
    for i in range(n + m - 1):
        C[i + 1] += C[i] // 10
        C[i] %= 10

    return C
</code></pre>
<p>and hence have a runtime of $\Theta(n^{2})$, where $n$ is the greater number of digits.</p>
<h4 id="karatsubas-method">Karatsuba&#39;s method</h4>
<p>Say we want to find the solution to the multiplication of
$$P(x) = a_1x + a_0,$$
$$Q(x) = b_1x + b_0,$$
a naive solution will require 4 multiplications</p>
<ol>
<li>$A = a_1 \times b_1$</li>
<li>$B = a_1 \times b_0$</li>
<li>$C = b_1 \times a_1$</li>
<li>$D = b_0 \times a_0$</li>
</ol>
<p>$$P(x)Q(x) = Ax^2 + (B + C)x + D$$
However we know that
$$(a_1x + a_0)(b_1x + b_0) = (a_1b_1 + a_1b_0 + a_0b_1 + a_0b_0) = (A + B + C + D),$$
requiring 2 additions (which can be computed easily) and 1 multiplication. Hence to find the final multiplication, we only need three multiplications</p>
<ol>
<li>$E = (A + B + C + D) = (a_1 + a_0) \times (b_1 + b_0)$</li>
<li>$A = a_1 \times b_1$</li>
<li>$D = b_0 \times a_0$</li>
</ol>
<p>Hence to find the final solution, we can do
$$P(x)Q(x) = Ax^2 + (E - A - D)x + D$$</p>
<h4 id="karatsubas-method-for-integers">Karatsuba&#39;s method for integers</h4>
<p>If we want to multiply $A$ with $B$, we can represent them as</p>
<p>$$
\begin{align*}
A &amp;= 10^{\frac{n}{2}}A_1 + A_0 \\
B &amp;= 10^{\frac{n}{2}}B_1 + B_0.
\end{align*}
$$</p>
<p>With this, we can apply Karatsuba&#39;s method, where</p>
<p>$$
\begin{align*}
  x &amp;= A_1B_1 \\
  z &amp;= A_0B_0 \\
  y &amp;= (A_1 + A_0)(B_1 + B_0) - x - z \\
  AB
    &amp;= 10^{n}x + 10^{\frac{n}{2}}y + z.
\end{align*}
$$</p>
<p>At each function call, there are 3 multiplications being performed (each of which is multiplied by the same algorithm), where the input size is halved in each recursive call, and the addition runs in $O(n)$. Therefore Karatsuba&#39;s method has the recurrence
$$ T(n) = 3T\left(\frac{n}{2}\right) + O(n) = O(n^{\lg 3}). $$</p>
<h4 id="karatsubas-method-for-polynomials">Karatsuba&#39;s method for polynomials</h4>
<p>We can view this representation of integers similar to that of polynomials. Say we are given polynomials $A(x)$, $B(x)$, where
$$A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$$
$$B(x) = b_nx^n + b_{n-1}x^{n-1} + \ldots + b_0.$$
We have $C(x) = A(x) \cdots B(x)$ of degree $2n$
$$C(x) = \sum_{j = 0}^{2n}c_jx^j = A(x)B(x) = \sum_{j=0}^{2n} \left( \sum_{i + k = j}a_ib_k \right)x^j$$
however, we want to find the coefficients of $c_j = \sum_{i+k=j}a_ib_k$ without performing $(n+1)^2$ many multiplications necessary to get all products of the form $a_ib_k$.</p>
<h3 id="fast-fourier-transform">Fast Fourier Transform</h3>
<p>A naive way to multiply two polynomials together would be to multiply each term of the first polynomial with a term of the second polynomial. The runtime of this would be $O(n^2)$, where $n$ is the degree/ number of terms of the polynomials.</p>
<p>Another method would be to convert both polynomial into it&#39;s point value form, multiply those points and convert it back into the polynomial form.
The fast Fourier transform (FFT) is an algorithm to calculate the Discrete Fourier transform (DFT) of a sequence, or the inverse (IDFT) in $O(n \log n)$.</p>
<h3 id="polynomial-interpolation-vandermonde-matrix">Polynomial Interpolation (Vandermonde Matrix)</h3>
<h4 id="from-coefficient-to-value-representation">From Coefficient to Value Representation</h4>
<p>A polynomial $A(x)$ of degree $n$ is uniquely determined by its values at any $n + 1$ distinct input values $x_0, x_1, \ldots, x_n$:
$$A(x) \leftrightarrow \langle(x_0, A(x_0)), (x_1, A(x_1)), \ldots, (x_n, A(x_n))\rangle$$</p>
<p>For $A(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_0$, these values can be obtained via a matrix multiplication:</p>
<p>$$
\begin{pmatrix}
1      &amp; x_0    &amp; x_0^2  &amp; \ldots &amp; x_0^n  \\
1      &amp; x_1    &amp; x_1^2  &amp; \ldots &amp; x_1^n  \\
\vdots &amp; \ldots &amp; \ldots &amp; \ddots &amp; \vdots \\
1      &amp; x_n    &amp; x_n^2  &amp; \ldots &amp; x_n^n  \\
\end{pmatrix}
\begin{pmatrix}
a_0 \\
a_1 \\
\vdots \\
a_n \\
\end{pmatrix} =
\begin{pmatrix}
A(x_0) \\
A(x_1) \\
\vdots \\
A(x_n) \\
\end{pmatrix}
$$</p>
<p>Such a matrix is called the Vandermonde matrix.</p>
<h4 id="from-value-to-coefficient-representation">From Value to Coefficient Representation</h4>
<p>It can be shown that if $x_i$ are all distinct, then this matrix is invertible.
Thus, if all $x_i$ are all distinct, given any values $A(x_0), A(x_1), \ldots, A(x_n)$ the coefficients $a_0, a_1, \ldots, a_n$ of the polynomial $A(x)$ are uniquely determined:</p>
<p>$$
\begin{pmatrix}
1      &amp; x_0    &amp; x_0^2  &amp; \ldots &amp; x_0^n  \\
1      &amp; x_1    &amp; x_1^2  &amp; \ldots &amp; x_1^n  \\
\vdots &amp; \ldots &amp; \ldots &amp; \ddots &amp; \vdots \\
1      &amp; x_n    &amp; x_n^2  &amp; \ldots &amp; x_n^n  \\
\end{pmatrix}^{-1}
\begin{pmatrix}
A(x_0)    \\
A(x_1)    \\
\vdots    \\
A(x_n)    \\
\end{pmatrix} =
\begin{pmatrix}
a_0    \\
a_1    \\
\vdots \\
a_n    \\
\end{pmatrix}
$$</p>
<h4 id="complexity-of-swapping-representation">Complexity of Swapping Representation</h4>
<p>If we fix the inputs $x_0, x_1, \ldots, x_n$ then commuting between a representation of a polynomial $A(x)$ via its coefficients and a representation via its values at these points is done via the following two matrix multiplications, with matrices made from constants, and thus, for fixed inputs $x_0, x_1, \ldots, x_n$, this switch between the two kinds of representations is done in linear time.</p>
<h4 id="discrete-fourier-transform">Discrete Fourier Transform</h4>
<p>For $\mathbf{a} = \langle a_0, a_1, \ldots a_{n-1} \rangle$ a sequence of $n$ real or complex numbers, we can form the corresponding polynomial $P_A(x) = \sum_{j=0}^{n-1}a_jx^j$, and evaluate it at all complex roots of unity of order n,
$$\forall \ 0 \leq k \leq n - 1, \quad P_A(\omega_n^k) = A_k = \sum_{j=0}^{n-1}a_j\omega_n^{jk}.$$</p>
<p>The DFT of a sequence $\mathbf{a}$ is a sequence $\mathbf{A}$ of the same length.</p>
<h4 id="inverse-discrete-fourier-transform">Inverse Discrete Fourier Transform</h4>
<p>The IDFT of a sequence $\mathbf{A} = \langle A_0, A_1, \ldots, A_{n-1} \rangle$ is the sequence of values $\mathbf{a} = \langle a_0, a_1, \ldots, a_{n-1} \rangle = \langle \frac{P_a(1)}{n}, \frac{P_a(\omega_n^{-1})}{n}, \frac{P_a(\omega_n^{-2})}{n}, \ldots, \frac{P_a(\omega_n^{1-n})}{n} \rangle$.</p>
<p>We can show that IDFT(DFT($\mathbf{a}$)) = $\mathbf{a}$ and DFT(IDFT($\mathbf{A}$)) = $\mathbf{A}$.</p>
<h4 id="computation-of-dft-idft">Computation of DFT/ IDFT</h4>
<p>Brute force computation of the DFT takes $\Theta(n^2)$, same for IDFT. The DFT of a sequence can be computed in $\Theta(n \lg n)$ using the FFT (as can be the IDFT).</p>
<h2 id="proofs">Proofs</h2>
<h3 id="proof-of-correctness">Proof of Correctness</h3>
<p>For Divide and Conquer proof of correctness, mathematical induction is used.</p>
<p>Say that we want to prove that an algorithm $A(n)$ is correct.</p>
<ol>
<li>Prove that the base case is correct, i.e. $A(0)$, $A(1)$ is correct (depends on your base case)</li>
<li>Prove induction step<ol>
<li>Assume that $A(k)$ is true, where $k &lt; n$ (Inductive hypothesis)</li>
<li>Therefore, recursive calls are correct as they call $A(k)$ where $k &lt; n$, which is true by the inductive hypothesis</li>
<li>Prove that the conquer part of the algorithm is correct</li>
</ol>
</li>
<li>Hence algorithm is correct for input size $n$</li>
</ol>
<h3 id="proof-of-runtime-master-theorem">Proof of Runtime (Master Theorem)</h3>
<h4 id="representation-1">Representation</h4>
<p>The time complexity of a divide and conquer algorithm can be represented in the form
$$T(n) = aT \left( \frac{n}{b} \right) + f(n)$$
where,</p>
<ul>
<li>$T(n)$<ul>
<li>The divide and conquer algorithm, with an input size of $n$</li>
</ul>
</li>
<li>$a \geq 1$<ul>
<li>$a$ is the number of recursive calls per function</li>
<li>The constraint implies we make at least 1 recursive call, otherwise there would be no recursion</li>
</ul>
</li>
<li>$b &gt; 1$<ul>
<li>$b$ is the decrease in input size per recursive call, the constraint implying that the input size must decrease in each recursive call</li>
<li>The constraint implies that the input size must decrease at each recursive call, otherwise we would loop infinitely, never reaching the base case</li>
</ul>
</li>
<li>$f(n)$<ul>
<li>The runtime within each function call</li>
</ul>
</li>
</ul>
<h4 id="runtime-cases">Runtime cases</h4>
<p>Using this information, we can find the runtime of most DAQ algorithms through one of the 3 cases where if $\epsilon &gt; 0$ is a constant, then</p>
<ol>
<li>If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$</li>
<li>If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$</li>
<li>If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$</li>
</ol>
<h4 id="examples">Examples</h4>
<h5 id="case-1">Case 1</h5>
<p>If $f(n) = O(n^{\log_b a - \epsilon})$, then $T(n) = \Theta(n^{log_b a})$ <br>Work per subproblem is dwarfed by number of subproblems</p>
<ul>
<li>Binary tree DFS | $T(n) = 2T \left( \frac{n}{2} \right) + c$<ul>
<li>Proof:<ul>
<li>$n^{log_b a} = n^{\lg 2} = n^{1} = n$</li>
<li>$f(n) = c = \Theta(1)$</li>
</ul>
</li>
<li>Therefore $T(n) = \Theta(n^{\lg 2}) = \Theta(n)$</li>
</ul>
</li>
<li>Karatsuba&#39;s integer multiplication | $T(n) = 3T \left( \frac{n}{2} \right) + n$<ul>
<li>Proof<ul>
<li>$n^{\log_2 3} = n^{\lg 3} \approx n^{1.58}$</li>
<li>$f(n) = n = \Theta(n)$</li>
</ul>
</li>
<li>Therefore $T(n) = \Theta(n^{1.58})$</li>
</ul>
</li>
</ul>
<h5 id="case-2">Case 2</h5>
<p>If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{log_b a} \cdot \log n)$ <br>Work per subproblem is comparable to number of subproblems</p>
<ul>
<li>Binary search | $T(n) = T \left( \frac{n}{2} \right) + c$<ul>
<li>Proof:<ul>
<li>$n^{\log_b a} = n^{\lg 1} = n^{0} = 1$</li>
<li>$f(n) = c = \Theta(1)$</li>
</ul>
</li>
<li>Therefore $T(n) = \Theta(n^{\lg 1} \cdot \Theta(1)) = \Theta(\log n)$</li>
</ul>
</li>
<li>Merge sort | $T(n) = 2T \left( \frac{n}{2} \right) + cn$<ul>
<li>Proof:<ul>
<li>$n^{\log_b a} = n^{\lg 2} = n^{1} = n$</li>
<li>$f(n) = cn = \Theta(n)$</li>
</ul>
</li>
<li>Therefore $T(n) = \Theta(n^{\lg 2} \cdot \Theta(n)) = \Theta(n \log n)$</li>
</ul>
</li>
</ul>
<h5 id="case-3">Case 3</h5>
<p>If $f(n) = \Omega(n^{\log_b a + \epsilon})$, then $T(n) = \Theta(f(n))$ <br>Work per subproblem dominates number of subproblems</p>
<h1 id="greedy-algorithms"><a href="#greedy-algorithms">Greedy Algorithms</a></h1>

<hr>
<p>Greedy algorithms are often used for optimisation problems (i.e. you want to maximise or minimise some quantity according to a set of constraints) and make the optimal choice at each step to find the overall optimal way to solve the entire problem.</p>
<p>Common examples of Greedy Algorithms include</p>
<ul>
<li>Kruksal&#39;s algorithm/ Prim&#39;s algorithm</li>
<li>A* path finding algorithm</li>
<li>Huffman encoding</li>
</ul>
<h2 id="sample-algorithms-1">Sample Algorithms</h2>
<h3 id="interval-scheduling">Interval Scheduling</h3>
<p>Given a set of $n$ tasks requests with start finish times,</p>
<p>$$R = \{(s_1, f_1), (s_2, f_2), ..., (s_n, f_n)\}$$</p>
<p>we want to find the maximum number of non overlapping tasks that we can complete.</p>
<p>An example of a greedy strategy that can solve this is <strong>Earliest Finish First</strong></p>
<pre><code class="language-py">def eaf(tasks: list[tuple[int, int]]) -&gt; list[tuple[int, int]]:
    # Sort by earliest finish time
    tasks.sort(key=lambda job: job[1])
    schedule = []
    prev_finish = 0

    for (start, finish) in tasks:
        # If the task doesn&#39;t conflict with the previous finish
        if start &gt; prev_finish:
            # Add it to our schedule and update the finish time
            schedule.append((start, finish))
            prev_finish = finish

    return schedule
</code></pre>
<h4 id="runtime">Runtime</h4>
<p>The runtime of this algorithm is $O(n \log n)$ as it is dominated by the time required to sort the jobs, as the following loop runs in $O(n)$ time.</p>
<h4 id="proof-of-correctness-1">Proof of Correctness</h4>
<p>First, we prove the result is correct (i.e. there are no tasks conflicting).</p>
<p>This is true as we only add a new task to the schedule if it does not conflict with previous tasks.</p>
<h4 id="proof-of-optimality">Proof of Optimality</h4>
<p>Secondly, we prove that the result is optimal (i.e. it is indeed the maximum number of non conflicting tasks we can schedule).</p>
<p>Let $O = \{o_1, ..., o_k\}$ be the tasks of an optimal solution listed in increasing order of finish time, and $G = \{g_1,..., g_k \}$ by the tasks of the EFF solution.</p>
<p>Since $O$ is optimal, it must contain at least as many tasks as $G$, hence there must be the first index $j$, where these two schedules differ, such that</p>
<p>$$
\begin{aligned}
O &amp;= \{o_1, ..., o_{j - 1}, o_j, ... \} \\
G &amp;= \{o_1, ..., o_{j - 1}, g_j, ... \}.
\end{aligned}
$$</p>
<p>Since EFF is correct and selects the earliest finish activity time, $g_j$ does not conflict with any earlier activity, and it finishes no later than $o_j$.</p>
<p>Now consider a &quot;greedier&quot; optimal solution $O&#39;$ where we replace $o_j$ with $g_j$ in $O$, such that</p>
<p>$$O&#39; = \{ o_1, ..., o_{j - 1}, g_j, o_{j + 1}, ... \}.$$</p>
<p>Clearly, $O&#39;$ is correct since $g_j$ finishes no later than $x_j$ so therefore there are no conflicts. Therefore, this new schedule has the same number of activities as $O$, and so it is at least as good.</p>
<p>By repeating this process, we will eventually convert $O$ into $G$, without decreasing the number of tasks. Therefore $G$ is optimal.</p>
<h3 id="minimum-spanning-tree-mst">Minimum Spanning Tree (MST)</h3>
<p>Let $G = (V, E)$ be a connected undirected graph.</p>
<p>A Spanning Tree is a subgraph $T = (V, E_T)$ of $G$ such that</p>
<ul>
<li>T does not contain any cycles</li>
<li>T is connected</li>
</ul>
<p>If $G$ is an edge weighted graph, then a MST is a spanning tree of minimum weight.</p>
<h4 id="kruskals-algorithm">Kruskal&#39;s Algorithm</h4>
<ol>
<li>Order the edges $E$ in a non-decreasing order by their weight</li>
<li>Build a tree by adding the lowest weight edge each time</li>
<li>An edge $E_i$ is added at a round $i$ of construction if it does not introduce a cycle</li>
<li>If adding an edge would introduce a cycle, that edge is discarded</li>
<li>This continues until the list of all edges has been exhausted</li>
</ol>
<h5 id="proof-of-correctness-2">Proof of Correctness</h5>
<p>Let $T$ be the output of the algorithm</p>
<ul>
<li>$T$ does not contain any cycle</li>
</ul>
<p>We show by contradiction that $T$ is connected:</p>
<ul>
<li>Assume there are two or more connected components $C_1$ and $C_2$</li>
<li>$G$ is connected, so there are some edges connecting $C_1$ to $C_2$ in $G$</li>
<li>The first of such edges would have been added to $T$ because it would not create any cycle in $T$.</li>
</ul>
<p>Therefore $T$ is a spanning tree</p>
<h5 id="proof-of-optimality-1">Proof of Optimality</h5>
<p>We consider the case where all weights are distinct.</p>
<p>Let $T$ be the output of Kruskal&#39;s algorithm.</p>
<ul>
<li>Consider a spanning tree $T&#39;$ distinct from $T$.</li>
<li>Let $e = \{u, v \}$ be the smallest-weight edge in $T$ that is not in $T&#39;$.</li>
<li>$T&#39;$ is spanning so there exists a path $P$ from $u$ to $v$.</li>
<li>Let $T&#39;&#39; = (V, \{e\} \cup E_{T&#39;} \backslash \{f\})$, which is spanning tree.</li>
<li>$w(e) &lt; w(f)$ because otherwise Kruskal&#39;s algorithm would have added $f$ to $T$ instead of $e$.</li>
<li>Furthermore, $T&#39;&#39;$ weighs less than $T&#39;$, so $T&#39;$ is not an MST.</li>
</ul>
<p>$G$ has an MST and any $T&#39; \neq T$ is not an MST, so $T$ is an MST.</p>
<h4 id="kruskals-vs-prims-algorithm">Kruskal&#39;s vs Prim&#39;s algorithm</h4>
<p>Prim&#39;s algorithm is another greedy algorithm to find the MST of a graph, however it&#39;s differences are listed as below.</p>
<table>
<thead>
<tr>
<th>Kruskal&#39;s algorithm</th>
<th>Prim&#39;s algorithm</th>
</tr>
</thead>
<tbody><tr>
<td>Tree built always remains connected</td>
<td>Tree build usually remains disconnected</td>
</tr>
<tr>
<td>Adds next cheapest edge</td>
<td>Adds next cheapest vertex</td>
</tr>
<tr>
<td>Faster for sparse graphs</td>
<td>Faster for dense graphs</td>
</tr>
</tbody></table>
<h2 id="proofs-1">Proofs</h2>
<p>Whilst greedy algorithms typically have a structure for correctness and optimality, the proof of runtime varies a lot depending on the data structures and algorithms used.</p>
<h3 id="proof-of-correctness-and-optimality">Proof of Correctness and Optimality</h3>
<p>Note that greedy algorithms are typically used to maximise or minimise a quantity under a constraint.</p>
<p>We typically prove for correctness - that it satisfies the constraints of the problem (i.e. in task scheduling, we prove no two tasks overlap).</p>
<p>Secondly prove that the greedy method is achieves the most optimal solution (i.e. it does maximise or minimises a quantity). This can be done multiple ways</p>
<ol>
<li><p><strong>Greedy stays ahead</strong>: This works by showing that according to the measure of optimality, the greedy algorithm is at least as far ahead as the optimal solution during each iteration of the algorithm. It is typically done in 4 steps.</p>
<ol>
<li><strong>Define your solution</strong> Introduce variables to denote the greedy solution $G$ and the optimal solution $O$ (i.e. for task scheduling, it is the set of tasks).</li>
<li><strong>Define your measure</strong> Define a measure of optimality (i.e. for tasks scheduling, it is the number of jobs).</li>
<li><strong>Prove greedy stays ahead</strong> Prove that the measure of optimality for the greedy solution is at least as good as the optimal.</li>
<li><strong>Prove optimality</strong> Because the greedy solution stays ahead, it must produce an optimal solution. This is typically done by contradiction by assuming the greedy solution isn&#39;t optimal and using the fact that greedy stays ahead as a contradiction.</li>
</ol>
</li>
<li><p><strong>Exchange arguments</strong>: This works by showing you can iteratively transform any optimal solution into the result of the greedy algorithm without changing the cost of the optimal solution, thereby proving the greedy solution is optimal.</p>
<ol>
<li><strong>Define your solution</strong> Introduce variables to denote the greedy solution $G$ and the optimal solution $O$ (i.e. for task scheduling, it is the set of tasks).</li>
<li><strong>Compare solutions</strong> Show that $G \neq O$ (i.e. there is some element of $G$ not in $O$. It helps to name this different element).</li>
<li><strong>Exchange pieces</strong> Show how to transform $G$ into $O$, typically by moving element previously defined in $G$ to $O$. Then prove by doing os you did not worsen $O$&#39;s optimality, and therefore have a different optimal solution.</li>
<li><strong>Iterate</strong> Argue by continuing the exchange of pieces, you can turn $G$ into $O$ without reducing optimality, and therefore $G$ is optimal.</li>
</ol>
</li>
</ol>
<h1 id="dynamic-programming"><a href="#dynamic-programming">Dynamic Programming</a></h1>

<hr>
<p>Dynamic programming is a method to find optimal solution to problems, from optimal solutions to subproblem. Because sets of subproblems to solve larger problems overlap, each subproblem can be calculated once, it&#39;s solutions are stored for next future calls.</p>
<p>Common examples of Dynamic Programming include</p>
<ul>
<li>Dijkstra&#39;s algorithm</li>
<li>Integer knapsack</li>
<li>Optimal matrix chain multiplication</li>
</ul>
<h2 id="sample-algorithms-2">Sample Algorithms</h2>
<h3 id="fibonacci">Fibonacci</h3>
<p>Whilst simple this problem showcases the concept of dynamic programming very well.</p>
<p>Create a function <code>fib</code> where given $i$, find the $i^{th}$ fibonacci number. This can be defined with the following mathematical recurrence relation</p>
<p>$$
F_n =
\begin{cases}
0 &amp; \text{if } n = 0 \\
1 &amp; \text{if } n = 1 \\
F_{n - 1} + F_{n - 2} &amp; \text{otherwise}
\end{cases}
$$</p>
<p>or with the following python code</p>
<pre><code class="language-py"># A function to calculate the nth fibonacci number
def fib(n: int) -&gt; int:
    if n == 0:
        return 0
    if n == 1:
        return 1
    return fib(n - 1) + fib(n - 2)
</code></pre>
<p>However, the runtime of this is $O(2^n)$, an exponential runtime, as each function call itself twice until it reaches the base case.</p>
<h4 id="top-down-solution">Top down solution</h4>
<p>In top down dynamic programming, we use the process of <strong>memoisation</strong> (caching previous function call results) to reduce the runtime to $O(n)$, and space complexity of $O(n)$.</p>
<pre><code class="language-py"># Top down solution to find the nth fibonacci number
def fib(n: int) -&gt; int:
    # Create memoisation table and add base cases
    memo = [-1] * n
    memo[0] = 0
    memo[1] = 1
    return dp(n, memo)

def dp(n: int, memo: list[int]) -&gt; int:
    # If we have not calculated the value before
    if memo[n] == -1:
        memo[n] = dp(n - 1, memo) + dp(n - 2, memo)

    # Now we can simply return the stored value in O(1)
    return memo[n]
</code></pre>
<h4 id="bottom-up-solution">Bottom up solution</h4>
<p>In bottom up DP, we write an iterative solution to compute the value of every subproblem to reduce the runtime to $O(n)$ and space complexity of $O(1)$.</p>
<pre><code class="language-py"># Bottom up solution to find the nth fibonacci number
def fib(n: int) -&gt; int:
    # Base Case
    if n == 0:
        return 0
    if n == 1:
        return 1

    # Iterate up to the nth fibonacci number
    a, b = 0, 1
    for i in range(2, n + 1):
        temp = a + b
        a = b
        b = temp
    return b
</code></pre>
<h3 id="integer-knapsack">Integer Knapsack</h3>
<p>Given a set of $n$ items with weights and values,</p>
<p>$$I = \{ (v_1, w_1), (v_2, w_2), ..., (v_n, w_n) \},$$</p>
<p>and a knapsack which has a weight capacity of $c$, choose a combination of items, maximising the value of the items while ensuring the total weight is less than $c$.</p>
<p>We can define the following recurrence relation</p>
<p>$$
F(i, c) =
\begin{cases}
0 &amp; \text{if } i = 0 \text{ or } c \leq 0 \\
\max(F(i - 1, c - w_i) + v_i, F(i - 1, c)) &amp; \text{otherwise}
\end{cases}
$$</p>
<p>where</p>
<ul>
<li>$i$ is the index of the current item</li>
<li>$c$ is the current capacity</li>
</ul>
<p>which has the base case that we return 0 if we have no items or we are out of capacity, else we start at an item, and then consider the maximum value we can get if:</p>
<ol>
<li><strong>we include it in the knapsack</strong> in which case, we increase the value we get by the value of the current item, reduce the capacity by the weight of the current item, and then consider what to do with the next item.</li>
<li><strong>we do not include it in the knapsack</strong> in which case, we do nothing, and merely consider what to do with the next item.</li>
</ol>
<p>The runtime of this is unfortunately $O(2^n)$ (since each subproblem makes two recursive calls and it has a maximum depth of $n$), however since there are recalculated subproblems (i.e. when $i$ and $c$ is the same), we can use dynamic programming.</p>
<h3 id="memoisation">Memoisation</h3>
<p>In calculating the $n^{th}$ fibonacci value, there was only one parameter changed in repeated subproblems, therefore we only needed a $n$ sized 1D memoisation table.</p>
<p>However, for the integer knapsack problem, the value of each subproblem is dependent on both $i$ and $c$ so therefore we need a 2D $n \times c$ memoisation table.</p>
<pre><code class="language-py"># Bottom up solution for the integer knapsack problem
def knapsack(items: list[tuple[int, int]], c: int) -&gt; int:
    n = len(items)
    memo = [[0] * (n + 1) for _ in range(c + 1)]

    # Build table in bottom up manner
    for i in range(n + 1):
        for w in range(c + 1):
            i_value, i_weight = items[i]
            # If no more items or remaining capacity is 0
            if i == 0 or w == 0:
                memo[i][w] = 0
            # If we can have the new item without going over capacity
            elif items[i][1] &lt;= w:
                include_v = memo[i - 1][w - i_weight] + i_value
                ignore_v = memo[i - 1][w]
                # Choose the option that gives the most value
                items[i][w] = max(include_v, ignore_v)
            # If we cannot fit any more items
            else:
                memo[i][w] = memo[i - 1][w]

    return memo[n][c]
</code></pre>
<p>Because at <code>memo[i][j]</code> will always be the maximum value the bag can hold, after considering whether to include or not the first <code>i</code> items, and when the bag has a maximum capacity of <code>j</code>, then therefore <code>memo[n][c]</code> will be the maximum value a bag of capacity <code>c</code> can hold after considering all <code>n</code> items.</p>
<h4 id="time-complexity">Time Complexity</h4>
<p>The final time complexity is $O(nc)$ as there is a nested loop that loops $n \times c$ times, and each operation within the loop is $O(1)$ as they all only require $O(1)$ array access or arithmetic.
Note that because the runtime is dependent on the <strong>numeric value</strong> of the input, and not just the <strong>length</strong> of the input size, the runtime of this algorithm is not polynomial, but <strong>pseudo-polynomial</strong>.</p>
<h4 id="space-complexity">Space Complexity</h4>
<p>The space complexity is $O(nc)$ since there is a 2D $n \times c$ memoisation table to store the solutions of previous computations.
It is important to keep in mind that despite the bottom up solution for fibonacci having a space complexity of $O(1)$, the space complexity here is the same as the top down solution as all previous computations need to be remembered.
Similar to runtime, this also has a pseudo-polynomial space complexity.</p>
<h1 id="linear-programming"><a href="#linear-programming">Linear Programming</a></h1>

<hr>
<p>Linear programming is an optimisation method to maximise or minimise a linear function (our objective) given linear constraints on variables.</p>
<p>This can be represented in mathematical model with</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Mathematical Representation</th>
</tr>
</thead>
<tbody><tr>
<td>Variables</td>
<td>$x_j \geq 0 \text{ for } 1 \leq j \leq n$</td>
</tr>
<tr>
<td>Objective</td>
<td>$\text{maximise or minimise } \sum^{n}_{j = 1} c_j x_j$</td>
</tr>
<tr>
<td>Constraints</td>
<td>$\sum^{n}_{j = 1} a_{ij} x_j R_i b_i, \text{ for } 1 \leq i \leq m \text{ with } R_i \in \{\leq, =, \geq \}$</td>
</tr>
</tbody></table>
<p>A <em>feasible solution</em> is a variable assignment satisfying all constraints.</p>
<p>An <em>optimal solution</em> is a feasible solution satisfying the objective.</p>
<h2 id="canonical-form">Canonical form</h2>
<p>A linear programming formulation can also be represented by the canonical form.</p>
<p>maximise</p>
<p>$$\textbf{c}^\text{T}\textbf{x}$$</p>
<p>subject to the constraints</p>
<p>$$
\begin{align*}
A\textbf{x} &amp; \leq \textbf{b} \\
\textbf{x} &amp; \geq 0.
\end{align*}
$$</p>
<p>where</p>
<p>$$
\textbf{x} =
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix},
\textbf{c} =
\begin{pmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n
\end{pmatrix},
\textbf{b} =
\begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m
\end{pmatrix},
A =
\begin{pmatrix}
a_{1, 1} &amp; a_{1, 2} &amp; \ldots &amp; a_{1, m} \\
a_{2, 1} &amp; a_{2, 2} &amp; \ldots &amp; a_{2, m} \\
\vdots   &amp; \vdots   &amp; \ddots &amp; \vdots   \\
a_{n, 1} &amp; a_{n, 2} &amp; \ldots &amp; a_{n, m}
\end{pmatrix}.
$$</p>
<h2 id="runtime-1">Runtime</h2>
<p>Solving a LP (Linear Programming) formulation is in <strong>P</strong> (has a polynomial runtime), whilst solving an ILP (Integer Linear Programming - similar to LP, except that all the variables must be integers) formulation is in <strong>NP</strong>.</p>
<p>In practice, given an optimisation problem, a person formulates the linear programming problem, which is then given to an LP solver which uses algorithms such as the simplex algorithm.</p>
<h1 id="reductions"><a href="#reductions">Reductions</a></h1>

<hr>
<p>Reductions are the conversion of one problem to another. An efficient reduction from A to B may be used to show that B is at least as difficult as A.</p>
<h2 id="complexity-classes">Complexity Classes</h2>
<h3 id="p-polynomial">P (Polynomial)</h3>
<p>A problem $A(n)$ is in class P, denoted by $A \in \mathbf{P}$, if there is an algorithm which solve it in polynomial time with regard to the size of the input.</p>
<h3 id="np-non-deterministic-polynomial-time">NP (Non deterministic Polynomial Time)</h3>
<p>A problem $A(n)$ is in class NP, denoted by $A \in \mathbf{NP}$, if there is an algorithm which can verify if a solution is correct or not in polynomial time with regard to the size of the input.</p>
<ul>
<li>A problem that is in P is also in NP.</li>
<li>Hence, NP problems are at least as hard as P problems.</li>
</ul>
<h3 id="np-hard-and-np-complete">NP-Hard and NP-Complete</h3>
<p>A problem is NP-Hard if any problem in NP is reducible to it (informally, a problem is NP-Hard if it it is at least as hard as the hardest problems in NP).</p>
<p>A problem is NP-Complete if it is in both NP, and NP-Hard (informally, a problem is NP-Complete if it is one of the hardest problems in NP, the &quot;complete&quot; meaning that it is able to simulate any problem in the same complexity class through reductions).</p>
<ul>
<li>Hence, NP-Hard problems are at least as hard as NP-Complete.</li>
</ul>
<h2 id="np-problems">NP Problems</h2>
<h3 id="3sat">3SAT</h3>
<p>The SAT problem is: Given a propositional formula in the CNF form $C_1 \land C_2 \land ... \land C_n$ where each clause $C_i$ is a disjunction of propositional variables or their negations, i.e.</p>
<p>$$
(P_1 \lor \lnot P_2 \lor P_3 \lor \lnot P_5) \land (P_2 \lor P_3 \lor \lnot P_5 \lor \lnot P_6) \land (\lnot P_3 \lor \lnot P_4 \lor P_5)
$$</p>
<p>Is there some assignment of boolean values to $P_1, P_2, ..., P_6$ which makes the formula true?</p>
<p>If each clause involves exactly 3 variables, then this problem is 3SAT.</p>
<h2 id="cooks-theorem-1982-turing-award">Cook&#39;s Theorem (1982 Turing Award)</h2>
<p>Theorem: Every NP problem is polynomially reducible to the SAT problem</p>
<p>This means that if the SAT problem is solvable in polynomial time, P = NP, as other NP problems can be reduced to SAT in polynomial time and then solved in polynomial time.</p>
<h2 id="karps-21-problems-1972-1985-turing-award">Karp&#39;s 21 Problems [1972] (1985 Turing Award)</h2>
<p>Karp&#39;s 21 problems are a set of problems which show that there is a many-one reduction from the SAT problem to other NP problems, therefore showing they are all NP-complete.</p>
<ul>
<li>Satisfiability<ul>
<li>0 - 1 Integer Programming</li>
<li>Clique<ul>
<li>Set Packing</li>
<li>Vertex Cover<ul>
<li>Set Cover</li>
<li>Feedback Node set</li>
<li>Feedback Arc Set</li>
<li>Direct Hamiltonian Cycle<ul>
<li>Undirected Hamiltonian Cycle</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>3SAT<ul>
<li>Graph Coloring<ul>
<li>Clique Cover</li>
<li>Exact cover<ul>
<li>Hitting Set</li>
<li>Steiner Tree</li>
<li>3D Match</li>
<li>Subset Sum<ul>
<li>Job Sequencing</li>
<li>Partition<ul>
<li>Max Cut</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3gc-to-sat-reduction">3GC to SAT Reduction</h2>
<p>3GC AKA 3 Graph Colouring is a problem that asks given an undirected graph $G = (V, E)$, and a set of colours $C = \{r, g, b\}$, is there a function $f: V \rightarrow C$ such that if $(v, w) \in E$ then $f(v) \neq f(w)$.</p>
<p>Using the notation that $v_i$ is a proposition that is <code>true</code> if vertex $v$ is the colour $i$, we can use the following rules to complete the reduction.</p>
<ol>
<li>Enforce that each vertex is <strong>one colour only</strong>.
We can enforce this through 2 rules.<ol>
<li>A vertex is <strong>no more than one colour</strong>:
$$\forall v \in V : (\lnot v_1 \lor \lnot v_2) \land (\lnot v_1 \lor \lnot v_3) \land (\lnot v_2 \land \lnot v_3)$$
If any of the vertices are more than one colour, then both propositions will be <code>true</code> inside the clause. Since we take the negation of the proposition, both propositions are evaluated as <code>false</code>, and their disjunction evaluates to <code>false</code>.</li>
<li>A vertex is <strong>at least one colour</strong>:
$$\forall v \in V : (v_1 \lor v_2 \lor v_3)$$
If a vertex is not any colour, then the clause evaluates to <code>false</code>.</li>
</ol>
</li>
<li>Enforce that <strong>adjacent vertices are not the same colour</strong>.
$$\forall (v, w) \in E : (\lnot v_1 \lor \lnot w_1) \land (\lnot v_2 \lor \lnot w_2) \land (\lnot v_3 \lor \lnot w_3)$$
Similar to 1.a, if any adjacent vertices are the same colour, then the clause evaluates to <code>false</code>.</li>
</ol>
</article></span></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"contents":"\u003cp\u003eThe course is split into four topics\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDivide and Conquer\u003c/li\u003e\n\u003cli\u003eGreedy Algorithms\u003c/li\u003e\n\u003cli\u003eDynamic Programming\u003c/li\u003e\n\u003cli\u003eLinear Programming and Reductions\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever for the sake of organising notes, I\u0026#39;ve included an extra section to cover knowledge not covered in the course\u0026#39;s prerequisite (COMP2521), and split Linear Programming and Reductions into two sections.\u003c/p\u003e\n\u003ch1 id=\"misc-knowledge\"\u003e\u003ca href=\"#misc-knowledge\"\u003eMisc Knowledge\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003ch2 id=\"asymptotic-runtime\"\u003eAsymptotic Runtime\u003c/h2\u003e\n\u003ch3 id=\"on-big-o\"\u003e$O(n)$, Big O\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDenotes the upper bound of the runtime of an algorithm\u003c/li\u003e\n\u003cli\u003eIf $f(n) = O(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \\leq f(n) \\leq cg(n),  \\forall n \\geq n_0$\u003c/li\u003e\n\u003cli\u003e$f(n) = O(g(n))$ means that $f(n)$ does not grow substantially faster than $g(n)$ because a multiple of $g(n)$ eventually dominates $f(n)$\u003c/li\u003e\n\u003cli\u003eMost commonly used as we are concerned with the worst runtime\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"thetan-big-theta\"\u003e$\\Theta(n)$, Big Theta\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDenotes a tight bound of the runtime of an algorithm\u003c/li\u003e\n\u003cli\u003e$f(n) = \\Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$\u003c/li\u003e\n\u003cli\u003eLess commonly used than $O(n)$, but more common than $\\Omega(n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"omegan-big-omega\"\u003e$\\Omega(n)$, Big Omega\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDenotes the lower bound of the runtime of an algorithm\u003c/li\u003e\n\u003cli\u003eIf $f(n) = \\Omega(g(n))$, there exist positive constants $c$ and $n_0$ such that $0 \\leq cg(n) \\leq f(n),  \\forall n \\geq n_0$\u003c/li\u003e\n\u003cli\u003e$f(n) = \\Omega(g(n))$ means that $f(n)$ grows at least as fast as $g(n)$, because $f(n)$ eventually dominates a multiple of $g(n)$\u003c/li\u003e\n\u003cli\u003eLeast often used as we usually aren\u0026#39;t concerned with the best runtime\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"math\"\u003eMath\u003c/h2\u003e\n\u003ch3 id=\"log-identity\"\u003eLog Identity\u003c/h3\u003e\n\u003cp\u003eIf $a, b, c \u0026gt; 0$ then\n$$a^{\\log_{b}c} = c^{\\log_{b}a}$$\nProof\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\log_{b}c \\cdot \\log_{b}a \u0026amp;= log_{b}a \\cdot \\log_{b}c \\\\\n\\log_{b}(a^{\\log_{b}c}) \u0026amp;= \\log_{b}(c^{\\log_{b}a}) \\\\\na^{log_{b}c} \u0026amp;= c^{\\log_{b}a}\n\\end{align*}\n$$\u003c/p\u003e\n\u003ch3 id=\"roots-of-unity\"\u003eRoots of Unity\u003c/h3\u003e\n\u003ch4 id=\"representation\"\u003eRepresentation\u003c/h4\u003e\n\u003cp\u003eComplex numbers $z = a + ib$ can be represented using\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$modulus$ $|z| = \\sqrt{a^2 + b^2}$\u003c/li\u003e\n\u003cli\u003e$argument$ $\\arg(z)$, which is an angle taking values in $(-\\pi, \\pi]$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eand satisfying:\n$$z = |z|e^{i \\arg(z)} = |z|(\\cos(\\arg(z)) + i \\sin(\\arg(z))).$$\u003c/p\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cp\u003e$$z^n = \\left(|z|e^{i \\arg(z)}\\right)^n = |z|^ne^{in\\arg(z)} = |z|^n(n\\cos(\\arg(z)) + i \\sin(n\\arg(z))).$$\u003c/p\u003e\n\u003ch4 id=\"properties\"\u003eProperties\u003c/h4\u003e\n\u003cp\u003eRoots of unity of order $n$ are complex numbers which satisfy $z^n = 1$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf $z^n = |z|^n(\\cos(n \\arg(z)) + i\\sin(n\\arg(z))) = 1$ then $|z| = 1$ and $n\\arg(z)$ is a multiple of $2\\pi$\u003c/li\u003e\n\u003cli\u003eThus, $n\\arg( z) = 2\\pi k$, i.e. $arg(z) = \\frac{2\\pi k}{n}$\u003c/li\u003e\n\u003cli\u003eWe denote $\\omega_n = e^{i \\frac{2\\pi}{n}}$, such that $\\omega_n$ is called a primitive root of unity order $n$\u003c/li\u003e\n\u003cli\u003eA root of unity $\\omega$ of order $n$ is primitive if all other roots of unity of the same order can be obtained as its powers $\\omega^k$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor $\\omega_n = e^{i \\frac{2\\pi}{n}}$ and for all $k$ such that $0 \\leq k \\leq n - 1$,\n$$ (\\omega_n^k)^n = ((\\omega_n)^k)^n = (\\omega_n)^{nk} = ((\\omega_n)^n)^k = 1^k = 1.$$\u003c/p\u003e\n\u003cp\u003eIf $k + m \\geq n$ then $k + m = n + l$ for $l = (k + m) \\mod(n)$ and we have\n$$\\omega_n^k \\omega_n^m = \\omega_n^{k+m} = \\omega_n^{n+l} = \\omega_n^n \\omega_n^l = 1 \\cdot \\omega_n^l = \\omega_n^l \\text{ where } 0 \\leq l \\leq n.$$\u003c/p\u003e\n\u003cp\u003eTherefore, the product of any two roots of unity of the same order is just another root of unity of the same order (this is not the case for addition, as the sum of two roots of unity is usually not another root of unity).\u003c/p\u003e\n\u003cp\u003eThe Cancellation Lemma: $\\omega_{kn}^{km} = \\omega_n^m$, and its proof is below\n$$\\omega_{kn}^{km} = (\\omega_{kn})^{km} = \\left(e^{i \\frac{2\\pi}{kn}}\\right)^{km} = e^{i \\frac{2\\pi k m}{k n}} = e^{i \\frac{2\\pi m}{n}} = \\left(e^{i \\frac{2\\pi}{n}}\\right)^m = \\omega_n^m.$$\u003c/p\u003e\n\u003ch1 id=\"divide-and-conquer\"\u003e\u003ca href=\"#divide-and-conquer\"\u003eDivide and Conquer\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003cp\u003eDivide and conquer algorithms recursively break down a problem into two or more non overlapping subproblems, before merging them together to become easy to solve.\u003c/p\u003e\n\u003cp\u003eCommon examples of Divide and Conquer algorithms include\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBinary Search\u003c/li\u003e\n\u003cli\u003eMerge Sort\u003c/li\u003e\n\u003cli\u003eFast Fourier Transform\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"sample-algorithms\"\u003eSample Algorithms\u003c/h2\u003e\n\u003ch3 id=\"karatsubas-fast-integer-multiplication\"\u003eKaratsuba\u0026#39;s Fast Integer Multiplication\u003c/h3\u003e\n\u003ch4 id=\"naive-method\"\u003eNaive method\u003c/h4\u003e\n\u003cp\u003eGiven 2 \u0026#39;big integers\u0026#39;, i.e. a number, stored as an array, where the values at an the $i^{th}$ index is the $i^{th}$ digit of the integer, and there are $n$ digits, a sample solution would look like\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Naive method for big integer multiplication\n\ndef mul(A: list[int], B: list[int]) -\u0026gt; list[int]:\n    n, m = len(A), len(B)\n    C = [0] * (n + m)\n\n    # Apply multiplication\n    for i in range(n):\n        for j in range(m):\n            C[i + j] += A[i] * B[j]\n\n    # Add carry amount to next digit and take mod of 10\n    for i in range(n + m - 1):\n        C[i + 1] += C[i] // 10\n        C[i] %= 10\n\n    return C\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand hence have a runtime of $\\Theta(n^{2})$, where $n$ is the greater number of digits.\u003c/p\u003e\n\u003ch4 id=\"karatsubas-method\"\u003eKaratsuba\u0026#39;s method\u003c/h4\u003e\n\u003cp\u003eSay we want to find the solution to the multiplication of\n$$P(x) = a_1x + a_0,$$\n$$Q(x) = b_1x + b_0,$$\na naive solution will require 4 multiplications\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e$A = a_1 \\times b_1$\u003c/li\u003e\n\u003cli\u003e$B = a_1 \\times b_0$\u003c/li\u003e\n\u003cli\u003e$C = b_1 \\times a_1$\u003c/li\u003e\n\u003cli\u003e$D = b_0 \\times a_0$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e$$P(x)Q(x) = Ax^2 + (B + C)x + D$$\nHowever we know that\n$$(a_1x + a_0)(b_1x + b_0) = (a_1b_1 + a_1b_0 + a_0b_1 + a_0b_0) = (A + B + C + D),$$\nrequiring 2 additions (which can be computed easily) and 1 multiplication. Hence to find the final multiplication, we only need three multiplications\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e$E = (A + B + C + D) = (a_1 + a_0) \\times (b_1 + b_0)$\u003c/li\u003e\n\u003cli\u003e$A = a_1 \\times b_1$\u003c/li\u003e\n\u003cli\u003e$D = b_0 \\times a_0$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHence to find the final solution, we can do\n$$P(x)Q(x) = Ax^2 + (E - A - D)x + D$$\u003c/p\u003e\n\u003ch4 id=\"karatsubas-method-for-integers\"\u003eKaratsuba\u0026#39;s method for integers\u003c/h4\u003e\n\u003cp\u003eIf we want to multiply $A$ with $B$, we can represent them as\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nA \u0026amp;= 10^{\\frac{n}{2}}A_1 + A_0 \\\\\nB \u0026amp;= 10^{\\frac{n}{2}}B_1 + B_0.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eWith this, we can apply Karatsuba\u0026#39;s method, where\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n  x \u0026amp;= A_1B_1 \\\\\n  z \u0026amp;= A_0B_0 \\\\\n  y \u0026amp;= (A_1 + A_0)(B_1 + B_0) - x - z \\\\\n  AB\n    \u0026amp;= 10^{n}x + 10^{\\frac{n}{2}}y + z.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eAt each function call, there are 3 multiplications being performed (each of which is multiplied by the same algorithm), where the input size is halved in each recursive call, and the addition runs in $O(n)$. Therefore Karatsuba\u0026#39;s method has the recurrence\n$$ T(n) = 3T\\left(\\frac{n}{2}\\right) + O(n) = O(n^{\\lg 3}). $$\u003c/p\u003e\n\u003ch4 id=\"karatsubas-method-for-polynomials\"\u003eKaratsuba\u0026#39;s method for polynomials\u003c/h4\u003e\n\u003cp\u003eWe can view this representation of integers similar to that of polynomials. Say we are given polynomials $A(x)$, $B(x)$, where\n$$A(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots + a_0$$\n$$B(x) = b_nx^n + b_{n-1}x^{n-1} + \\ldots + b_0.$$\nWe have $C(x) = A(x) \\cdots B(x)$ of degree $2n$\n$$C(x) = \\sum_{j = 0}^{2n}c_jx^j = A(x)B(x) = \\sum_{j=0}^{2n} \\left( \\sum_{i + k = j}a_ib_k \\right)x^j$$\nhowever, we want to find the coefficients of $c_j = \\sum_{i+k=j}a_ib_k$ without performing $(n+1)^2$ many multiplications necessary to get all products of the form $a_ib_k$.\u003c/p\u003e\n\u003ch3 id=\"fast-fourier-transform\"\u003eFast Fourier Transform\u003c/h3\u003e\n\u003cp\u003eA naive way to multiply two polynomials together would be to multiply each term of the first polynomial with a term of the second polynomial. The runtime of this would be $O(n^2)$, where $n$ is the degree/ number of terms of the polynomials.\u003c/p\u003e\n\u003cp\u003eAnother method would be to convert both polynomial into it\u0026#39;s point value form, multiply those points and convert it back into the polynomial form.\nThe fast Fourier transform (FFT) is an algorithm to calculate the Discrete Fourier transform (DFT) of a sequence, or the inverse (IDFT) in $O(n \\log n)$.\u003c/p\u003e\n\u003ch3 id=\"polynomial-interpolation-vandermonde-matrix\"\u003ePolynomial Interpolation (Vandermonde Matrix)\u003c/h3\u003e\n\u003ch4 id=\"from-coefficient-to-value-representation\"\u003eFrom Coefficient to Value Representation\u003c/h4\u003e\n\u003cp\u003eA polynomial $A(x)$ of degree $n$ is uniquely determined by its values at any $n + 1$ distinct input values $x_0, x_1, \\ldots, x_n$:\n$$A(x) \\leftrightarrow \\langle(x_0, A(x_0)), (x_1, A(x_1)), \\ldots, (x_n, A(x_n))\\rangle$$\u003c/p\u003e\n\u003cp\u003eFor $A(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots + a_0$, these values can be obtained via a matrix multiplication:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix}\n1      \u0026amp; x_0    \u0026amp; x_0^2  \u0026amp; \\ldots \u0026amp; x_0^n  \\\\\n1      \u0026amp; x_1    \u0026amp; x_1^2  \u0026amp; \\ldots \u0026amp; x_1^n  \\\\\n\\vdots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n1      \u0026amp; x_n    \u0026amp; x_n^2  \u0026amp; \\ldots \u0026amp; x_n^n  \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na_0 \\\\\na_1 \\\\\n\\vdots \\\\\na_n \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nA(x_0) \\\\\nA(x_1) \\\\\n\\vdots \\\\\nA(x_n) \\\\\n\\end{pmatrix}\n$$\u003c/p\u003e\n\u003cp\u003eSuch a matrix is called the Vandermonde matrix.\u003c/p\u003e\n\u003ch4 id=\"from-value-to-coefficient-representation\"\u003eFrom Value to Coefficient Representation\u003c/h4\u003e\n\u003cp\u003eIt can be shown that if $x_i$ are all distinct, then this matrix is invertible.\nThus, if all $x_i$ are all distinct, given any values $A(x_0), A(x_1), \\ldots, A(x_n)$ the coefficients $a_0, a_1, \\ldots, a_n$ of the polynomial $A(x)$ are uniquely determined:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{pmatrix}\n1      \u0026amp; x_0    \u0026amp; x_0^2  \u0026amp; \\ldots \u0026amp; x_0^n  \\\\\n1      \u0026amp; x_1    \u0026amp; x_1^2  \u0026amp; \\ldots \u0026amp; x_1^n  \\\\\n\\vdots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n1      \u0026amp; x_n    \u0026amp; x_n^2  \u0026amp; \\ldots \u0026amp; x_n^n  \\\\\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nA(x_0)    \\\\\nA(x_1)    \\\\\n\\vdots    \\\\\nA(x_n)    \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\na_0    \\\\\na_1    \\\\\n\\vdots \\\\\na_n    \\\\\n\\end{pmatrix}\n$$\u003c/p\u003e\n\u003ch4 id=\"complexity-of-swapping-representation\"\u003eComplexity of Swapping Representation\u003c/h4\u003e\n\u003cp\u003eIf we fix the inputs $x_0, x_1, \\ldots, x_n$ then commuting between a representation of a polynomial $A(x)$ via its coefficients and a representation via its values at these points is done via the following two matrix multiplications, with matrices made from constants, and thus, for fixed inputs $x_0, x_1, \\ldots, x_n$, this switch between the two kinds of representations is done in linear time.\u003c/p\u003e\n\u003ch4 id=\"discrete-fourier-transform\"\u003eDiscrete Fourier Transform\u003c/h4\u003e\n\u003cp\u003eFor $\\mathbf{a} = \\langle a_0, a_1, \\ldots a_{n-1} \\rangle$ a sequence of $n$ real or complex numbers, we can form the corresponding polynomial $P_A(x) = \\sum_{j=0}^{n-1}a_jx^j$, and evaluate it at all complex roots of unity of order n,\n$$\\forall \\ 0 \\leq k \\leq n - 1, \\quad P_A(\\omega_n^k) = A_k = \\sum_{j=0}^{n-1}a_j\\omega_n^{jk}.$$\u003c/p\u003e\n\u003cp\u003eThe DFT of a sequence $\\mathbf{a}$ is a sequence $\\mathbf{A}$ of the same length.\u003c/p\u003e\n\u003ch4 id=\"inverse-discrete-fourier-transform\"\u003eInverse Discrete Fourier Transform\u003c/h4\u003e\n\u003cp\u003eThe IDFT of a sequence $\\mathbf{A} = \\langle A_0, A_1, \\ldots, A_{n-1} \\rangle$ is the sequence of values $\\mathbf{a} = \\langle a_0, a_1, \\ldots, a_{n-1} \\rangle = \\langle \\frac{P_a(1)}{n}, \\frac{P_a(\\omega_n^{-1})}{n}, \\frac{P_a(\\omega_n^{-2})}{n}, \\ldots, \\frac{P_a(\\omega_n^{1-n})}{n} \\rangle$.\u003c/p\u003e\n\u003cp\u003eWe can show that IDFT(DFT($\\mathbf{a}$)) = $\\mathbf{a}$ and DFT(IDFT($\\mathbf{A}$)) = $\\mathbf{A}$.\u003c/p\u003e\n\u003ch4 id=\"computation-of-dft-idft\"\u003eComputation of DFT/ IDFT\u003c/h4\u003e\n\u003cp\u003eBrute force computation of the DFT takes $\\Theta(n^2)$, same for IDFT. The DFT of a sequence can be computed in $\\Theta(n \\lg n)$ using the FFT (as can be the IDFT).\u003c/p\u003e\n\u003ch2 id=\"proofs\"\u003eProofs\u003c/h2\u003e\n\u003ch3 id=\"proof-of-correctness\"\u003eProof of Correctness\u003c/h3\u003e\n\u003cp\u003eFor Divide and Conquer proof of correctness, mathematical induction is used.\u003c/p\u003e\n\u003cp\u003eSay that we want to prove that an algorithm $A(n)$ is correct.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eProve that the base case is correct, i.e. $A(0)$, $A(1)$ is correct (depends on your base case)\u003c/li\u003e\n\u003cli\u003eProve induction step\u003col\u003e\n\u003cli\u003eAssume that $A(k)$ is true, where $k \u0026lt; n$ (Inductive hypothesis)\u003c/li\u003e\n\u003cli\u003eTherefore, recursive calls are correct as they call $A(k)$ where $k \u0026lt; n$, which is true by the inductive hypothesis\u003c/li\u003e\n\u003cli\u003eProve that the conquer part of the algorithm is correct\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eHence algorithm is correct for input size $n$\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"proof-of-runtime-master-theorem\"\u003eProof of Runtime (Master Theorem)\u003c/h3\u003e\n\u003ch4 id=\"representation-1\"\u003eRepresentation\u003c/h4\u003e\n\u003cp\u003eThe time complexity of a divide and conquer algorithm can be represented in the form\n$$T(n) = aT \\left( \\frac{n}{b} \\right) + f(n)$$\nwhere,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$T(n)$\u003cul\u003e\n\u003cli\u003eThe divide and conquer algorithm, with an input size of $n$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$a \\geq 1$\u003cul\u003e\n\u003cli\u003e$a$ is the number of recursive calls per function\u003c/li\u003e\n\u003cli\u003eThe constraint implies we make at least 1 recursive call, otherwise there would be no recursion\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$b \u0026gt; 1$\u003cul\u003e\n\u003cli\u003e$b$ is the decrease in input size per recursive call, the constraint implying that the input size must decrease in each recursive call\u003c/li\u003e\n\u003cli\u003eThe constraint implies that the input size must decrease at each recursive call, otherwise we would loop infinitely, never reaching the base case\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$f(n)$\u003cul\u003e\n\u003cli\u003eThe runtime within each function call\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"runtime-cases\"\u003eRuntime cases\u003c/h4\u003e\n\u003cp\u003eUsing this information, we can find the runtime of most DAQ algorithms through one of the 3 cases where if $\\epsilon \u0026gt; 0$ is a constant, then\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf $f(n) = O(n^{\\log_b a - \\epsilon})$, then $T(n) = \\Theta(n^{log_b a})$\u003c/li\u003e\n\u003cli\u003eIf $f(n) = \\Theta(n^{\\log_b a})$, then $T(n) = \\Theta(n^{log_b a} \\cdot \\log n)$\u003c/li\u003e\n\u003cli\u003eIf $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$, then $T(n) = \\Theta(f(n))$\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"examples\"\u003eExamples\u003c/h4\u003e\n\u003ch5 id=\"case-1\"\u003eCase 1\u003c/h5\u003e\n\u003cp\u003eIf $f(n) = O(n^{\\log_b a - \\epsilon})$, then $T(n) = \\Theta(n^{log_b a})$ \u003cbr\u003eWork per subproblem is dwarfed by number of subproblems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBinary tree DFS | $T(n) = 2T \\left( \\frac{n}{2} \\right) + c$\u003cul\u003e\n\u003cli\u003eProof:\u003cul\u003e\n\u003cli\u003e$n^{log_b a} = n^{\\lg 2} = n^{1} = n$\u003c/li\u003e\n\u003cli\u003e$f(n) = c = \\Theta(1)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore $T(n) = \\Theta(n^{\\lg 2}) = \\Theta(n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKaratsuba\u0026#39;s integer multiplication | $T(n) = 3T \\left( \\frac{n}{2} \\right) + n$\u003cul\u003e\n\u003cli\u003eProof\u003cul\u003e\n\u003cli\u003e$n^{\\log_2 3} = n^{\\lg 3} \\approx n^{1.58}$\u003c/li\u003e\n\u003cli\u003e$f(n) = n = \\Theta(n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore $T(n) = \\Theta(n^{1.58})$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"case-2\"\u003eCase 2\u003c/h5\u003e\n\u003cp\u003eIf $f(n) = \\Theta(n^{\\log_b a})$, then $T(n) = \\Theta(n^{log_b a} \\cdot \\log n)$ \u003cbr\u003eWork per subproblem is comparable to number of subproblems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBinary search | $T(n) = T \\left( \\frac{n}{2} \\right) + c$\u003cul\u003e\n\u003cli\u003eProof:\u003cul\u003e\n\u003cli\u003e$n^{\\log_b a} = n^{\\lg 1} = n^{0} = 1$\u003c/li\u003e\n\u003cli\u003e$f(n) = c = \\Theta(1)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore $T(n) = \\Theta(n^{\\lg 1} \\cdot \\Theta(1)) = \\Theta(\\log n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMerge sort | $T(n) = 2T \\left( \\frac{n}{2} \\right) + cn$\u003cul\u003e\n\u003cli\u003eProof:\u003cul\u003e\n\u003cli\u003e$n^{\\log_b a} = n^{\\lg 2} = n^{1} = n$\u003c/li\u003e\n\u003cli\u003e$f(n) = cn = \\Theta(n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore $T(n) = \\Theta(n^{\\lg 2} \\cdot \\Theta(n)) = \\Theta(n \\log n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"case-3\"\u003eCase 3\u003c/h5\u003e\n\u003cp\u003eIf $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$, then $T(n) = \\Theta(f(n))$ \u003cbr\u003eWork per subproblem dominates number of subproblems\u003c/p\u003e\n\u003ch1 id=\"greedy-algorithms\"\u003e\u003ca href=\"#greedy-algorithms\"\u003eGreedy Algorithms\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003cp\u003eGreedy algorithms are often used for optimisation problems (i.e. you want to maximise or minimise some quantity according to a set of constraints) and make the optimal choice at each step to find the overall optimal way to solve the entire problem.\u003c/p\u003e\n\u003cp\u003eCommon examples of Greedy Algorithms include\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKruksal\u0026#39;s algorithm/ Prim\u0026#39;s algorithm\u003c/li\u003e\n\u003cli\u003eA* path finding algorithm\u003c/li\u003e\n\u003cli\u003eHuffman encoding\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"sample-algorithms-1\"\u003eSample Algorithms\u003c/h2\u003e\n\u003ch3 id=\"interval-scheduling\"\u003eInterval Scheduling\u003c/h3\u003e\n\u003cp\u003eGiven a set of $n$ tasks requests with start finish times,\u003c/p\u003e\n\u003cp\u003e$$R = \\{(s_1, f_1), (s_2, f_2), ..., (s_n, f_n)\\}$$\u003c/p\u003e\n\u003cp\u003ewe want to find the maximum number of non overlapping tasks that we can complete.\u003c/p\u003e\n\u003cp\u003eAn example of a greedy strategy that can solve this is \u003cstrong\u003eEarliest Finish First\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003edef eaf(tasks: list[tuple[int, int]]) -\u0026gt; list[tuple[int, int]]:\n    # Sort by earliest finish time\n    tasks.sort(key=lambda job: job[1])\n    schedule = []\n    prev_finish = 0\n\n    for (start, finish) in tasks:\n        # If the task doesn\u0026#39;t conflict with the previous finish\n        if start \u0026gt; prev_finish:\n            # Add it to our schedule and update the finish time\n            schedule.append((start, finish))\n            prev_finish = finish\n\n    return schedule\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"runtime\"\u003eRuntime\u003c/h4\u003e\n\u003cp\u003eThe runtime of this algorithm is $O(n \\log n)$ as it is dominated by the time required to sort the jobs, as the following loop runs in $O(n)$ time.\u003c/p\u003e\n\u003ch4 id=\"proof-of-correctness-1\"\u003eProof of Correctness\u003c/h4\u003e\n\u003cp\u003eFirst, we prove the result is correct (i.e. there are no tasks conflicting).\u003c/p\u003e\n\u003cp\u003eThis is true as we only add a new task to the schedule if it does not conflict with previous tasks.\u003c/p\u003e\n\u003ch4 id=\"proof-of-optimality\"\u003eProof of Optimality\u003c/h4\u003e\n\u003cp\u003eSecondly, we prove that the result is optimal (i.e. it is indeed the maximum number of non conflicting tasks we can schedule).\u003c/p\u003e\n\u003cp\u003eLet $O = \\{o_1, ..., o_k\\}$ be the tasks of an optimal solution listed in increasing order of finish time, and $G = \\{g_1,..., g_k \\}$ by the tasks of the EFF solution.\u003c/p\u003e\n\u003cp\u003eSince $O$ is optimal, it must contain at least as many tasks as $G$, hence there must be the first index $j$, where these two schedules differ, such that\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\nO \u0026amp;= \\{o_1, ..., o_{j - 1}, o_j, ... \\} \\\\\nG \u0026amp;= \\{o_1, ..., o_{j - 1}, g_j, ... \\}.\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003eSince EFF is correct and selects the earliest finish activity time, $g_j$ does not conflict with any earlier activity, and it finishes no later than $o_j$.\u003c/p\u003e\n\u003cp\u003eNow consider a \u0026quot;greedier\u0026quot; optimal solution $O\u0026#39;$ where we replace $o_j$ with $g_j$ in $O$, such that\u003c/p\u003e\n\u003cp\u003e$$O\u0026#39; = \\{ o_1, ..., o_{j - 1}, g_j, o_{j + 1}, ... \\}.$$\u003c/p\u003e\n\u003cp\u003eClearly, $O\u0026#39;$ is correct since $g_j$ finishes no later than $x_j$ so therefore there are no conflicts. Therefore, this new schedule has the same number of activities as $O$, and so it is at least as good.\u003c/p\u003e\n\u003cp\u003eBy repeating this process, we will eventually convert $O$ into $G$, without decreasing the number of tasks. Therefore $G$ is optimal.\u003c/p\u003e\n\u003ch3 id=\"minimum-spanning-tree-mst\"\u003eMinimum Spanning Tree (MST)\u003c/h3\u003e\n\u003cp\u003eLet $G = (V, E)$ be a connected undirected graph.\u003c/p\u003e\n\u003cp\u003eA Spanning Tree is a subgraph $T = (V, E_T)$ of $G$ such that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eT does not contain any cycles\u003c/li\u003e\n\u003cli\u003eT is connected\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf $G$ is an edge weighted graph, then a MST is a spanning tree of minimum weight.\u003c/p\u003e\n\u003ch4 id=\"kruskals-algorithm\"\u003eKruskal\u0026#39;s Algorithm\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eOrder the edges $E$ in a non-decreasing order by their weight\u003c/li\u003e\n\u003cli\u003eBuild a tree by adding the lowest weight edge each time\u003c/li\u003e\n\u003cli\u003eAn edge $E_i$ is added at a round $i$ of construction if it does not introduce a cycle\u003c/li\u003e\n\u003cli\u003eIf adding an edge would introduce a cycle, that edge is discarded\u003c/li\u003e\n\u003cli\u003eThis continues until the list of all edges has been exhausted\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch5 id=\"proof-of-correctness-2\"\u003eProof of Correctness\u003c/h5\u003e\n\u003cp\u003eLet $T$ be the output of the algorithm\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$T$ does not contain any cycle\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe show by contradiction that $T$ is connected:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAssume there are two or more connected components $C_1$ and $C_2$\u003c/li\u003e\n\u003cli\u003e$G$ is connected, so there are some edges connecting $C_1$ to $C_2$ in $G$\u003c/li\u003e\n\u003cli\u003eThe first of such edges would have been added to $T$ because it would not create any cycle in $T$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTherefore $T$ is a spanning tree\u003c/p\u003e\n\u003ch5 id=\"proof-of-optimality-1\"\u003eProof of Optimality\u003c/h5\u003e\n\u003cp\u003eWe consider the case where all weights are distinct.\u003c/p\u003e\n\u003cp\u003eLet $T$ be the output of Kruskal\u0026#39;s algorithm.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConsider a spanning tree $T\u0026#39;$ distinct from $T$.\u003c/li\u003e\n\u003cli\u003eLet $e = \\{u, v \\}$ be the smallest-weight edge in $T$ that is not in $T\u0026#39;$.\u003c/li\u003e\n\u003cli\u003e$T\u0026#39;$ is spanning so there exists a path $P$ from $u$ to $v$.\u003c/li\u003e\n\u003cli\u003eLet $T\u0026#39;\u0026#39; = (V, \\{e\\} \\cup E_{T\u0026#39;} \\backslash \\{f\\})$, which is spanning tree.\u003c/li\u003e\n\u003cli\u003e$w(e) \u0026lt; w(f)$ because otherwise Kruskal\u0026#39;s algorithm would have added $f$ to $T$ instead of $e$.\u003c/li\u003e\n\u003cli\u003eFurthermore, $T\u0026#39;\u0026#39;$ weighs less than $T\u0026#39;$, so $T\u0026#39;$ is not an MST.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$G$ has an MST and any $T\u0026#39; \\neq T$ is not an MST, so $T$ is an MST.\u003c/p\u003e\n\u003ch4 id=\"kruskals-vs-prims-algorithm\"\u003eKruskal\u0026#39;s vs Prim\u0026#39;s algorithm\u003c/h4\u003e\n\u003cp\u003ePrim\u0026#39;s algorithm is another greedy algorithm to find the MST of a graph, however it\u0026#39;s differences are listed as below.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eKruskal\u0026#39;s algorithm\u003c/th\u003e\n\u003cth\u003ePrim\u0026#39;s algorithm\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eTree built always remains connected\u003c/td\u003e\n\u003ctd\u003eTree build usually remains disconnected\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAdds next cheapest edge\u003c/td\u003e\n\u003ctd\u003eAdds next cheapest vertex\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFaster for sparse graphs\u003c/td\u003e\n\u003ctd\u003eFaster for dense graphs\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch2 id=\"proofs-1\"\u003eProofs\u003c/h2\u003e\n\u003cp\u003eWhilst greedy algorithms typically have a structure for correctness and optimality, the proof of runtime varies a lot depending on the data structures and algorithms used.\u003c/p\u003e\n\u003ch3 id=\"proof-of-correctness-and-optimality\"\u003eProof of Correctness and Optimality\u003c/h3\u003e\n\u003cp\u003eNote that greedy algorithms are typically used to maximise or minimise a quantity under a constraint.\u003c/p\u003e\n\u003cp\u003eWe typically prove for correctness - that it satisfies the constraints of the problem (i.e. in task scheduling, we prove no two tasks overlap).\u003c/p\u003e\n\u003cp\u003eSecondly prove that the greedy method is achieves the most optimal solution (i.e. it does maximise or minimises a quantity). This can be done multiple ways\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eGreedy stays ahead\u003c/strong\u003e: This works by showing that according to the measure of optimality, the greedy algorithm is at least as far ahead as the optimal solution during each iteration of the algorithm. It is typically done in 4 steps.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDefine your solution\u003c/strong\u003e Introduce variables to denote the greedy solution $G$ and the optimal solution $O$ (i.e. for task scheduling, it is the set of tasks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDefine your measure\u003c/strong\u003e Define a measure of optimality (i.e. for tasks scheduling, it is the number of jobs).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProve greedy stays ahead\u003c/strong\u003e Prove that the measure of optimality for the greedy solution is at least as good as the optimal.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProve optimality\u003c/strong\u003e Because the greedy solution stays ahead, it must produce an optimal solution. This is typically done by contradiction by assuming the greedy solution isn\u0026#39;t optimal and using the fact that greedy stays ahead as a contradiction.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eExchange arguments\u003c/strong\u003e: This works by showing you can iteratively transform any optimal solution into the result of the greedy algorithm without changing the cost of the optimal solution, thereby proving the greedy solution is optimal.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDefine your solution\u003c/strong\u003e Introduce variables to denote the greedy solution $G$ and the optimal solution $O$ (i.e. for task scheduling, it is the set of tasks).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompare solutions\u003c/strong\u003e Show that $G \\neq O$ (i.e. there is some element of $G$ not in $O$. It helps to name this different element).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExchange pieces\u003c/strong\u003e Show how to transform $G$ into $O$, typically by moving element previously defined in $G$ to $O$. Then prove by doing os you did not worsen $O$\u0026#39;s optimality, and therefore have a different optimal solution.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIterate\u003c/strong\u003e Argue by continuing the exchange of pieces, you can turn $G$ into $O$ without reducing optimality, and therefore $G$ is optimal.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"dynamic-programming\"\u003e\u003ca href=\"#dynamic-programming\"\u003eDynamic Programming\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003cp\u003eDynamic programming is a method to find optimal solution to problems, from optimal solutions to subproblem. Because sets of subproblems to solve larger problems overlap, each subproblem can be calculated once, it\u0026#39;s solutions are stored for next future calls.\u003c/p\u003e\n\u003cp\u003eCommon examples of Dynamic Programming include\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDijkstra\u0026#39;s algorithm\u003c/li\u003e\n\u003cli\u003eInteger knapsack\u003c/li\u003e\n\u003cli\u003eOptimal matrix chain multiplication\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"sample-algorithms-2\"\u003eSample Algorithms\u003c/h2\u003e\n\u003ch3 id=\"fibonacci\"\u003eFibonacci\u003c/h3\u003e\n\u003cp\u003eWhilst simple this problem showcases the concept of dynamic programming very well.\u003c/p\u003e\n\u003cp\u003eCreate a function \u003ccode\u003efib\u003c/code\u003e where given $i$, find the $i^{th}$ fibonacci number. This can be defined with the following mathematical recurrence relation\u003c/p\u003e\n\u003cp\u003e$$\nF_n =\n\\begin{cases}\n0 \u0026amp; \\text{if } n = 0 \\\\\n1 \u0026amp; \\text{if } n = 1 \\\\\nF_{n - 1} + F_{n - 2} \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eor with the following python code\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# A function to calculate the nth fibonacci number\ndef fib(n: int) -\u0026gt; int:\n    if n == 0:\n        return 0\n    if n == 1:\n        return 1\n    return fib(n - 1) + fib(n - 2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever, the runtime of this is $O(2^n)$, an exponential runtime, as each function call itself twice until it reaches the base case.\u003c/p\u003e\n\u003ch4 id=\"top-down-solution\"\u003eTop down solution\u003c/h4\u003e\n\u003cp\u003eIn top down dynamic programming, we use the process of \u003cstrong\u003ememoisation\u003c/strong\u003e (caching previous function call results) to reduce the runtime to $O(n)$, and space complexity of $O(n)$.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Top down solution to find the nth fibonacci number\ndef fib(n: int) -\u0026gt; int:\n    # Create memoisation table and add base cases\n    memo = [-1] * n\n    memo[0] = 0\n    memo[1] = 1\n    return dp(n, memo)\n\ndef dp(n: int, memo: list[int]) -\u0026gt; int:\n    # If we have not calculated the value before\n    if memo[n] == -1:\n        memo[n] = dp(n - 1, memo) + dp(n - 2, memo)\n\n    # Now we can simply return the stored value in O(1)\n    return memo[n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"bottom-up-solution\"\u003eBottom up solution\u003c/h4\u003e\n\u003cp\u003eIn bottom up DP, we write an iterative solution to compute the value of every subproblem to reduce the runtime to $O(n)$ and space complexity of $O(1)$.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Bottom up solution to find the nth fibonacci number\ndef fib(n: int) -\u0026gt; int:\n    # Base Case\n    if n == 0:\n        return 0\n    if n == 1:\n        return 1\n\n    # Iterate up to the nth fibonacci number\n    a, b = 0, 1\n    for i in range(2, n + 1):\n        temp = a + b\n        a = b\n        b = temp\n    return b\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"integer-knapsack\"\u003eInteger Knapsack\u003c/h3\u003e\n\u003cp\u003eGiven a set of $n$ items with weights and values,\u003c/p\u003e\n\u003cp\u003e$$I = \\{ (v_1, w_1), (v_2, w_2), ..., (v_n, w_n) \\},$$\u003c/p\u003e\n\u003cp\u003eand a knapsack which has a weight capacity of $c$, choose a combination of items, maximising the value of the items while ensuring the total weight is less than $c$.\u003c/p\u003e\n\u003cp\u003eWe can define the following recurrence relation\u003c/p\u003e\n\u003cp\u003e$$\nF(i, c) =\n\\begin{cases}\n0 \u0026amp; \\text{if } i = 0 \\text{ or } c \\leq 0 \\\\\n\\max(F(i - 1, c - w_i) + v_i, F(i - 1, c)) \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$i$ is the index of the current item\u003c/li\u003e\n\u003cli\u003e$c$ is the current capacity\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ewhich has the base case that we return 0 if we have no items or we are out of capacity, else we start at an item, and then consider the maximum value we can get if:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ewe include it in the knapsack\u003c/strong\u003e in which case, we increase the value we get by the value of the current item, reduce the capacity by the weight of the current item, and then consider what to do with the next item.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ewe do not include it in the knapsack\u003c/strong\u003e in which case, we do nothing, and merely consider what to do with the next item.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe runtime of this is unfortunately $O(2^n)$ (since each subproblem makes two recursive calls and it has a maximum depth of $n$), however since there are recalculated subproblems (i.e. when $i$ and $c$ is the same), we can use dynamic programming.\u003c/p\u003e\n\u003ch3 id=\"memoisation\"\u003eMemoisation\u003c/h3\u003e\n\u003cp\u003eIn calculating the $n^{th}$ fibonacci value, there was only one parameter changed in repeated subproblems, therefore we only needed a $n$ sized 1D memoisation table.\u003c/p\u003e\n\u003cp\u003eHowever, for the integer knapsack problem, the value of each subproblem is dependent on both $i$ and $c$ so therefore we need a 2D $n \\times c$ memoisation table.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003e# Bottom up solution for the integer knapsack problem\ndef knapsack(items: list[tuple[int, int]], c: int) -\u0026gt; int:\n    n = len(items)\n    memo = [[0] * (n + 1) for _ in range(c + 1)]\n\n    # Build table in bottom up manner\n    for i in range(n + 1):\n        for w in range(c + 1):\n            i_value, i_weight = items[i]\n            # If no more items or remaining capacity is 0\n            if i == 0 or w == 0:\n                memo[i][w] = 0\n            # If we can have the new item without going over capacity\n            elif items[i][1] \u0026lt;= w:\n                include_v = memo[i - 1][w - i_weight] + i_value\n                ignore_v = memo[i - 1][w]\n                # Choose the option that gives the most value\n                items[i][w] = max(include_v, ignore_v)\n            # If we cannot fit any more items\n            else:\n                memo[i][w] = memo[i - 1][w]\n\n    return memo[n][c]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause at \u003ccode\u003ememo[i][j]\u003c/code\u003e will always be the maximum value the bag can hold, after considering whether to include or not the first \u003ccode\u003ei\u003c/code\u003e items, and when the bag has a maximum capacity of \u003ccode\u003ej\u003c/code\u003e, then therefore \u003ccode\u003ememo[n][c]\u003c/code\u003e will be the maximum value a bag of capacity \u003ccode\u003ec\u003c/code\u003e can hold after considering all \u003ccode\u003en\u003c/code\u003e items.\u003c/p\u003e\n\u003ch4 id=\"time-complexity\"\u003eTime Complexity\u003c/h4\u003e\n\u003cp\u003eThe final time complexity is $O(nc)$ as there is a nested loop that loops $n \\times c$ times, and each operation within the loop is $O(1)$ as they all only require $O(1)$ array access or arithmetic.\nNote that because the runtime is dependent on the \u003cstrong\u003enumeric value\u003c/strong\u003e of the input, and not just the \u003cstrong\u003elength\u003c/strong\u003e of the input size, the runtime of this algorithm is not polynomial, but \u003cstrong\u003epseudo-polynomial\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4 id=\"space-complexity\"\u003eSpace Complexity\u003c/h4\u003e\n\u003cp\u003eThe space complexity is $O(nc)$ since there is a 2D $n \\times c$ memoisation table to store the solutions of previous computations.\nIt is important to keep in mind that despite the bottom up solution for fibonacci having a space complexity of $O(1)$, the space complexity here is the same as the top down solution as all previous computations need to be remembered.\nSimilar to runtime, this also has a pseudo-polynomial space complexity.\u003c/p\u003e\n\u003ch1 id=\"linear-programming\"\u003e\u003ca href=\"#linear-programming\"\u003eLinear Programming\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003cp\u003eLinear programming is an optimisation method to maximise or minimise a linear function (our objective) given linear constraints on variables.\u003c/p\u003e\n\u003cp\u003eThis can be represented in mathematical model with\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eComponent\u003c/th\u003e\n\u003cth\u003eMathematical Representation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eVariables\u003c/td\u003e\n\u003ctd\u003e$x_j \\geq 0 \\text{ for } 1 \\leq j \\leq n$\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eObjective\u003c/td\u003e\n\u003ctd\u003e$\\text{maximise or minimise } \\sum^{n}_{j = 1} c_j x_j$\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eConstraints\u003c/td\u003e\n\u003ctd\u003e$\\sum^{n}_{j = 1} a_{ij} x_j R_i b_i, \\text{ for } 1 \\leq i \\leq m \\text{ with } R_i \\in \\{\\leq, =, \\geq \\}$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eA \u003cem\u003efeasible solution\u003c/em\u003e is a variable assignment satisfying all constraints.\u003c/p\u003e\n\u003cp\u003eAn \u003cem\u003eoptimal solution\u003c/em\u003e is a feasible solution satisfying the objective.\u003c/p\u003e\n\u003ch2 id=\"canonical-form\"\u003eCanonical form\u003c/h2\u003e\n\u003cp\u003eA linear programming formulation can also be represented by the canonical form.\u003c/p\u003e\n\u003cp\u003emaximise\u003c/p\u003e\n\u003cp\u003e$$\\textbf{c}^\\text{T}\\textbf{x}$$\u003c/p\u003e\n\u003cp\u003esubject to the constraints\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nA\\textbf{x} \u0026amp; \\leq \\textbf{b} \\\\\n\\textbf{x} \u0026amp; \\geq 0.\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cp\u003e$$\n\\textbf{x} =\n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix},\n\\textbf{c} =\n\\begin{pmatrix}\nc_1 \\\\\nc_2 \\\\\n\\vdots \\\\\nc_n\n\\end{pmatrix},\n\\textbf{b} =\n\\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{pmatrix},\nA =\n\\begin{pmatrix}\na_{1, 1} \u0026amp; a_{1, 2} \u0026amp; \\ldots \u0026amp; a_{1, m} \\\\\na_{2, 1} \u0026amp; a_{2, 2} \u0026amp; \\ldots \u0026amp; a_{2, m} \\\\\n\\vdots   \u0026amp; \\vdots   \u0026amp; \\ddots \u0026amp; \\vdots   \\\\\na_{n, 1} \u0026amp; a_{n, 2} \u0026amp; \\ldots \u0026amp; a_{n, m}\n\\end{pmatrix}.\n$$\u003c/p\u003e\n\u003ch2 id=\"runtime-1\"\u003eRuntime\u003c/h2\u003e\n\u003cp\u003eSolving a LP (Linear Programming) formulation is in \u003cstrong\u003eP\u003c/strong\u003e (has a polynomial runtime), whilst solving an ILP (Integer Linear Programming - similar to LP, except that all the variables must be integers) formulation is in \u003cstrong\u003eNP\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn practice, given an optimisation problem, a person formulates the linear programming problem, which is then given to an LP solver which uses algorithms such as the simplex algorithm.\u003c/p\u003e\n\u003ch1 id=\"reductions\"\u003e\u003ca href=\"#reductions\"\u003eReductions\u003c/a\u003e\u003c/h1\u003e\n\n\u003chr\u003e\n\u003cp\u003eReductions are the conversion of one problem to another. An efficient reduction from A to B may be used to show that B is at least as difficult as A.\u003c/p\u003e\n\u003ch2 id=\"complexity-classes\"\u003eComplexity Classes\u003c/h2\u003e\n\u003ch3 id=\"p-polynomial\"\u003eP (Polynomial)\u003c/h3\u003e\n\u003cp\u003eA problem $A(n)$ is in class P, denoted by $A \\in \\mathbf{P}$, if there is an algorithm which solve it in polynomial time with regard to the size of the input.\u003c/p\u003e\n\u003ch3 id=\"np-non-deterministic-polynomial-time\"\u003eNP (Non deterministic Polynomial Time)\u003c/h3\u003e\n\u003cp\u003eA problem $A(n)$ is in class NP, denoted by $A \\in \\mathbf{NP}$, if there is an algorithm which can verify if a solution is correct or not in polynomial time with regard to the size of the input.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA problem that is in P is also in NP.\u003c/li\u003e\n\u003cli\u003eHence, NP problems are at least as hard as P problems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"np-hard-and-np-complete\"\u003eNP-Hard and NP-Complete\u003c/h3\u003e\n\u003cp\u003eA problem is NP-Hard if any problem in NP is reducible to it (informally, a problem is NP-Hard if it it is at least as hard as the hardest problems in NP).\u003c/p\u003e\n\u003cp\u003eA problem is NP-Complete if it is in both NP, and NP-Hard (informally, a problem is NP-Complete if it is one of the hardest problems in NP, the \u0026quot;complete\u0026quot; meaning that it is able to simulate any problem in the same complexity class through reductions).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHence, NP-Hard problems are at least as hard as NP-Complete.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"np-problems\"\u003eNP Problems\u003c/h2\u003e\n\u003ch3 id=\"3sat\"\u003e3SAT\u003c/h3\u003e\n\u003cp\u003eThe SAT problem is: Given a propositional formula in the CNF form $C_1 \\land C_2 \\land ... \\land C_n$ where each clause $C_i$ is a disjunction of propositional variables or their negations, i.e.\u003c/p\u003e\n\u003cp\u003e$$\n(P_1 \\lor \\lnot P_2 \\lor P_3 \\lor \\lnot P_5) \\land (P_2 \\lor P_3 \\lor \\lnot P_5 \\lor \\lnot P_6) \\land (\\lnot P_3 \\lor \\lnot P_4 \\lor P_5)\n$$\u003c/p\u003e\n\u003cp\u003eIs there some assignment of boolean values to $P_1, P_2, ..., P_6$ which makes the formula true?\u003c/p\u003e\n\u003cp\u003eIf each clause involves exactly 3 variables, then this problem is 3SAT.\u003c/p\u003e\n\u003ch2 id=\"cooks-theorem-1982-turing-award\"\u003eCook\u0026#39;s Theorem (1982 Turing Award)\u003c/h2\u003e\n\u003cp\u003eTheorem: Every NP problem is polynomially reducible to the SAT problem\u003c/p\u003e\n\u003cp\u003eThis means that if the SAT problem is solvable in polynomial time, P = NP, as other NP problems can be reduced to SAT in polynomial time and then solved in polynomial time.\u003c/p\u003e\n\u003ch2 id=\"karps-21-problems-1972-1985-turing-award\"\u003eKarp\u0026#39;s 21 Problems [1972] (1985 Turing Award)\u003c/h2\u003e\n\u003cp\u003eKarp\u0026#39;s 21 problems are a set of problems which show that there is a many-one reduction from the SAT problem to other NP problems, therefore showing they are all NP-complete.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSatisfiability\u003cul\u003e\n\u003cli\u003e0 - 1 Integer Programming\u003c/li\u003e\n\u003cli\u003eClique\u003cul\u003e\n\u003cli\u003eSet Packing\u003c/li\u003e\n\u003cli\u003eVertex Cover\u003cul\u003e\n\u003cli\u003eSet Cover\u003c/li\u003e\n\u003cli\u003eFeedback Node set\u003c/li\u003e\n\u003cli\u003eFeedback Arc Set\u003c/li\u003e\n\u003cli\u003eDirect Hamiltonian Cycle\u003cul\u003e\n\u003cli\u003eUndirected Hamiltonian Cycle\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3SAT\u003cul\u003e\n\u003cli\u003eGraph Coloring\u003cul\u003e\n\u003cli\u003eClique Cover\u003c/li\u003e\n\u003cli\u003eExact cover\u003cul\u003e\n\u003cli\u003eHitting Set\u003c/li\u003e\n\u003cli\u003eSteiner Tree\u003c/li\u003e\n\u003cli\u003e3D Match\u003c/li\u003e\n\u003cli\u003eSubset Sum\u003cul\u003e\n\u003cli\u003eJob Sequencing\u003c/li\u003e\n\u003cli\u003ePartition\u003cul\u003e\n\u003cli\u003eMax Cut\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"3gc-to-sat-reduction\"\u003e3GC to SAT Reduction\u003c/h2\u003e\n\u003cp\u003e3GC AKA 3 Graph Colouring is a problem that asks given an undirected graph $G = (V, E)$, and a set of colours $C = \\{r, g, b\\}$, is there a function $f: V \\rightarrow C$ such that if $(v, w) \\in E$ then $f(v) \\neq f(w)$.\u003c/p\u003e\n\u003cp\u003eUsing the notation that $v_i$ is a proposition that is \u003ccode\u003etrue\u003c/code\u003e if vertex $v$ is the colour $i$, we can use the following rules to complete the reduction.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEnforce that each vertex is \u003cstrong\u003eone colour only\u003c/strong\u003e.\nWe can enforce this through 2 rules.\u003col\u003e\n\u003cli\u003eA vertex is \u003cstrong\u003eno more than one colour\u003c/strong\u003e:\n$$\\forall v \\in V : (\\lnot v_1 \\lor \\lnot v_2) \\land (\\lnot v_1 \\lor \\lnot v_3) \\land (\\lnot v_2 \\land \\lnot v_3)$$\nIf any of the vertices are more than one colour, then both propositions will be \u003ccode\u003etrue\u003c/code\u003e inside the clause. Since we take the negation of the proposition, both propositions are evaluated as \u003ccode\u003efalse\u003c/code\u003e, and their disjunction evaluates to \u003ccode\u003efalse\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eA vertex is \u003cstrong\u003eat least one colour\u003c/strong\u003e:\n$$\\forall v \\in V : (v_1 \\lor v_2 \\lor v_3)$$\nIf a vertex is not any colour, then the clause evaluates to \u003ccode\u003efalse\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eEnforce that \u003cstrong\u003eadjacent vertices are not the same colour\u003c/strong\u003e.\n$$\\forall (v, w) \\in E : (\\lnot v_1 \\lor \\lnot w_1) \\land (\\lnot v_2 \\lor \\lnot w_2) \\land (\\lnot v_3 \\lor \\lnot w_3)$$\nSimilar to 1.a, if any adjacent vertices are the same colour, then the clause evaluates to \u003ccode\u003efalse\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n","metadata":{"title":"COMP3821","description":"Extended Algorithms and Programming Techniques","date":"2021-04-25","mathjax":true,"hljs":true},"toc":[{"id":"misc-knowledge","title":"Misc Knowledge"},{"id":"divide-and-conquer","title":"Divide and Conquer"},{"id":"greedy-algorithms","title":"Greedy Algorithms"},{"id":"dynamic-programming","title":"Dynamic Programming"},{"id":"linear-programming","title":"Linear Programming"},{"id":"reductions","title":"Reductions"}]},"__N_SSG":true},"page":"/writings/[slug]","query":{"slug":"comp3821"},"buildId":"ILkuclJ7g-R-hn-4-ve6L","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>