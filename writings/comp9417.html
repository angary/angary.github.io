<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>COMP9417</title><meta name="next-head-count" content="3"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#ffffff"/><link rel="preload" href="/_next/static/css/f553854d468f6c8c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f553854d468f6c8c.css" data-n-g=""/><link rel="preload" href="/_next/static/css/33bcc2031ebe3241.css" as="style"/><link rel="stylesheet" href="/_next/static/css/33bcc2031ebe3241.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-0ecb9ccfcb6c9b24.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7db202e315b716c1.js" defer=""></script><script src="/_next/static/chunks/996-502a5e68cb1e9d91.js" defer=""></script><script src="/_next/static/chunks/207-81e7acd450235f3f.js" defer=""></script><script src="/_next/static/chunks/pages/writings/%5Bslug%5D-7e316cc9cfef0534.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_buildManifest.js" defer=""></script><script src="/_next/static/ILkuclJ7g-R-hn-4-ve6L/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,n='data-theme',s='setAttribute';var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';d[s](n,'dark')}else{d.style.colorScheme = 'light';d[s](n,'light')}}else if(e){d[s](n,e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="Writing_body__d5Mqz"><div class="Writing_layout__X8ptu"><aside class="Writing_toc__C4kbO" aria-label="Table of contents"><h2 class="Writing_tocTitle__8Z0ek">Contents</h2><ul class="Writing_tocList__2Nnoy"><li class="Writing_tocItem__rjAOC"><a href="#statistical-techniques-for-data-analysis" class="Writing_tocLinkActive__0kigr">Statistical Techniques for Data Analysis</a></li><li class="Writing_tocItem__rjAOC"><a href="#regression" class="Writing_tocLink__ztBHG">Regression</a></li><li class="Writing_tocItem__rjAOC"><a href="#classification" class="Writing_tocLink__ztBHG">Classification</a></li><li class="Writing_tocItem__rjAOC"><a href="#tree-learning" class="Writing_tocLink__ztBHG">Tree Learning</a></li><li class="Writing_tocItem__rjAOC"><a href="#kernel-methods" class="Writing_tocLink__ztBHG">Kernel Methods</a></li><li class="Writing_tocItem__rjAOC"><a href="#ensemble-learning" class="Writing_tocLink__ztBHG">Ensemble Learning</a></li><li class="Writing_tocItem__rjAOC"><a href="#neural-learning" class="Writing_tocLink__ztBHG">Neural Learning</a></li><li class="Writing_tocItem__rjAOC"><a href="#unsupervised-learning" class="Writing_tocLink__ztBHG">Unsupervised Learning</a></li><li class="Writing_tocItem__rjAOC"><a href="#learning-theory" class="Writing_tocLink__ztBHG">Learning Theory</a></li></ul></aside><aside class="Writing_balanceNav__vSGQh" aria-label="Site navigation"><a class="Writing_siteTitle__O17hB" href="/">Gary Sun // <span class="cn">孫健</span></a><ul class="Writing_siteNavList__zzxAq"><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#about">About</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="https://github.com/angary/">Projects</a></li><li class="Writing_siteNavItem__TeLqH"><a class="Writing_siteNavLink__EVB8A" href="/#writings">Writings</a></li></ul></aside><div class="Writing_articleHeader__rl9TS"><h1>COMP9417</h1><p class="Writing_description__Rk8wD">Machine Learning and Data Mining</p><p class="Writing_date__BYvoK">2 May 2022</p></div><div class="Writing_articleBody__A1uFB"><span style="display:block"><article><h1 id="statistical-techniques-for-data-analysis"><a href="#statistical-techniques-for-data-analysis">Statistical Techniques for Data Analysis</a></h1>

<p>Probability vs Statistics</p>
<ul>
<li><strong>Probability:</strong> reasons from populations to samples (deductive reasoning)</li>
<li><strong>Statistics:</strong> reasons from samples to populations (inductive reasoning)</li>
</ul>
<p><strong>Sampling</strong> is a way to draw conclusions about the population without measuring the whole population.
The characteristics of an ideal sampling model include</p>
<ul>
<li>No systematic bias</li>
<li>The chance of obtaining an unrepresentative sample can be calculated</li>
<li>The chance of obtaining an unrepresentative sample decreases with the size of the sample</li>
</ul>
<h2 id="estimation">Estimation</h2>
<p>In statistics, estimation refers to the process by which one makes inferences about a population.</p>
<h2 id="mean">Mean</h2>
<p>The arithmetic mean is $m = \frac{1}{n}\sum_{i=1}^n x_i$, where the observations are $x_1, x_2, ..., x_n$.
If $x_i$ occurs $f_i$ times, and we define relative frequencies as $p_i = \frac{f_i}{n}$, then the mean is $m = \sum_i x_i p_i$.
Therefore the expected value of a discrete random variable $X$ is:</p>
<p>$$E(X) = \sum_i x_i P(X=x_i)$$</p>
<h2 id="variance">Variance</h2>
<p>Variance measures how far a set of random numbers are spread out from their average value.
Standard deviation (square root of variance) is estimated as:</p>
<p>$$s = \sqrt{\frac{1}{n-1} \sum_i (x_i - m)^2}$$</p>
<p>We can write this in terms of expected values $E(X)$ as</p>
<p>$$Var(x) = E((X - E(X))^2) = E(X^2) - [E(x)]^2$$</p>
<p>Variance is the &quot;mean of squares minus the squares of means&quot;</p>
<h2 id="covariance-and-correlation">Covariance and Correlation</h2>
<p><strong>Covariance</strong> is a measure of relationship between two random variables:</p>
<p>$$cov(x,y) = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{n-1} = \frac{(\sum_i x_i y_i) - n \bar{x} \bar{y}}{n-1}$$</p>
<p><strong>Correlation</strong> is a measure to show how strongly a pair of random variables are related:</p>
<p>$$r = \frac{cov(x,y)}{\sqrt{var(x)}\sqrt{var(y)}}$$</p>
<p>This is also called Pearson&#39;s correlation coefficient.</p>
<ul>
<li>The correlation coefficient is a number between $-1$ and $+1$ that shows whether a pair of variables $x$ and $y$ are associated or not and whether their scatter in the association is high or low:<ul>
<li>A value close to 1 shows signifies strong positive association between $x$ and $y$, while a value close to $-1$ shows a strong inverse association</li>
<li>A value near 0 indicates there is no association and there is a large scatter</li>
</ul>
</li>
<li>This is only approximate when $x$ and $y$ are roughly linearly associated (does not work well with association is curved)</li>
<li>Do not use correlation to imply $x$ causes $y$ or the other way around</li>
</ul>
<h2 id="bias-and-variance">Bias and Variance</h2>
<p><strong>Bias</strong> is the difference between the average prediction of our model and the correct value.</p>
<ul>
<li>Models with high bias pays very little attention to the training data and oversimplifies the model.</li>
<li>It always leads to high error on training and test data.</li>
</ul>
<p><strong>Variance</strong> is the variability of model prediction for a given data point or a value which tells us spread of our data.</p>
<ul>
<li>Models with high variances pays a lot of attention to training data and does not generalize unseen data.</li>
<li>As a result, such models perform very well on training data but has high error rates on test data.</li>
</ul>
<h3 id="bias-variance-decomposition">Bias-Variance Decomposition</h3>
<p>When we assume $y = f + \epsilon$ and we estimate $f$ with $\hat{f}$, the expectation of error:</p>
<p>$$E[(y - \hat{f})^2] = (f - E[\hat{f}])^2 + Var(\hat{f}) + Var(\epsilon)$$</p>
<p>So, the mean of squared error (MSE) can be written as</p>
<p>$$MSE = Bias^2 + Variance + Irreducible Error$$</p>
<ul>
<li><strong>Irreducible error</strong> is associated with a natural variability in a system (noise).
It can not be reduced since it is due to unknown factors or due to chance.</li>
<li><strong>Reducible error</strong>, as the name suggests, can be and should be minimized further by adjustments to the model.</li>
</ul>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<p>When comparing unbiased estimators, we would like select the one with minimum variance</p>
<ul>
<li>If our model is too simple and has very few parameters, then it may have high bias and low variance.</li>
<li>If it has a large number of parameters, it may have high variance and low bias.</li>
</ul>
<h1 id="regression"><a href="#regression">Regression</a></h1>

<p>Regression can be used to predict numeric values from numeric attributes.
In linear models, the outcome is a linear combination of attributes:</p>
<p>$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n = \sum_{i=0}^n \theta_i x_i = h(x)$$</p>
<p>There is:</p>
<ul>
<li>Univariate regression: one input variable/feature/attribute is used to predict one output variable</li>
<li>Multiple regression / multivariable regression: more than one variable/features/attributes are used to predict one output variable</li>
</ul>
<h2 id="least-squares">Least Squares</h2>
<p>The most popular estimation model is &quot;Least Squares&quot; for fitting a linear regression.
Error is defined as difference between the predicted and actual value, i.e.</p>
<p>$$J(\theta) = \sum_{y=1}^m \left(y_j - \sum_{i=0}^n \theta_i x_{ji} \right)^2 = \sum_{j=1}^m \left(y_j - x_j^T \theta \right)^2 = (y - X \theta)^T (y - X \theta)$$</p>
<p>We want to minimize the error over all samples.</p>
<h2 id="gradient-descent">Gradient Descent</h2>
<p>The goal of Linear Regression is to minimise $J(\theta)$.
If we compute $J(\theta)$ for different values of $\theta$, we get a convex function with one global minima.
To find the value of $\theta$ that provides this minimum, we can use gradient descent.</p>
<p>Gradient descent starts with some initial $\theta$, and repeatedly performs an update:</p>
<p>$$\theta_i^{(t+1)} := \theta_t^{(t)} - \alpha \frac{\partial}{\partial \theta_i} J \left( \theta_i^{(t)} \right)$$</p>
<p>where $\alpha$ is the learning rate / step size, and each iteration it takes a step in the direction of the steepest decrease in $J(\theta)$.</p>
<p>The partial derivative term for $J(\theta)$ can be determined as</p>
<p>$$\frac{\partial}{\partial \theta_i} J(\theta) = -2 \left( y_j - h_\theta (x_j) \right) x_{ji}$$</p>
<p>So, for a <strong>single training sample</strong>, the update Least Mean Squares (LMS) rule is:</p>
<p>$$\theta_{i+1} := \theta_i + 2 \alpha \left( y_j - h_\theta (x_j) \right) x_{ji}$$</p>
<p>For other samples, there are two methods.</p>
<ol>
<li><strong>Batch Gradient Descent:</strong>
$$\theta_i^{t+1} = \theta_i^{(t)} + \alpha \frac{2}{m} \sum_{j=1}^m \left( y_j - h_{\theta^{(t)}} (x_j) \right) x_{ji}$$
Replace the gradient with the sum of gradient for all samples and continue until convergence (when the estimated $\theta$ is stabilized).</li>
<li><strong>Stochastic Gradient Descent:</strong>
$$ \text{ for } j = 1 \text{ to } m \left\{ \theta _ i = \theta_i + 2 \alpha \left( y_jj - h _ \theta (x _ j) \right) x _ ji \text{ for every } i \right\}$$</li>
</ol>
<p>In stochastic gradient descent, $\theta$ gets updated at any sample separately, so it is faster to calculate, but may never converge to the minimum.</p>
<h2 id="minimizing-squared-error-normal-equations">Minimizing Squared Error (normal equations)</h2>
<p>Gradient descent is an iterative algorithm, however, you may find the minimum of $J(\theta)$ by finding and setting it&#39;s derivatives to 0.
This is also called the exact or closed-form solution.</p>
<p>$$
\frac{\partial}{\partial \theta} J(\theta) = 0 \\
J(\theta) = ( \mathbb{y} - X \theta )^T (\mathbb{y} - X \theta) \\
\frac{\partial}{\partial \theta} J(\theta) = - 2X^T (\mathbb{y} - X \theta) = 0 \\
X^T (\mathbb{y} - X \theta) = 0 \\
\theta = (X^TX)^{-1}X^T \mathbb{y}
$$</p>
<h2 id="minimizing-squared-error-normal-equations-1">Minimizing Squared Error (normal equations)</h2>
<p>We can write the relationship between input variable $x$ and output variable $y$ as:</p>
<p>$$y_j = x_j^T \theta + \epsilon_j$$</p>
<p>And $\epsilon_j$ is an error term which might be unmodeled effect or random noise.
Assume $\epsilon_j$&#39;s are independent and identically distributed (i.i.d.) according to a Gaussian distribution, i.e.:</p>
<p>$$
\epsilon_j \sim N \left( 0, \sigma^2 \right) \\
p(\epsilon_j) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{e_j^2}{2 \sigma^2} \right)
$$</p>
<p>This implies that:</p>
<p>$$p(\epsilon_j) = p(y_j | x_j; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{e_j^2}{2 \sigma^2} \right)$$</p>
<p>So we want to estimate $\theta$ s.t. we maximise the probability of output $y$ given input $x$ over all $m$ training samples:</p>
<p>$$
\begin{align*}
\mathcal{L}(\theta)
    &amp;= p(\mathbb{y} | X, \theta) \\
    &amp;= \prod_{j=1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{\left( y_j - x_j^T \theta \right)^2}{2 \sigma^2} \right)
\end{align*}
$$</p>
<p>Note this is called the <strong>Likelihood</strong> function.</p>
<p>However, to find $\theta$ that maximises $\mathcal{L}(\theta)$, we can also maximise any strictly increasing function of $\mathcal{L}(\theta)$.
In this case, we can find <strong>maximize the log likelihood</strong> $mathcal{l}(\theta)$:</p>
<p>$$
\begin{align*}
\mathcal{l}(\theta)
    &amp;= \log \mathcal{L}(\theta) \\
    &amp;= \log \prod_{j=1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( - \frac{\left( y_j - x_j^T \theta \right)^2}{2 \sigma^2} \right) \\
    &amp;= m \log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) - \frac{1}{\sigma^2} \frac{1}{2} \sum_{j=1}^m \left( y_j - x_j^T \theta \right)^2
\end{align*}
$$</p>
<p>So maximizing $\mathcal{l}(\theta)$ is equal to minimising $\sum_{j=1}^m \left( y_j - x_j^T \theta \right)^2$.
This means under certain assumptions, the least squared regression is equivalent to finding maximum likelihood estimate of $\theta$.</p>
<h2 id="linear-regression-assumptions">Linear Regression Assumptions</h2>
<ul>
<li><strong>Linearity:</strong> The relationship between $x$ and the mean of $y$ is linear</li>
<li><strong>Homoscedasticity:</strong> The variance of residual is the same of any value of $x$</li>
<li><strong>Independence:</strong> Observations are independent of each other</li>
<li><strong>Normality:</strong> For any fixed value of $x$, $y$ is normally distributed</li>
</ul>
<h2 id="univariate-linear-regression">Univariate Linear Regression</h2>
<p>In univariate regression we aim to find the relationship between $y$ and one independent variable $x$.</p>
<p>As an example, if we want to investigate the relationship between height and weight, we could collect $m$ measurements</p>
<p>$$(h_j, w_j), \quad j = 1, ..., m$$</p>
<p>Univariate linear regression assumes a linear relation, $\hat{w} = \theta_0 + \theta_1 h$.</p>
<h2 id="multiple-linear-regression">Multiple Linear Regression</h2>
<p>Here, we model the relationship of $y$ to several other variables.</p>
<p>As an example, say we now want to predict people&#39;s weight from their height and body frame size (i.e. wrist size).
We collect $m$, height, weight and body frame measurements:</p>
<p>$$(h_j, f_j, w_j), \quad j = 1, ..., m$$</p>
<p>The linear regression model is now:</p>
<p>$$\hat{w} = \theta_0 + \theta_1 h + \theta_2 f$$</p>
<h2 id="linear-regression-for-curves">Linear Regression for curves</h2>
<p>We can also apply some changes to produce curves with linear regression, i.e.</p>
<p>To model:</p>
<p>$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$$</p>
<p>we can treat $x_2 = x_1^2$ and use the model</p>
<p>$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2$$</p>
<p>These nonlinear models can still be treated like linear regression and can fit curvature.
They are still linear in parameters.
Non linear regression is not linear in parameters, i.e., $\hat{y} = \frac{\theta_1 x}{\theta_2 + x}$</p>
<p>However, increasing the degree of the model can result in overfitting.</p>
<h2 id="regularisation">Regularisation</h2>
<p>Regularisation is a method to avoid overfitting by adding constraints to the weight vector.
An approach is to ensure the weights are, small in magnitude: this is called shrinkage.</p>
<p>The cost function, given data $(x_1, y_1), ..., (x_m, y_m)$ is</p>
<p>$$J(\theta) = \sum_j (y_j - h_\theta (x_j))^2 + \lambda \sum_i \theta_i^2$$</p>
<ul>
<li>Parameter estimation by optimisation wil attempt to find values for $\theta_0, ..., \theta_n$ s.t. $J(\theta)$ is a minimum</li>
<li>Similar to before, this can be solved by gradient descent or finding the closed form solution</li>
</ul>
<p>The multiple least-squares regression problem is an optimisation problem, and can be written as:</p>
<p>$$\theta* = \arg \min_\theta (y - X \theta)^T (y - X\theta)$$</p>
<p>The regularised version of this is then as follows:</p>
<p>$$\theta* = \arg \min (y - X \theta)^T (y - X \theta) + \lambda || \theta ||^2$$</p>
<p>Where $||\theta||^2 = \sum_i \theta_i^2$ is the squared norm of the vector $\theta$, or equivalently, the dot product $\theta^T\theta$; $\lambda$ is a scalar determining the amount of regularisation.</p>
<p>This still has a closed form solution</p>
<p>$$\theta = (X^TX + \lambda I)^{-1} X^T y$$</p>
<p>where $I$ is the identity matrix.
Regularisation amounts to adding $\lambda$ to the diagonal of $X^TX$, improves the numerical stability of matrix inversion.
This form of least-squares regression is known as <strong>ridge regression</strong>.</p>
<p>An alterative of regularised regression is provided by the <strong>lasso</strong>.
It replaces the term $\sum_i \theta_i^2$ with $\sum_i |\theta_i |$.
The result is that some weights are shrunk, but others are set to 0, hence this favours sparse solutions.</p>
<h2 id="model-selection">Model Selection</h2>
<p>Suppose there are a lot of variables / features $(x)$.
Taking all the features will lead to an overly complex model.
There are 3 ways to reduce complexity.</p>
<ol>
<li><strong>Subset-selection</strong>, by search over subset lattice.
Each subset results in a new model, and the problem is so select one of the models.</li>
<li><strong>Shrinkage</strong>, or regularization of coefficients to zero, by optimization.
There is a single model, and unimportant variables have near-zero coefficients.</li>
<li><strong>Dimensionality-reduction</strong>, by projecting points into a lower dimensional space (this is different to subset-selection)</li>
</ol>
<h1 id="classification"><a href="#classification">Classification</a></h1>

<p>Classification doesn&#39;t have convenient mathematical properties, so in classification, train a classifier, which is usually a function that maps from an input data point to a set of discrete outputs (i.e. the classes).</p>
<ul>
<li><strong>Generative algorithm:</strong>
builds some models for each of the classes and then makes classification predictions based on looking at the test example see it is more similar to which of the models<ul>
<li>Learns $p(x|y)$</li>
<li>So, we can get $p(x,y) = p(x|y)p(y)$</li>
<li>It learns the mechanism by which the data has been generated</li>
</ul>
</li>
<li><strong>Discriminative algorithm:</strong>
Do not build models for difference classes, but rather focuses on finding a decision boundary that separates classes<ul>
<li>Learns $p(y|x)$</li>
</ul>
</li>
</ul>
<h2 id="linear-classification">Linear Classification</h2>
<p>For a linear classifier of 2 features and 2 classes</p>
<ul>
<li>We a line that separates the two classes: $ax_1 + bx_2 + c = 0$.</li>
<li>We define a weight vector $w^T = [a,b]$, $x^T = [x_1,x_2]$.</li>
<li>So, the line can be defined by $x^Tw = -c = t$</li>
<li>$w$ is perpendicular to the decision boundary</li>
<li>$t$ is the decision threshold (if $x^Tw &gt; t$, $x$ belongs to $P$ but if $x^Tw &lt; t$, $x$ belongs to $N$)</li>
</ul>
<p>If $p$ and $n$ is the respective centers of mass of the positive and negative points, the basic linear classifier is described by the equation $x^Tw = t$ and $w = p - n$.</p>
<p>As we know, $\frac{p+n}{2}$ is on the decision boundary, so we have:</p>
<p>$$t = \left( \frac{p+n}{2} \right)^T \cdot (p-n) = \frac{||p||^2 - ||n||^2}{2}$$</p>
<p>where $||x||$ denotes the length of vector $x$.</p>
<h2 id="generalisation">Generalisation</h2>
<p>Generalisation means how well a trained model can classify or forecast unseen data.
There are 3 basic assumptions for generalisation</p>
<ol>
<li>Examples are drawn independent and identically (i.i.d) at random for the distribution</li>
<li>The distribution is stationary; that is the distribution doesn&#39;t change within the data set</li>
<li>We always pull from the same distribution (for training, validation and test samples)</li>
</ol>
<h2 id="cross-validation">Cross-validation</h2>
<p>CV is a validation technique to assess the results of a model to an independent data set.
A portion of the dataset is used to train the model, and another is used to test.
Types of CV include:</p>
<ol>
<li><strong>Holdout method</strong>
Randomly assign data points to two sets $d_0$ (training set) and $d_1$ (test set).</li>
<li><strong>Leave-One-Out Cross validation (LOOCV)</strong>
A variation of the leave-p-out cross validation where 1 sample is left out, to test, whilst the rest is used for training.</li>
<li><strong>K-fold Cross Validation</strong>
Partition the dataset into $k$ equally sized subsamples.
Of the $k$ subsamples, one subsample is used as for testing, and the rest for training.
Repeat with all subsets to produce a single estimation</li>
</ol>
<p>There are certain parameters that need to be estimated during learning.
We use the data, but NOT the training set, OR the test set.
Instead we use a separate validation or development set.</p>
<p><strong>Validation set:</strong> To make the hyperparameter tuning and model selection independent from the test set, we define another set within the train set.</p>
<h2 id="evaluation-of-error">Evaluation of error</h2>
<p>If we have a binary classification, we can have a contingency table, AKA confusion matrix:</p>
<table>
<thead>
<tr>
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Actual Positive</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr>
<td><strong>Actual Negative</strong></td>
<td>False Positive (FN)</td>
<td>True Negative (TN)</td>
</tr>
</tbody></table>
<p>Classification Accuracy on a sample of labelled pairs $(x,c(x))$ given a learned classification model that predicts, for each instance $x$, a class value $\hat{c}(x)$:</p>
<p>$$acc = \frac{1}{|Test|} \sum_{x \in Test} I[ \hat{c}(x) = c(x) ]$$</p>
<p>where $Test$ is a test set, and $I[]$ is the indicator function which is 1 iff its argument evaluates to true and 0 otherwise.
The classification Error $= 1 - acc$.</p>
<p>Other evaluation metrics:</p>
<p><strong>Precision / correctness</strong></p>
<ul>
<li>is the number of relevant objects classified correctly divided byt he total number of relevant objects classified</li>
</ul>
<p>$$Precision = \frac{TP}{TP + FP}$$</p>
<p><strong>Recall / sensitivity / completeness / true positive rate (TPR)</strong>
is the number of relevant objects classified correctly divided by total number of relevant / correct objects</p>
<p>$$Recall = \frac{TP}{TP + FN}$$</p>
<p><strong>F1 score</strong>
is the harmonic mean of precision and recall and is defined as:</p>
<p>$$F_1 = \frac{2 \times precision \times recall}{precision + recall}$$</p>
<p>This measure gives equal importance to precision and recall which is sometime undesirable; so, we have to decide which metric to use depending on the task and what&#39;s important for the task.</p>
<p><strong>AUC-ROC (Area Under the Curve - Receiver Operating Characteristics)</strong>
is an important curve for performance of classification models.
It evaluates the model at different threshold settings and can inform the capability of the model in distinguishing between classes.</p>
<ul>
<li>$TPR = \frac{TP}{TP+FN}$</li>
<li>$FPR = \frac{FP}{FP+TN}$</li>
<li>A good model has AUC close to 1, a very poor model has AUC close to 0, if the AUC = 0.5, it means there is no class separation</li>
</ul>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-2ea4818fbebd91c5001d9ebf2ca346e6.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<h2 id="missing-values">Missing Values</h2>
<p>Often times data may be incomplete / missing labels.
To handle this, there are several strategies:</p>
<ul>
<li><strong>Delete samples with missing values</strong><ul>
<li>Pros:
A robust and probably more accurate model.</li>
<li>Cons:
Loss of information and data.
Becomes a poorly trained model if percentage of missing values is high.</li>
</ul>
</li>
<li><strong>Replace missing value with mean/median/mode</strong><ul>
<li>Pros:
When data size is small, it is better than deleting and can prevent data loss.</li>
<li>Cons:
Inputting the approximations add bias to the model (it reduces the variance).
Also works poorly compared to other models.</li>
</ul>
</li>
<li><strong>If categorical, assigning a unique category or the most frequent category</strong><ul>
<li>Pros:
Works well with small datasets and easy to implement and results in no loss of data.</li>
<li>Cons:
Unique category works only for categorical features.
Adding another feature (e.g., a new unique category) to the model may result in high variance in the model.
Adding the most frequent category can increase the bias in the model.</li>
</ul>
</li>
<li><strong>Predicting the missing values</strong><ul>
<li>Pros:
Inputting the missing variable is an improvement as long as the bias from it is smaller than the omitted variable bias.
Also yields unbiased estimates of the model parameters.</li>
<li>Cons:
Bias also arises when an incomplete conditioning set is used for a categorical variable.
Considered only as a proxy for the true values.</li>
</ul>
</li>
<li><strong>Using algorithms that support missing values</strong><ul>
<li>Pros:
Does not require creation of a predictive model however, correlation of the data is neglected.</li>
<li>Cons:
Some of these algorithms are very time-consuming and it can be critical in data mining where large databases are being extracted.</li>
</ul>
</li>
</ul>
<h2 id="nearest-neighbour">Nearest Neighbour</h2>
<p>Nearest Neighbour is a regression or classification algorithm that predicts whatever is the output value of the nearest data point to some query.</p>
<p>To find the nearest data point, we have to find the distance between the query and other points. So we have to decide how to define the distance.</p>
<h3 id="minkowski-distance">Minkowski Distance</h3>
<p>If $\mathcal{X} \rightarrow \mathbb{R}^d$, $x, in \in \mathcal{X}$, the Minkowski distance of order $p &gt; 0$ is defined as:</p>
<p>$$Dist_p(x,y) = \left( \sum_{j=1}^d |x_j - y_j|^P \right)^{\frac{1}{p}} = ||x-y||_p$$</p>
<p>Where $||z||_p = \left( \sum_{j=1}^d |z_j|^p \right)^{\frac{1}{p}}$ is the p-norm (sometimes denoted $L_p$ norm) of the vector $z$.</p>
<ul>
<li>The 2-norm refers to the euclidean distance</li>
<li>The 1-norm denotes manhattan distance, also called cityblock distance</li>
</ul>
<p>If we let $p$ grow larger, the distance will be more dominated by the largest coordinate-wise distance, from which we can infer that $Dist_\infty = max_j | x_j - y_j|$; this is also called Chebyshev distance.</p>
<p>If the data is not in $\mathcal{R}^d$, but we can turn it into Boolean features / character sequences, we can still apply distance measures, i.e.</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>Distance Measure</th>
</tr>
</thead>
<tbody><tr>
<td>Equal Length Binary String</td>
<td>Hamming Distance</td>
</tr>
<tr>
<td>Unequal Length Non-Binary String</td>
<td>Levenshtein Distance</td>
</tr>
</tbody></table>
<h3 id="distance-metrics">Distance Metrics</h3>
<p>Given an instance space $\mathcal{X}$, a distance metric is a function $Dist : \mathcal{X} \times \mathcal{X} \rightarrow [0, \infty)$ such that for any $x,y,z \in \mathcal{X}$</p>
<ol>
<li>Distances between a point and itself are zero</li>
<li>All other distances are larger than zero</li>
<li>Distances are symmetric</li>
<li>Detours can not shorten the distance (triangle inequality)</li>
</ol>
<p>If the second condition is weakened to a non-strict inequality - i.e. $Dist(x,y) = 0, x \neq y$, the function $Dist$ is called a pseudo-metric.</p>
<p>The arithmetic mean $\mu$ of a set of data points $D$ in a Euclidean space is the unique point that minimises the sum of squared Euclidean distances to those data points.</p>
<ul>
<li>Note minimising the sum of squared Euclidean distances of a given set of points is the same as minimising the average squared Euclidean distance</li>
<li>If we drop the squares, the point is known as the geometric median<ul>
<li>For univariate data, it corresponds to the median / middle value of a set of numbers</li>
<li>For multivariate data, there is no closed-form expression, and needs to be calculated by successive approximation</li>
</ul>
</li>
</ul>
<h2 id="nearest-centroid-classifier">Nearest Centroid Classifier</h2>
<p>This is a classifier based on minimum distance principle, where the class exemplars are just the centroids (or means).</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple and fast</li>
<li>Works well when classes are compact and far from each other</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>For complex classes (e.g. Multimodal, non-spherical) may give very poor results</li>
<li>Cannot handle outliers or noisy data well and cannot handle missing data</li>
</ul>
<h2 id="nearest-neighbour-classification">Nearest Neighbour Classification</h2>
<p>This is related to the simplest form of learning</p>
<ul>
<li>Training instances are searched for instance that most closely resembles new or query instances</li>
<li>The instances themselves represent the knowledge</li>
<li>Called: instance based, memory based learning or case based learning; often a form of local learning</li>
</ul>
<p>Nearest neighbour:</p>
<ul>
<li>Given query instance $x_q$, first locate nearest training example $x_n$, then estimate $\hat{f}(x_q) \leftarrow f(x_n)$</li>
</ul>
<p>$k$-Nearest neighbour:</p>
<ul>
<li>Given $x_q$ take vote among its $k$ nearest neighbours (if discrete-valued targe function)</li>
<li>Take mean of $f$ values of $k$ nearest neighours (if real valued)
$$\hat{f}(x_q) \leftarrow \frac{\sum_{j=1}^kf(x_j)}{k}$$</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Can be very accurate</li>
<li>Training is very fast</li>
<li>can learn complex target functions</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Slow at query time: basic algorithm scans entire training data to derive a prediction, and suffers from &quot;curse of dimensionality&quot;</li>
<li>Assumes all attributes are equally important, so easily fooled by irrelevant attributes (can be remedied by attribute selection or weights)</li>
<li>Problem of noisy instances (can be remedied by removing those from data set, but it&#39;s difficult to determine which are noisy)</li>
<li>Finding the optimal $k$ can be challenging<ul>
<li>1NN perfectly separates training data, so low bias but high variance</li>
<li>By increasing $k$, we increase bias and decrease variance</li>
</ul>
</li>
<li>Needs homogenous feature type and scale</li>
</ul>
<p>Note that the data needs to be normalized because different attributes may be measured on different scales, i.e.
$$x&#39; = \frac{x - \min(x)}{\max(x) - \min(x)}$$</p>
<p>where $x$ is the actual value of attribute / feature, and $x&#39;$ is the normalized value.</p>
<p>Nearest Neighbour should be considered when</p>
<ul>
<li>Instances map to points in $\mathbb{R}^d$</li>
<li>There are less than 20 attributes per instance (or number of attributes can be reduced)</li>
<li>There is lots of training data</li>
<li>No requirement for &quot;explanatory&quot; model to be learned</li>
</ul>
<h2 id="distance-weighted-knn">Distance-Weighted KNN</h2>
<p>We might want to use the distance function to construct a weight $w_i$.
The final line of the classification algorithm can be changed to be</p>
<p>$$\hat{f}(x_q) \leftarrow \arg \max_{v \in V} \sum_{i=1}^k w_i \delta(v,(f(x_j)))$$</p>
<p>where</p>
<p>$$w_i = \frac{1}{Dist(x_q, x_i)^2}$$</p>
<p>For real-valued target functions, the final line can be changed to be</p>
<p>$$\hat{f}(x_q) \leftarrow \frac{\sum_{i=1}^k w_i f(x_i)}{\sum_{i=1}^k w_i}$$</p>
<ul>
<li>Note the denominator normalizes the contribution of individual weights</li>
<li>Now we can consider using all the training examples.<ul>
<li>Using all examples (i.e. when $k = m$) with the rule above is called Shepard&#39;s method</li>
</ul>
</li>
</ul>
<p>Lazy learners like this do not construct a model, i.e.</p>
<ul>
<li>1-NN: training set error is always 0</li>
<li>$k$-NN: overfitting may be hard to detect</li>
</ul>
<p>The solution is to use Leave-one-out cross-validation (LOOCV), ie. leave out one example and predict it given the rest.</p>
<h3 id="curse-of-dimensionality">Curse of Dimensionality</h3>
<p>As dimension increases, the effectiveness of distance metrics decrease, and the concept of proximity may not be qualitatively meaningful as all points look equidistant.</p>
<p>Some other problems include:</p>
<ul>
<li>It becomes polynomially harder to estimate many parameters (e.g. covariances)</li>
<li>It becomes more difficult to visualize data</li>
<li>Enormous amount of data is needed to train a model</li>
<li>Number of &quot;cells&quot; / data points in the instance space grows exponentially in the number of features</li>
</ul>
<p>One approach to overcome the curse of dimensionality:</p>
<ul>
<li>Stretch $j^{th}$ axis by weight $z_j$, where $z_1, ..., z_d$ is chosen to minimize prediction error</li>
<li>Use cross-validation to automatically choose weights $z_1, ..., z_d$</li>
<li>Note setting $z_j$ to zero eliminates this dimension altogether</li>
</ul>
<h2 id="inductive-bias">Inductive Bias</h2>
<p>Inductive Bias is the combination of assumptions and restrictions placed on the models and algorithms used to solve a learning problem.
It means the algorithm and model combination you are using to solve the learning problem is appropriate for the task.</p>
<h2 id="bayesian-methods">Bayesian Methods</h2>
<p>Provides practical learning algorithms:</p>
<ul>
<li>Naive Bayes classifier learning</li>
<li>Bayesian networking learning, etc</li>
<li>Combines prior knowledge (prior probabilities) with observed data</li>
</ul>
<p>Provides useful conceptual framework</p>
<ul>
<li>Provides a &quot;gold standard&quot; for evaluating other learning algorithms</li>
</ul>
<p>Bayes theorem is stated as</p>
<p>$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$</p>
<p>where</p>
<ul>
<li>$P(h)$ = prior probability of hypothesis $h$</li>
<li>$P(D)$ = prior probability of training data $D$</li>
<li>$P(h|D)$ = probability of $h$ given $D$</li>
<li>$P(D|h)$ = probability of $D$ given $h$</li>
</ul>
<h3 id="choosing-hypotheses">Choosing Hypotheses</h3>
<p>If the output belongs to a set of $k$ classes: $y \in \{ C_1, C_2, ..., C_k \}$ for $1 \leq i \leq k$.</p>
<p>Then in Bayesian framework:</p>
<p>$$P(y = C_i | x) = \frac{P(x|C_i)P(C_i)}{P(x)}$$</p>
<p>where $P(x) = \sum_i P(x|C_i)(P(C_i)$</p>
<p>The decision rule is to select a class which maximises the posterior probability for the prediction.</p>
<p>Generally we want the most probable hypothesis given the training data, Maximum a posteriori hypothesis $h_{MAP}$:</p>
<p>$$
\begin{align*}
h_{MAP}
    &amp;= \arg \max_{h \in H} P(h|D) \\
    &amp;= \arg \max_{h \in H} \frac{P(D|h)P(h)}{P(D)} \\
    &amp;= \arg \max_{h \in H} P(D|h)P(h)
\end{align*}
$$</p>
<p>To get the posterior probability of a hypothesis $h$</p>
<p>Divide $P$ ($\oplus$) (probability of data) to normalize result for $h$:</p>
<p>$$P(h|D) = \frac{P(D|h) P(h)}{\sum_{h_i \in H}P(D|h_i)P(h_i)}$$</p>
<p>Denominator ensures we obtain posterior probabilities that sum to 1.
Sum for all possible numerator values, since hypotheses are mutually exclusive.</p>
<ul>
<li>Product Rule: probabiltiy $P(A \land B)$ of conjunction of two events $A$ and $B$:
$$P(A \land B) = P(A|B)P(B) = P(B|A)P(A)$$</li>
<li>Sum rule: probability of disjunction of two events $A$ and $B$:
$$P(A \lor B) = P(A) + P(B) - P(A \land B)$$</li>
</ul>
<p>Theorem of total probability: if events $A_1, ..., A_n$ are mutually exclusive with $\sum_{i=1}^n P(A_i) = 1$, then:</p>
<p>$$P(B) = \sum_{i=1}^n P(B|A_i)P(A_i)$$</p>
<h3 id="bayesian-expected-loss">Bayesian Expected Loss</h3>
<p>So far, we decide $h_1$ if $P(h_1|D) &gt; P(h_2|D)$ else $h_2$.
Alternatively, we can use a loss function $L(h)$, where $L(h)$ is the loss that occurs when decision $h$ is made.</p>
<p>For example, if the cost of misclassifying a patient who has cancer as &quot;not cancer&quot; is 10 times more than classifying a patient who doesn&#39;t have cancer as &quot;cancer&quot;, who will that affect our decision?</p>
<p>If the cost of misclassification is not the same for different classes, then instead of maximizing a posteriori, we have to minimize the expected loss:</p>
<ul>
<li>So, if we define the loss associated to action $\alpha_i$ as $\lambda(\alpha_i|h)$</li>
<li>Then the expected loss associated to action $\alpha_i$ si:
$$E[L(\alpha_i)] = R(\alpha_i | x) = \sum_{h \in H} \lambda (a_i|h) P(h|x)$$</li>
</ul>
<p>An optimal Bayesian decision strategy is to minimize the expected loss.</p>
<h2 id="learning-a-real-valued-function">Learning a real valued function</h2>
<p>Consider any real-valued target function $f$.
Training examples $\langle x_i, y_i \rangle$ where $y_i$ is noisy training value</p>
<ul>
<li>$y_i = \hat{f}(x_i) + \epsilon_i$</li>
<li>$\epsilon_i$ is random variable (noise) drawn independently fore ach $x_i$ according to some Gaussian (normal) distribution with mean zero</li>
</ul>
<p>Then the maximum likelihood hypothesis $H_{ML}$ is the one that minimizes the sum of squared errors</p>
<p>$$
\begin{align*}
h_{ML}
    &amp;= \arg \max_{h \in H} P(D|h) \\
    &amp;= \arg \max_{h \in H} \prod_{i=1}^m P(y_i|\hat{f}) \\
    &amp;= \arg \max_{h \in H} \prod_{i=1}^m \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2} \left(\frac{y_i - \hat{f}(x_i)}{\sigma} \right)^2 \right)
\end{align*}
$$</p>
<p>where $\hat{f} = h_{ML}$.</p>
<p>We can maximise the natural log to give a simpler expression:</p>
<p>$$
\begin{align*}
h_{ML}
    &amp;= \arg \max_{h \in H} \sum_{i=1}^m \ln \frac{1}{\sqrt{2 \pi \sigma^2}} - \frac{1}{2} \left( \frac{y_i - \hat{f}(x_i)}{\sigma} \right)^2 \\
    &amp;= \arg \max_{h \in H} \sum_{i=1}^m - \frac{1}{2} \left( \frac{y_i - \hat{f}(x_i)}{\sigma} \right)^2 \\
    &amp;= \arg \max_{h \in H} \sum_{i=1}^m - \left(y_i - \hat{f}(x_i)\right)^2 \\
\end{align*}
$$</p>
<p>Equivalently, we can minimise the positive version of the expression:</p>
<p>$$h_{ML} = \arg \min_{h \in H} \sum_{i=1}^m \left( y_i - \hat{f}(x_i) \right)^2$$</p>
<h2 id="discriminative-vs-generative-probabilistic-models">Discriminative vs Generative Probabilistic Models</h2>
<p><strong>Discriminative models</strong> model the posterior probability distribution $P(y|x)$.
That is, given $x$ they return a probability distribution over $y$.</p>
<p><strong>Generative models</strong> model the joint distribution $P(y,x)$.
Once we have the joint distribution, we can derive any conditional or marginal distribution involving the same variables.</p>
<p>Such models are called &#39;generative&#39; because we can sample from the joint distribution to obtain new data points together with their labels.</p>
<h2 id="bayesian-optimal-classifier">Bayesian Optimal Classifier</h2>
<p>In Bayesian optimal classification, the most probable classification
is obtained by combining the predictions of ALL hypotheses, weighted by their posterior probabilities:</p>
<p>$$\arg \max_{v_j \in V} \sum_{h_i \in H} P(v_j|h_i)P(h_i|D)$$</p>
<p>where $v_j$ is a class value.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Mathematically shown no other classification method using the same hypothesis space and prior knowledge outperforms the Bayes Optimal Classifier method on average.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><p>Bayes rule depends on unknown quantities so we need to use the data to find some approximation of those quantities</p>
</li>
<li><p>Is very inefficient.
An alternative is the <strong>Gibbs algorithm</strong>.</p>
<ol>
<li>Choose one hypothesis at random, according to $P(h|D)$</li>
<li>Use this to classify new instance</li>
</ol>
<p>Assuming target concepts are drawn at random from $H$
$$E[error_{Gibbs}] \leq 2 \times E[error_{BayesOptimal}]$$</p>
</li>
</ul>
<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<p>When to use</p>
<ul>
<li>Moderate or large training set available</li>
<li>Attributes that describe instances are conditionally independent given classification</li>
</ul>
<p>Successful applications:</p>
<ul>
<li>Classifying text documents</li>
<li>Gaussian Naive Bayes for real-valued data</li>
</ul>
<p>Assume target function $f : X \rightarrow V$, where each instance $x$ described by attributes $(x_1, x_2, ..., x_n)$.</p>
<p>The most probable value of $f(x)$ is:</p>
<p>$$v_{MAP} = \arg \max_{v_j \in V} P(x_1, x_2, ..., x_n|v_j)P(v_j)$$</p>
<p>Naive Bayes assumes attributes are statistically independent i.e.</p>
<p>$$P(x_1, x_2, ..., x_n|v_j) = \prod_i P(x_i|v_j)$$</p>
<p>Which gives <strong>Naive Bayes classifier</strong></p>
<p>$$v_{NB} = \arg \max_{v_j \in V} P(v_j) \prod_i P(x_i|v_j)$$</p>
<p>Psuedocode:</p>
<p>$\text{for each target value }v_j:$</p>
<ul>
<li>$\hat{P}(v_j) \leftarrow \text{estimate}P(v_j)$</li>
<li>$\text{for each attribute value } x_i:$<ul>
<li>$\hat{P}(x_i|v_j) \leftarrow$ $\text{ estimate }$ $P(x_i|v_j)$</li>
</ul>
</li>
</ul>
<p>If none of the training instances with target value $v_j$ have attribute $x_i$, then:</p>
<p>$$
\hat{P}(x_i|v_j) = 0 \\
\therefore \hat{P}(v_j) \prod_i \hat{P}(x_i|v_j) = 0
$$</p>
<p>Pseudo-counts add 1 to each count (a version of the Laplace Estimator).</p>
<h3 id="naive-bayes-numeric-attributes">Naive Bayes: numeric attributes</h3>
<ul>
<li><p>Usual assumption: attributes have a normal or Gaussian probability distribution (given the class)
$$x|v_j \sim N(\mu,\sigma^2)$$</p>
</li>
<li><p>the probability density function for the normal distribution is defined by two parameters</p>
<ul>
<li>The sample mean $\mu$:
$$\mu = \frac{1}{n}\sum_{i=1}^n x_i$$</li>
<li>The standard deviation $\sigma$:
$$\sigma = \frac{1}{n-1} \sum_{i=1}^n(x_i - \mu)^2$$</li>
</ul>
</li>
</ul>
<p>This gives the density function $f(x)$:</p>
<p>$$f(x) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left( \frac{(x - \mu)^2}{2 \sigma^2} \right)$$</p>
<h2 id="categorical-random-variables">Categorical Random Variables</h2>
<p>Categorical variables appear often in ML, i.e. text classification</p>
<ul>
<li>The most common form is the Bernoulli distribution model; whether or not a word occurs in a document.<ul>
<li>For the $i^{th}$ word, we have a random variable $X_i$ governed by a Bernoulli distribution.</li>
<li>The joint distribution over the bit vector $X = (X_1, ..., X_k)$ is called a <strong>multivariate Bernoulli distribution</strong> which shows whether a word occurs or not</li>
</ul>
</li>
<li>Variables with more than two outcomes are also common.<ul>
<li>The <strong>multinomial distribution</strong> manifests itself as a count vector: a histogram of the number of occurrences of all vocabulary words</li>
</ul>
</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Easy and fast at prediction. Also does well in multi class prediction.</li>
<li>When assumption of independence holds, performs better than other models like logistic regression with less training data.</li>
<li>Performs well in case of categorical input variables compared to numerical.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>If zero-frequency case happens, it is unable to make a prediction (solution: use the smoothing technique).</li>
<li>On the other side Naive Bayes is known as a bad estimator, so the probability outputs cannot be taken too seriously.</li>
<li>Strong assumption of independent attributes. In real life, it is almost impossible we get a set of attributes which are completely independent.</li>
</ul>
<h2 id="logistic-regression">Logistic Regression</h2>
<p>Logistic regression can be used for binary classification, where we transform the $y$ values into probability values (in the range $[0,1]$).</p>
<p>We can model this with a sigmoid curve.</p>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-1dd589ccab73f60638a2ce0e9a3a1db6.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<p>Now $f(x)$ can have a value inbetween $-\infty$ and $+\infty$ and in Logistic Regression we estimate $f(x)$ with a line.</p>
<p>$$\hat{f}(x) = x^T\beta \Rightarrow \log \frac{P(y=1|x)}{1-P(y=1|x)}$$</p>
<p>Logistic regression seeks to</p>
<ul>
<li>Model the probability of a class given the values of independent input variables</li>
<li>Estimate the probability that a class occurs for a random observation</li>
<li>Classify an observation based on the probability estimations</li>
</ul>
<p>$$\hat{P}(y=1|x) = \frac{1}{1+e^{-x^T\beta}}$$</p>
<ul>
<li><p>If $P(y=1|x) \geq 0.5$ (same as saying $x^T\beta \geq 0$) then predict as class 1</p>
</li>
<li><p>If $P(y=1|x) &lt; 0.5$ (same as saying $x^T\beta &lt; 0$) then predict as class 0</p>
</li>
<li><p>This is equivalent of having a linear decision boundary separating the two classes (because we have a linear solution to our problem, this is what makes Logistic Regression a linear model)</p>
</li>
</ul>
<h3 id="logistic-regression-parameter-estimation">Logistic Regression Parameter Estimation</h3>
<p>Cannot use cost function in Linear Regression because it will result in a non-convex function with many local minimums.
Instead, the following cost function is used:</p>
<p>Let&#39;s define $\hat{P}(y=1|x) = h_\beta(x)$</p>
<p>$$
cost \left( h_\beta(x),y \right) =
\begin{cases}
  -\log \left( h_\beta(x) \right) &amp; \text{if $y = 1$} \\
  -\log \left( 1-h_\beta(x) \right) &amp; \text{if $y = 0$}
\end{cases}
$$</p>
<p>$$J(\beta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log \left( h_\beta \left( x^{(i)} \right) \right) + \left( 1 - y^{(i)} \right) \log \left( 1 - h_\beta \left( x^{(i)} \right) \right) \right]$$</p>
<p>The values of the parameters that minimise $J(\beta)$ can be found using gradient descent.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Relatively easy to implement and interpret and relatively fast at training and testing</li>
<li>Can easily extend to multi-classes</li>
<li>Provides probabilistic predictions</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Prone to overfitting in high-dimensional data (one remedy: regularization)</li>
<li>Provides linear decision boundary</li>
<li>Requires moderate or no correlation (collinearity) between input variables and may lead to poor model, and also sensitive to outliers</li>
</ul>
<h1 id="tree-learning"><a href="#tree-learning">Tree Learning</a></h1>

<p>Probably the single most popular tool</p>
<ul>
<li>Easy to understand, implement and use</li>
<li>Computationally cheap (efficient, even on big data)</li>
</ul>
<p>Decision tree representation:</p>
<ul>
<li>Each internal node tests an attribute</li>
<li>Each branch corresponds to attribute value</li>
<li>Each leaf node assigns a classification</li>
</ul>
<h2 id="decision-tree-expressiveness">Decision Tree: Expressiveness</h2>
<p>Decision Trees can be used to represent Boolean functions like AND ($\land$), OR ($\lor$), XOR ($\oplus$)</p>
<p>$$X \land Y$$</p>
<pre><code class="language-py">if X == True:
    if Y == True: return True
    if Y == False: return False
if X == False:
    return False
</code></pre>
<p>$$X \lor Y$$</p>
<pre><code class="language-py">if X == True: return True
if X == False:
    if Y == True: return True
    if Y == False: return True
</code></pre>
<p>$$X \oplus Y$$</p>
<pre><code class="language-py">if X == True:
    if Y == True: return False
    if Y == False: return True
if X == False:
    if Y == True: return True
    if Y == False: return False
</code></pre>
<p>In general, decisions trees represent a <strong>disjunction of conjunctions</strong></p>
<p>When to use decision trees:</p>
<ul>
<li>Instance described by a mix of numeric features and discrete attribute value pairs</li>
<li>Target function is discrete valued (otherwise use regression trees)</li>
<li>Possibly noisy training data</li>
<li>Interpretability is an advantage</li>
</ul>
<p>The main loop for top-down induction of decision trees (TDIDT)</p>
<ol>
<li>$A \leftarrow$ the &quot;best&quot; decision attribute for the next node to split examples</li>
<li>Assign $A$ as decision attribute for node</li>
<li>For each value of $A$, create new descendant of node (child node)</li>
<li>Split training examples to child nodes</li>
<li>If training examples perfectly classified (pure subset), then STOP ,else iterate over new child nodes</li>
</ol>
<h2 id="entropy">Entropy</h2>
<p>If we want to determine yes or no, is <code>outlook</code> or <code>wind</code> a better attribute</p>
<pre><code class="language-py">def use_outlook(outlook: attribute) -&gt; dict:
    match outlook:
        case sunny: return {&quot;yes&quot;: 2, &quot;no&quot;: 3}
        case overcast: return {&quot;yes&quot;: 4, &quot;no&quot;: 0}
        case rain: return {&quot;yes&quot;: 3, &quot;no&quot;: 2}

def use_wind(wind: attribute) -&gt; dict:
    match wind:
        weak: return {&quot;yes&quot;: 6, &quot;no&quot;: 2}
        strong: return {&quot;yes&quot;: 3, &quot;no&quot;: 3}
</code></pre>
<ul>
<li>We are using a split with higher &quot;purity&quot;</li>
<li>We need to measure the purity of the split<ul>
<li>More certain about our classes after a split<ul>
<li>A set with all examples belonging to one class is 100% pure</li>
<li>A set with 50% examples in one class and 50% in the other is 100% uncertain and impure</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Entropy</strong></p>
<ul>
<li>From a statistical point of view</li>
<li>From information theory point of view: The amount of information (in the Shannon sense) needed to specify the full state of a system</li>
</ul>
<p>Entropy measures the &quot;impurity&quot; of $S$.</p>
<p>$$Entropy(S) = H(S) = -p_\oplus \log_2 p_\oplus - p_\ominus \log_2 p_\ominus$$</p>
<ul>
<li>$S$ is subset of training examples</li>
<li>$p_\oplus$, $p_\ominus$ are the portion (%) of positive and negative examples in $S$</li>
</ul>
<p>Interpretation: if item $x$ belongs to $S$, how many bits are needed to tell if $x$ is positive or negative?</p>
<div class="tikz-diagram">
  <img src="/tikz-cache/tikz-d9df73e9a719e07e339e88aaf2a059ba.svg" alt="TikZ Diagram" class="tikz-svg" />
</div>

<ul>
<li>&quot;High Entropy&quot; / &quot;impure set&quot; means $X$ is very uniform and boring<ul>
<li>E.g. (3 samples from $\oplus$, 3 samples from $\ominus$)</li>
<li>$H(S) = - \frac{3}{6}\log_2\frac{3}{6} - \frac{3}{6}\log_2\frac{3}{6} = 1$ (can be interpreted as 1 bits)</li>
</ul>
</li>
<li>&quot;Low Entropy&quot; / &quot;pure set&quot; means $X$ is not uniform and interesting<ul>
<li>E.g. (6 samples from $\oplus$, 0 samples from $\ominus$)</li>
<li>$H(S) = - \frac{6}{6}\log_2\frac{6}{6} - \frac{0}{6}\log_2 \frac{0}{6} = 0$ (can be interpreted as 0 bits)</li>
</ul>
</li>
</ul>
<h2 id="information-gain">Information Gain</h2>
<p>$Gain(S,A)$ is the expected reduction in entropy due to sorting on $A$</p>
<p>$$Gain(S,A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|}Entropy(S_v)$$</p>
<ul>
<li>$v$ is the possible values of attribute $A$</li>
<li>$S$ is the set of examples we want to split</li>
<li>$S_v$ is the subset of examples where $X_A = v$</li>
</ul>
<p>We want to find the attribution which maximizes the gain (this is also called &quot;mutual information&quot; between attribute $A$ and class labels of $S$).</p>
<p>To select the best attribute at each branch:</p>
<ul>
<li>take every/remaining attributes in your data</li>
<li>Compute information gain for that attribute</li>
<li>Select the attribute that has the highest information gain</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Information gain is more biased towards attributes with large numbers of values / categories<ul>
<li>Subsets are more likely to be pure if there is a large number of values</li>
<li>This can result in overfitting which doe snot generalize well to unseen data</li>
</ul>
</li>
<li>Suggested solution: gain ratio<ul>
<li>A modification of information gain that reduces the bias</li>
<li>Takes number and size of branches into account</li>
</ul>
</li>
</ul>
<p><strong>Gain Ratio</strong></p>
<p>$$SplitEntropy(S,A) = - \sum_{v \in Values(A)} \frac{S_v}{S} \log_2 \frac{S_v}{S}$$
Where:</p>
<ul>
<li>$A$: candidate attribute</li>
<li>$v$: possible values of $A$</li>
<li>$S$: Set of examples ($X$) at the node</li>
<li>$S_v$: subset where $X_A = v$</li>
</ul>
<p>$$GainRatio(S,A) = \frac{Gain(S,A)}{SplitEntropy(S,A)}$$</p>
<h2 id="overfitting-in-decision-trees">Overfitting in Decision Trees</h2>
<ul>
<li>Can always classify training examples perfectly<ul>
<li>If necessary, keep splitting until each node contains 1 example</li>
<li>Singleton subset (leaf nodes with one example) which is by definition pure</li>
</ul>
</li>
<li>But this is not always ideal because on leaf nodes with singleton subsets, you have no confidence in your decision</li>
</ul>
<p>Consider error of hypothesis $h$ over</p>
<ul>
<li>Training data: $error_{train}(h)$</li>
<li>Entire distribution $D$ of data $error_{D}(h)$</li>
</ul>
<p><strong>Definition</strong>
Hypothesis $h \in H$ overfits training data if there is an alternative hypothesis $h&#39; \in H$ such that</p>
<p>$$error_{train}(h) &lt; error_{train}(h&#39;) \land error_D(h) &gt; error_D(h&#39;)$$</p>
<p>To avoid overfitting in decision trees, we can use <strong>pruning</strong></p>
<ul>
<li><strong>pre-pruning:</strong> Stop growing when data split not statistically significant.<ul>
<li>Some stopping conditions:<ul>
<li>Lower than some lower-bound on the number of examples in a leaf</li>
<li>Stop when Entropy changes is smaller than a lower-bound</li>
</ul>
</li>
</ul>
</li>
<li><strong>post-pruning:</strong> Grow full tree, then remove sub-trees which are overfitting (based on validation set). This avoids the problem of &quot;early stopping&quot;<ul>
<li>Methods of finding subtrees:<ul>
<li>Split data into training and validation set, and prune subtrees until further pruning is harmful</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Easy to interpret</li>
<li>Can handle irrelevant attributes (Gain = 0)</li>
<li>Can handle categorical and numerical data, along with missing data</li>
<li>Can handle missing data</li>
<li>Very compact (number of nodes &lt;&lt; number of examples)</li>
<li>Very fast at testing</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Only axis-aligned splits of the data</li>
<li>Tend to overfit</li>
<li>Greedy (may not find the best tree)<ul>
<li>Exponentially many possible trees</li>
</ul>
</li>
</ul>
<h2 id="regression-tree">Regression Tree</h2>
<p>Mention things here</p>
<h1 id="kernel-methods"><a href="#kernel-methods">Kernel Methods</a></h1>

<table>
<thead>
<tr>
<th>Task</th>
<th>Label and Output Space</th>
<th>Learning Problem</th>
</tr>
</thead>
<tbody><tr>
<td>Classification</td>
<td>$\mathcal{L} = \mathcal{C}$, $\mathcal{Y} = \mathcal{C}$</td>
<td>Learn an approximation $\hat{c} : \mathcal{X} \rightarrow \mathcal{C}$ to the true labelling function $c$</td>
</tr>
<tr>
<td>Scoring and ranking</td>
<td>$\mathcal{L} = \mathcal{C}$, $\mathcal{Y} = \mathbb{R}^{|\mathcal{C}|}$</td>
<td>Learn a model that outputs a score vector over classes</td>
</tr>
<tr>
<td>Probability estimation</td>
<td>$\mathcal{L} = \mathcal{C}$, $\mathcal{Y} = [0,1]^{|\mathcal{C}|}$</td>
<td>Learn a model that outputs a probability vector over classes</td>
</tr>
<tr>
<td>Regression</td>
<td>$\mathcal{L} = \mathbb{R}$, $\mathcal{Y} = \mathbb{R}$</td>
<td>Learn an approximation $\hat{f} : \mathcal{X} \rightarrow \mathbb{R}$ to the true labelling function $f$</td>
</tr>
</tbody></table>
<h2 id="perceptron">Perceptron</h2>
<p><strong>Perceptron</strong> is an algorithm for binary classification that uses a linear prediction function.
Note the perceptron predicts a binary class label, but linear regression predicts a real value.</p>
<p>For a general case with $n$ attributes</p>
<p>$$
f(x) =
\begin{cases}
    +1 &amp; \text{if $w_0 + w_1x_1 + ... + w_nx_n &gt; 0$} \\
    -1 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>If we add $x_0 = 1$ to the feature vector:</p>
<p>$$
f(x) =
\begin{cases}
    +1 &amp; \text{if $\sum_{i=0}^n w_ix_i &gt; 0$} \\
    -1 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>$$\hat{y} = f(x) = sign(w \cdot x)$$</p>
<p>where $sign$ is the sign function.
Now to find a good set of weights using our training set.</p>
<p>The perceptron algorithm initializes all weights $w_i$ to zero, and learns the weights using the following update rule:</p>
<p>$$w := w + \frac{1}{2} \left( y_j - f(x_j) \right)x_j$$</p>
<p>There are 4 cases:</p>
<ol>
<li>$y=+1, f(x)=+1 \Rightarrow (y-f(x))=0$</li>
<li>$y=+1, f(x)=-1 \Rightarrow (y-f(x))=+2$</li>
<li>$y=-1, f(x)=+1 \Rightarrow (y-f(x))=-2$</li>
<li>$y=-1, f(x)=-1 \Rightarrow (y-f(x))=0$</li>
</ol>
<h3 id="perceptron-training-algorithm">Perceptron training algorithm</h3>
<p><strong>Algorithm</strong> Perceptron($D$) / perceptron training for linear classification</p>
<p><strong>Input</strong> labelled training data $D$ in homogeneous coordinates</p>
<p><strong>Output</strong> weight vector $w$</p>
<ul>
<li>$w \leftarrow 0$</li>
<li>$converged \leftarrow false$</li>
<li>$\textbf{while } coverged = false \textbf{ do}$<ul>
<li>$converged \leftarrow true$</li>
<li>$\textbf{for } i = 1 ..|D| \textbf{ do}$<ul>
<li>$\textbf{if } y_iw \cdot x_i \leq 0 \textbf{ do}$<ul>
<li>$w \leftarrow w + y_ix_i$</li>
<li>$converged \leftarrow false$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
<h3 id="extending-linear-classification">Extending Linear Classification</h3>
<p>Linear classification cannot model nonlinear class boundaries.
However we can map attributes into new space consisting combination of attribute values.</p>
<p>E.g. for 2 attributes</p>
<p>$$y = w_1x_1^3 + w_2x_1^2x_2 + w_3x_1x_2^2 + w_4x_2^3$$</p>
<p>$y$ is predicted output for instances with two attributes $x_1$ and $x_2$.</p>
<p>However, there are issues with this approach</p>
<ul>
<li><strong>Efficiency:</strong><ul>
<li>With 10 attributes and polynomial function with order 5, more than 2000 coefficients have to be learned</li>
</ul>
</li>
<li><strong>Overfitting:</strong><ul>
<li>&quot;Too nonlinear&quot; - number of coefficients large relative to number of training instances</li>
<li>Curse of dimensionality</li>
</ul>
</li>
</ul>
<h2 id="dual-form">Dual Form</h2>
<p>With an optimisation problem, we can construct another optimisation problem which is called <strong>dual problem</strong> and is related to our original problem (<strong>primal problem</strong>).</p>
<p>After training a perceptron, each example has been misclassified zero or more times.
Denoting this number as $\alpha_i$ for example $x_i$, the weight vector for $m$ observations can be expressed as</p>
<p>$$w = \sum_{i=1}^m \alpha_iy_ix_i$$</p>
<p>In the dual instance-based view, we are learning instance weights $\alpha_i$ rather than feature weights $w_j$.
An instance $x$ is classified as</p>
<p>$$\hat{y} = f(x) = sign(w \cdot x)$$
$$\hat{y} = sign \left( \sum_{i=1}^m a_iy_i(x_i \cdot x) \right)$$</p>
<h3 id="perceptron-training-algorithm-in-dual-form">Perceptron training algorithm in dual form</h3>
<p><strong>Algorithm</strong> Dual-Perceptron($D$) / perceptron training for linear classification in dual form</p>
<p><strong>Input</strong> labelled training data $D$ in homogeneous coordinates</p>
<p><strong>Output</strong> coefficients $\alpha_i$ defining weight vector $W = \sum_{i=1}^{|D|} \alpha_i y_i x_i$</p>
<ul>
<li>$\alpha_i \leftarrow 0$</li>
<li>$converged \leftarrow false$</li>
<li>$\textbf{while } coverged = false \textbf{ do}$<ul>
<li>$converged \leftarrow true$</li>
<li>$\textbf{for } i = 1 ..|D| \textbf{ do}$<ul>
<li>$\textbf{if } y_i \sum_{j=1}^{|D|} a_j y_j x_j \cdot x_i\leq 0 \textbf{ do}$<ul>
<li>$\alpha_i \leftarrow \alpha_i + 1$</li>
<li>$converged \leftarrow false$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
</li>
<li>$\textbf{end}$</li>
</ul>
<h2 id="nonlinear-dual-perceptron">Nonlinear dual perceptron</h2>
<ul>
<li>We can use nonlinear mapping to map attributes into new space consisting of combinations of attribute values.
$$x \rightarrow \varphi(x)$$</li>
<li>The perceptron decision will be:
$$\hat{y} = sign \left( \sum_{i=1}^m a_j y_i (\varphi(x_i) \cdot \varphi(x))\right)$$</li>
<li>So the only thing we need is the <strong>dot product in the new feature space</strong> $(\varphi(x_i) \cdot \varphi(x))$ or $\langle \varphi(x_i), \varphi(x) \rangle$</li>
</ul>
<p>Let $x = (x_1, x_2)$ and $x&#39; = (x_1&#39;, x_2&#39;)$ be two data points, and consider the following mapping to a three-dimensional feature space:</p>
<p>$$(x_1, x_2) \rightarrow (x_1^2, x_2^2, \sqrt{2}x_1x_2)$$</p>
<p>$$\text{(original feature space) } \mathcal{X} \rightarrow \mathcal{Z} \text{ (new feature space)}$$</p>
<p>The points in feature space corresponding to $x$ and $x&#39;$ are</p>
<p>$z = (x_1^2, x_2^2, \sqrt{2}x_1x_2)$ and $z&#39; = (x_1&#39;^2, x_2&#39;^2, \sqrt{2}x_1&#39;x_2&#39;)$</p>
<p>The dot product of these two feature vectors is</p>
<p>$$z \cdot z&#39; = x_1^2x_1&#39;^2 + x_2^2x_2&#39;^2 + 2x_1x_1&#39;x_2x_2&#39; = (x_1x_1&#39; + x_2x_2&#39;)^2 = (x \cdot x&#39;)^2$$</p>
<p>By squaring in original space, we obtain the dot product in the new space.
A function that directly calculates the dot product in the new feature space from vectors in the original space is called a kernel - here the kernel is $K(x_1,x_2) = (x_1 \cdot x_2)^2$.</p>
<p>A <strong>valid kernel</strong> function is equivalent to a <strong>dot product in some space</strong>.</p>
<p>$$K(x,x&#39;) = \varphi(x) \cdot \varphi(x&#39;)$$</p>
<ul>
<li>A kernel function is a <strong>similarity</strong> function that corresponds to a dot product in some expanded feature space.</li>
<li>Some very useful kernels in machine learning are <strong>polynomial kernel</strong> and <strong>radial basis function kernel (RBF kernel)</strong></li>
<li>Polynomial kernel is defined as:
$$K(x,x&#39;) = (x \cdot x&#39; + c)^q$$</li>
<li>RBF kernel is defined as:
$$K(x,x&#39;) = \exp \left( - \frac{||x-x&#39;||^2}{2 \sigma^2} \right)$$
(Using Taylor expansion, it can be shown that RBF kernel is equivalent of mapping features into infinite dimensions)</li>
</ul>
<p>Using the kernel trick, the nonlinear perceptron can be solved using the dual form</p>
<p>$$\hat{y} = sign\left( \sum_{i=1}^m a_i y_i (\varphi(x_i) \cdot \varphi(x)) \right) = sign\left( \sum_{i=1}^m a_i y_i K(x_i, x) \right)$$</p>
<h2 id="support-vector-machine">Support Vector Machine</h2>
<ul>
<li>Support Vector Machines (SVMs) can find the optimal linear classification by fitting the maximum margin hyperplane that has the greatest separation between classes</li>
<li>Can avoid overfitting - learn a form of decision boundary called the maximum margin hyperplane</li>
<li>Fast for mappings to nonlinear spaces<ul>
<li>employ a mathematical trick (kernel) to avoid the actual creation of new &quot;pseudo-attributes&quot; in transformed instance spaces</li>
<li>i.e. the nonlinear space is created implicitly</li>
</ul>
</li>
</ul>
<p>Let $x_s$ be the closest point to the separating hyperplane (line in 2D) with the following equation:</p>
<p>$$w \cdot x = t$$</p>
<p>Let&#39;s have 2 minor technicalities to simply the math later:</p>
<ol>
<li><p>Pull out $w_0 : w = [w_0, ..., w_n]$ and $w_0 = -t$, therefore we will have:
$$w \cdot x - t = 0$$</p>
</li>
<li><p>Normalize $w$:</p>
<p>We know that $|w \cdot x_s - t| &gt; 0$ and we know that we can sacle $w$ and $t$ together without having any effect on the hyperplane, so we choose the scale such that:
$$|w \cdot x_s - t| = 1$$</p>
<p>This means $m = 1$.</p>
</li>
</ol>
<p>$w$ is perpendicular to the line (hyperplane).</p>
<p>For every two points $x&#39;$ and $x&#39;&#39;$ on the line (hyperplane), we can write:</p>
<p>$$
w \cdot x&#39; - t = 0 \text{ and } w \cdot x&#39;&#39; - t = 0 \\
\Rightarrow \quad w \cdot (x&#39; - x&#39;&#39;) = 0
$$</p>
<p>Since their dot product is 0, it means they are perpendicular.</p>
<p>Since, distance between the point $x_s$ and the hyperplane can be found as</p>
<p>$$distance = \frac{|w \cdot x_s - t|}{||w||}$$</p>
<p>And we have $|w \cdot x_s - t|=1$, so:</p>
<p>$$distance = \frac{1}{||w||}$$</p>
<p>This distance is the <strong>margin</strong> of our classifier which we want to minimize:</p>
<p>$$\max \frac{1}{||w||} \text{ subject to } \min_{t=1,...,m} |w \cdot x_i - t| = 1$$</p>
<p>(This is not a friendly optimisation as it has &quot;min&quot; in the constraint).</p>
<p>We can transform the maximization problem into the following minimization problem:</p>
<p>$$
\min_w \frac{1}{2} ||w||^2 \text{ subject to } y_i(w \cdot x_i - t) \geq 1 \\
\text{ for } i = 1,...,m, w \in \mathbb{R}^n, t \in \mathbb{R}
$$</p>
<p>This can be solved using Lagrangian multipliers</p>
<h2 id="lagrangian-multipliers">Lagrangian multipliers</h2>
<p>In Lagrangian form, the optimization problem becomes:</p>
<p>$$\max_{\alpha_1,...,\alpha_m} \min_{x_1,...,x_n} \mathcal{L}(x_1,...,x_n,\alpha_1,...,\alpha_m) \text{ s.t. } \alpha_j \geq 0 \forall j$$</p>
<ul>
<li>first minimizing with respect to $x_1,...,x_n$</li>
<li>then maximizing with respect to $\alpha_1,...,\alpha_m$</li>
</ul>
<p>Adding the constraints with multiplier $\alpha_i$ for each training example gives the Lagrange function:</p>
<p>$$
\begin{align*}
\mathcal{w,t,\alpha_1,...,\alpha_m}
  &amp;= \frac{1}{2} ||w||^2 - \sum_{i=1}^m \alpha_i (y_i (w \cdot x_i - t) - 1) \\
  &amp;= \frac{1}{2} ||w||^2 - \sum_{i=1}^m \alpha_iy_i(w \cdot x_i) + \sum_{i=1}^m \alpha_i y_i t + \sum_{i=1}^m \alpha_i \\
  &amp;= \frac{1}{2} w \cdot w - w \cdot \left( \sum_{i=1}^m \alpha_iy_ix_i \right) + t \left( \sum_{i=1}^m \alpha_i y_i \right) + \sum_{i=1}^m \alpha_i
\end{align*}
$$</p>
<ul>
<li>First we have to minimize $\mathcal{L}$ with respect to $w$ and $t$</li>
<li>By taking the partial derivative of the Lagrange function with respect to $t$ and setting it to 0, we find:
$$\sum_{i=1}^m\alpha_iy_i = 0$$</li>
<li>Similarly, by taking the partial derivative of the Lagrange function with respect to $w$ and setting to 0 we obtain:
$$w = \sum_{i=1}^m \alpha_i y_i x_i$$<ul>
<li>The same expression as we derived for the perceptron</li>
</ul>
</li>
<li>These expressions allows us to eliminate $w$ and $t$ and lead to the dual Lagrangian
$$
\begin{align*}
\mathcal{L}(\alpha_1, ..., \alpha_n)
  &amp;= - \frac{1}{2} \left( \sum_{i=1}^m \alpha_i y_i x_i \right) \cdot \left( \sum_{i=1}^m \alpha_i y_i x_i \right) + \sum_{i=1}^m \alpha_i \\
  &amp;= - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i \cdot x_j + \sum_{i=1}^m \alpha_i
\end{align*}
$$</li>
</ul>
<p>The dual optimization problem for SVMs is to maximize the dual Lagrangian under positive constraints and one equality constraint:</p>
<p>$$\alpha_i*, ..., \alpha_m* = \arg \max_{\alpha_1, ..., \alpha_m} - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i \cdot x_j + \sum_{i=1}^m \alpha_i $$</p>
<p>$$\text{subject to } \alpha_i \geq 0, 1 \leq i \leq m, \sum_{i=1}^m \alpha_i y_i = 0$$</p>
<p>Solving for $\alpha_1*, ..., \alpha_m*$, they will be mostly zero, except for points that are closest to the hyperplane.
These points are called the support vectors.</p>
<p>$$w = \sum_{X_i \in \{ support : vectors\}} \alpha_i y_i x_i$$</p>
<p>Solve for $t$ using any of support vectors:</p>
<p>$$y_i (w \cdot x_i - t) = 1$$</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Effective in high dimensional space and when dimension &gt; num samples</li>
<li>Memory efficient</li>
<li>Works very well if classes are separable even if non linearly (through kernels)</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Doesn&#39;t scale well to big data (by increasing the number of samples, the number of support vector grows and the prediction will be slow)</li>
<li>Doesn&#39;t directly provide probabilistic explanation</li>
<li>Hard to interpret, especially when using kernels</li>
</ul>
<h1 id="ensemble-learning"><a href="#ensemble-learning">Ensemble Learning</a></h1>

<p>Ensemble learning is a form of multi-level learning: learning a number of base-level models from the data, and learning to combine these models as an ensemble.</p>
<p>It can help to reduce <strong>bias</strong> (expected error due to mismatch between learner&#39;s hypothesis space and space of target concepts) and <strong>variance</strong> (expected error due to differences in the training sets used).</p>
<p>Three commonly used methods to find a good bias-variance tradeoff are:</p>
<ul>
<li>Regularization</li>
<li>Bagging</li>
<li>Boosting</li>
</ul>
<h2 id="bias-variance-with-big-data">Bias-variance with &quot;Big Data&quot;</h2>
<ul>
<li>High bias algorithms are often used for efficiency<ul>
<li>Usually simpler to compute</li>
</ul>
</li>
<li>Big data can reduce variance<ul>
<li>&quot;small&quot; concepts will occur more frequently</li>
<li>low bias algorithms can be applied</li>
</ul>
</li>
</ul>
<p>However, it is difficult to compute big data efficiently.</p>
<p>The follow scenarios happen in real-world AI</p>
<ol>
<li>Training-set error is observed to be high compared to human-level<ul>
<li>Bias is too high - solution: move to a more expressive (lower bias) model</li>
</ul>
</li>
<li>Training-set error is observed to be similar to human-level, but validation set error is high compared to human-level<ul>
<li>Variance is too high - solution: get more data, try regularization, ensembles, move to a different model architecture</li>
</ul>
</li>
</ol>
<h2 id="stability">Stability</h2>
<p>For some given data $\mathcal{D}$, train an algorithm $L$ on training sets $S_1$, $S_2$ sampled from $\mathcal{D}$.
If the model from $L$ is very similar on both $S_1$ and $S_2$, $L$ is a stable learning algorithm, otherwise it is unstable.</p>
<table>
<thead>
<tr>
<th></th>
<th>typical stable algorithm</th>
<th>typical unstable algorithm</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Example model</strong></td>
<td>$k$NN (for some $k$)</td>
<td>decision-tree learning</td>
</tr>
<tr>
<td><strong>Cons</strong></td>
<td>high bias</td>
<td>high variance</td>
</tr>
</tbody></table>
<p>Note parameters have effect on stability, i.e. in $k$NN:</p>
<ul>
<li>1NN perfectly separates training data, so low bias but high variance</li>
<li>By increasing $k$, we increase bias and decrease variance</li>
</ul>
<h2 id="supervised-learning">Supervised Learning</h2>
<p>Ensemble methods are meta-algorithms that <strong>combine different models</strong> into one model and they can:</p>
<ul>
<li>Decrease variance</li>
<li>Decrease bias</li>
<li>Improve performance</li>
</ul>
<p>This idea relates to the &quot;wisdom of crowd&quot; phenomenon.</p>
<p>There are different ways of combining predictors</p>
<ol>
<li><p>Simple ensembles like majority vote or unweighted average</p>
</li>
<li><p>Weighted averages / weighted votes: Every model gets a weight (i.e. depending on its performance)</p>
<p>In practice:</p>
<ul>
<li><p>Learning algorithms may not be independent</p>
</li>
<li><p>Some better fit the data so make less error</p>
<p>We can define weights in different ways:</p>
</li>
<li><p>Decrease weight of correlated learners</p>
</li>
<li><p>Increase weight of good learners</p>
</li>
</ul>
</li>
<li><p>Treat the output of each model as a feature and train a model on that</p>
<ul>
<li>If the task is binary classification and we choose the fusion model to be a linear model, then this will become a weighted vote</li>
<li>We train the fusion model on unseen data otherwise it will be biased towards the models that performed better on the training data</li>
</ul>
</li>
<li><p>Mixture of experts</p>
<ul>
<li>Weight $\alpha_i(x)$ indicates &quot;expertise&quot;</li>
<li>It divides the feature space into homogeneous regions</li>
<li>It may use a weighted average or just pick the model with largest expertise</li>
<li>It is a kind of local learning</li>
</ul>
</li>
<li><p>&quot;Bagging&quot; method: (&quot;<strong>B</strong>ootstrap <strong>Agg</strong>regation&quot;)</p>
<ul>
<li>Training many classifiers, but each with only a portion of the data</li>
<li>Then aggregate through model averaging / majority voting</li>
</ul>
</li>
</ol>
<h2 id="bagging">Bagging</h2>
<p><strong>Boostrap:</strong></p>
<ul>
<li>Create a random subset of data by sampling with replacement</li>
<li>Draw $m&#39;$ samples from $m$ sample with replacement $(m&#39; \leq m)$</li>
</ul>
<p><strong>Bagging:</strong></p>
<ul>
<li>Repeat $k$ times to generate $k$ subsets<ul>
<li>Some of the samples get repeated and some will not be left out</li>
</ul>
</li>
<li>Train one classifier on each subset</li>
<li>To test, aggregate the output of $k$ classifiers that you trained in the previous step using either majority vote / unweighted average</li>
</ul>
<p>Error of any model has two components:</p>
<ul>
<li><strong>Bias:</strong> due to model choice which<ul>
<li>can be reduced by increasing complexity</li>
</ul>
</li>
<li><strong>Variance:</strong> due to small sample size or high complexity of the model<ul>
<li>Can be reduced by increasing the data or reducing the complexity</li>
</ul>
</li>
</ul>
<p>Bagging is applied on a collection of low-bias high-variance models and by averaging them, the bias is unaffected, but the variance reduces.</p>
<p>To calculate <strong>bagging error:</strong></p>
<ul>
<li>If learners are independent</li>
<li>If each learner makes an error with probability $p$</li>
</ul>
<p>The probability that $k&#39;$ out of $k$ learners make an error is:</p>
<p>$$\binom{k}{k&#39;}p^{k&#39;}(1-p)^{k-k&#39;}$$</p>
<p>If we use majority voting to decide the output, then the error happens if more than $\frac{k}{2}$ of learners make an error, so the error for majority voting is:</p>
<p>$$\sum_{k&#39; &gt; \frac{k}{2}} \binom{k}{k&#39;}p^{k&#39;}(1-p)^{k-k&#39;}$$</p>
<p><strong>Advantage of bagging:</strong></p>
<ul>
<li>Reduces overfitting (harder for aggregated model to memorize full dataset)</li>
<li>This improves performance in almost all cases esp if learning scheme is unstable</li>
<li>Can be applied to numeric prediction and classification</li>
<li>Can help a lot if data is noisy</li>
</ul>
<h2 id="random-forests">Random Forests</h2>
<p>Tree models can be bagged.</p>
<p><strong>Pros</strong></p>
<ul>
<li>Reduces the overfitting</li>
<li>Generalizes better</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>When we bag a model, any simple structure is lost<ul>
<li>This is because a bagged tree is no longer a tree, but a forest so this reduces claim to interpretability</li>
<li>stable models like nearest neighbor not very affected by bagging but unstable models like trees most affected by bagging</li>
</ul>
</li>
<li>With lots of data, we usually learn the same classifier, so averaging them does not help</li>
</ul>
<p>The solution for ensemble decision trees with lots of data is <strong>random forests</strong>.</p>
<ul>
<li>Have extra variability in the learners and introduce more randomness to the procedure.</li>
</ul>
<p>To produce a random forest</p>
<ol>
<li>Select a random subsample from the dataset with replacement</li>
<li>Select a subset of features randomly</li>
<li>Build a full tree without pruning using the selected features and samples</li>
<li>Repeat previous steps $k$ times</li>
</ol>
<h2 id="boosting">Boosting</h2>
<p>A problem with parallel learners is that they can all be mistaken in the same region.
Boosting tries to solve this:</p>
<ul>
<li>Uses &quot;weak&quot; learners which are trained sequentially</li>
<li>New learners focus on errors of earlier learners</li>
<li>New learners try to get these &quot;hard&quot; examples right by operating on a weighted train set in favor of misclassified instances<ul>
<li>Start with the same weight for all the instances</li>
<li>Misclassified instances gain higher weights: so the next classifier is more likely to classify it correctly<ul>
<li>Give different weights to the loss function for different instances</li>
<li>Create a new collection of data with multiple copies of samples with higher weight</li>
</ul>
</li>
<li>Correctly classified instances lose weight</li>
</ul>
</li>
<li>Combine all learners in the end</li>
</ul>
<ol>
<li>set $w_i = 1/m$ for $i = 1,...,m$</li>
<li>Repeat until sufficient number of hypothesis<ul>
<li>Train model $L_j$ using the dataset with weight $w$</li>
<li>Increase $w_i$ for misclassified instances of $L_j$</li>
</ul>
</li>
<li>Ensemble hypothesis is the weighted majority / weighted average of $k$ learners $L_1, ..., L_k$ with weight $\lambda_1, ..., \lambda_k$ which are proportional to the accuracy of $L_j$</li>
</ol>
<p>We always aim to minimize some cost function:</p>
<table>
<thead>
<tr>
<th>Unweighted average loss</th>
<th>Weighted average loss</th>
</tr>
</thead>
<tbody><tr>
<td>$J(\theta) = \frac{1}{N} \sum_i J_i (\theta, x_i)$</td>
<td>$J(\theta) = \sum_i w_iJ_i (\theta, x_i)$</td>
</tr>
</tbody></table>
<p>Boosting works well as long as we use <strong>weak learners</strong> (weak learner is a model that is slightly better than random), i.e.</p>
<ul>
<li>Perceptron</li>
<li>Decision stumps (trees with one node)</li>
</ul>
<h3 id="adaboost-adaptive-boosting">AdaBoost (Adaptive Boosting)</h3>
<ul>
<li>AdaBoost usually uses <strong>stump trees</strong> (trees with one node and two leaves) as the base learner<ul>
<li>Not very accurate at classification on their own</li>
<li>AdaBoost combines stumps to boost the performance so it creates a forest of stumps instead of forest of trees</li>
<li>In AdaBoost, stumps are created sequentially</li>
<li>The error of each stump affects the training data weight in the next stump</li>
<li>Depending on the performance, each stump gets different weight ($\lambda_i$) in the final classification decision</li>
</ul>
</li>
</ul>
<p>For boosting:</p>
<p><strong>Pros:</strong></p>
<ul>
<li>No need to use complex models (can boost performance of any weak learner)</li>
<li>Very simple to implement</li>
<li>Decreases the bias and variance</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Lack of interpretability</li>
<li>Slow during training and potentially testing</li>
</ul>
<h3 id="gradient-boosting">Gradient Boosting</h3>
<p>Gradient boosting is apply similar ideas for regression (but can be used for classification as well).</p>
<p>Simple linear regression or simple regression trees can be used as weak learners.</p>
<ol>
<li>Learn a regression predictor</li>
<li>Compute the error residual</li>
<li>Learn to predict residual</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>Bagging</th>
<th>Boosting</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Purpose</strong></td>
<td>Variance reduction</td>
<td>Bias reduction</td>
</tr>
<tr>
<td><strong>Models Used</strong></td>
<td>Used with high variance models</td>
<td>Used with high bias models</td>
</tr>
<tr>
<td><strong>Model Examples</strong></td>
<td>Random Forests</td>
<td>Linear classifiers / univariate decision trees (decision stumps)</td>
</tr>
</tbody></table>
<h1 id="neural-learning"><a href="#neural-learning">Neural Learning</a></h1>

<p>Artificial Neural Networks (NN) are inspired by human nervous systems.
NNs are composed of a large number of interconnected processing elements known as neurons.
They use supervised error correcting rules with back-propagation to learn a specific task.</p>
<p>A single perceptron has multiple inputs $x_1, ..., x_n$ and a binary output, where the output $o$ can be modeled as</p>
<p>$$
o(x_1, ..., x_n =
\begin{cases}
  +1 &amp; \text{if } w_0 + w_1x_1 + ... + x_0x_0 &gt; 0 \\
  -1 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>Or in vector notation:</p>
<p>$$
o(\mathbf{x} =
\begin{cases}
  +1 &amp; \text{if } \mathbf{w} \cdot \mathbf{x} &gt; 0
  -1 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>As a result a perceptron is able to represent some useful functions which are linearly separable</p>
<h2 id="perceptron-learning">Perceptron Learning</h2>
<p>Perceptron learning is simply an iterative weight-update scheme to find a good set of weights.</p>
<p>$$w_{i+1} \leftarrow w_i + \Delta w_i$$</p>
<p>where the weight update $\Delta w_i$ depends only on misclassified examples and is modulated by a &quot;smoothing&quot; parameter $\eta$ AKA learning rate.</p>
<p>For example, the update rule can be written as</p>
<p>$$w_{i+1} \leftarrow w_i + \eta y_ix_i$$</p>
<p>where</p>
<ul>
<li>$w_t$ is the weight at iteration $t$</li>
<li>$\eta$ is the learning rate</li>
<li>$y_i \in \{+1, 0, -1\}$ acts to chance the sign (so if the current weights resulted in a missclassification, $y_i$ would reflect the update that needs to be applied to the weight)</li>
<li>$x_i$ is the instance value</li>
</ul>
<p>Perception training will converge (under some mild assumptions) for linearly separable classification problems.</p>
<p>However, with a relatively minor modification many perceptrons can be combined together to form one model</p>
<ul>
<li>Multilayer perceptrons, the classic &quot;neural network&quot;</li>
</ul>
<h2 id="gradient-descent-1">Gradient Descent</h2>
<p>Gradient descent / ascent is an optimisation algorithm that seeks to minimize / maximize a function.</p>
<p>For a simple linear unit, where</p>
<p>$$o = w_0 + w_1x_1 + ... + x_nx_n$$</p>
<p>Let&#39;s learn $w_i$ that minimizes the squared error</p>
<p>$$E[w] = \frac{1}{2} \sum_{d \in D} (t_d - o_d)^2$$</p>
<p>where $D$ is the set of training samples.</p>
<p>The gradient is given by</p>
<p>$$\nabla E[w] = \left[ \frac{\partial E}{\partial w_0}, \frac{\partial E}{\partial w_1}, ..., \frac{\partial E}{\partial w_n} \right]$$</p>
<p>The gradient vector gives the direction of steepest increase in error $E$.
Negative of the gradient, i.e. steepest decrease, is what we want.</p>
<p>Training rule: $\Delta \mathbf{w} = -\eta \nabla E[\mathbf{w}]$, i.e. $\Delta w_i = - \eta \frac{\partial E}{\partial w_i}$.</p>
<p>$$
\begin{align*}
\frac{\partial E}{\partial w_i}
  &amp;= \frac{\partial}{\partial w_i} \frac{1}{2} \sum_{d \in D} (t_d - o_d)^2 \\
  &amp;= \frac{1}{2} \sum_d \frac{\partial}{\partial w_i} (t_d - o_d)^2 \\
  &amp;= \frac{1}{2} \sum_d 2(t_d - o_d) \frac{\partial}{\partial w_i}(t_d - o_d) \\
  &amp;= \sum_d (t_d - o_d) \frac{\partial}{\partial w_i} (t_d - \mathbf{w} \cdot \mathbf{x}_d) \\
  &amp;= \sum_d (t_d - o_d)(-x_{i,d})
\end{align*}
$$</p>
<p>Perceptron training rule guaranteed to succeed if:</p>
<ul>
<li>Training examples are linearly separable</li>
<li>Sufficiently small learn rate $\eta$</li>
</ul>
<p>Linear unit training rule uses gradient descent</p>
<ul>
<li>Guaranteed to converge to hypothesis with minimum squared error</li>
<li>Given sufficiently small learning rate $\eta$</li>
<li>Even when training data contains noise and or not separable by $H$</li>
</ul>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<p><strong>Batch mode</strong> Gradient Descent</p>
<ul>
<li>Do until satisfied<ul>
<li>Compute the gradient $\nabla E_D[\mathbf{w}] = \frac{1}{2} \sum_{d \in D} (t_d - o_d)^2$</li>
<li>$\mathbf{w} \leftarrow \mathbf{w} \eta \nabla E_D[\mathbf{w}]$</li>
</ul>
</li>
</ul>
<p><strong>Stochastic (incremental) mode</strong> Gradient Descent</p>
<ul>
<li>Do until satisfied<ul>
<li>For each training example $d \in D$<ul>
<li>Compute the gradient $\nabla E_d[\mathbf{w}] = \frac{1}{2}(t_d - o_d)^2$</li>
<li>$\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla E_d[\mathbf{w}]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>SGD can approximate Batch Gradient Descent arbitrarily closely, if $\eta$ made small enough</p>
<ul>
<li>Very useful for training large networks, or online learning from data streams</li>
<li>Stochastic implies examples should be selected at random</li>
</ul>
<h2 id="multilayer-networks">Multilayer Networks</h2>
<p>Multilayer networks can represent arbitrary functions.
It typically consists of an input, hidden, and output layer, each fully connected to the next.
The weights determine the function computed.
Given an arbitrary number of hidden units, any boolean function can be computed with a single hidden layer.</p>
<p>Properties of Artificial Neural Networks (ANN&#39;s):</p>
<ul>
<li>Many neuron-like threshold switching units</li>
<li>Many weighted interconnections among units</li>
<li>Highly parallel, distributed process</li>
</ul>
<p>Consider when</p>
<ul>
<li>Input is high-dimensional</li>
<li>Output can be a vector of values or discrete, or real valued</li>
<li>Form of target function is unknown</li>
<li>Interpretability of result is not important</li>
</ul>
<p>Examples include speech recognition, image classification, and many others.</p>
<h2 id="sigmoid-unit">Sigmoid Unit</h2>
<p>Same as a perceptron except step function is replaced by a nonlinear sigmoid function.</p>
<p>$$\sigma(x) = \frac{1}{1+e^{-x}}$$
Nonlinearity makes it easy for the model to generalise or adapt with variety of data and to differentiate between the output.</p>
<p>The sigmoid function is used because its derivative has a nice property</p>
<p>$$\frac{d \sigma(x)}{dx} = \sigma(x)(1 - \sigma(x))$$</p>
<p>We can derive gradient descent rules to train</p>
<ul>
<li>One sigmoid unit</li>
<li>Multilayer networks of sigmoid units -&gt; Backpropagation</li>
</ul>
<p>Note: in practice, particularly for deep networks, sigmoid functions are less common than other non-linear activation functions that are easier to train, however sigmoids are mathematically convenient.</p>
<p>To determine the error gradient of a sigmoid unit</p>
<p>Start by assuming we want to minimize squared error over a set of training examples $D$.</p>
<p>$$
\begin{align*}
\frac{\partial E}{\partial w_i}
  &amp;= \frac{\partial}{\partial w_i} \frac{1}{2} \sum_{d \in D}(t_d - o_d)^2 \\
  &amp;= \frac{1}{2}  \sum_{d \in D} \frac{\partial}{\partial w_i} (t_d - o_d)^2 \\
  &amp;= \frac{1}{2}  \sum_{d \in D} 2(t_d - o_d)\frac{\partial}{\partial w_i} (t_d - o_d) \\
  &amp;= \sum_{d \in D} 2(t_d - o_d) \left( \frac{\partial o_d}{\partial w_i} \right) \\
  &amp;= -\sum_{d \in D} 2(t_d - o_d) \frac{\partial o_d}{\partial net_d} \frac{\partial net_d}{\partial w_i}\\
\end{align*}
$$</p>
<p>We know:</p>
<p>$$\frac{\partial o_d}{\partial net_d} = \frac{\partial \sigma(net_d)}{\partial net_d} = o_d(1 - o_d)$$</p>
<p>$$\frac{\partial net_d}{\partial w_i} = \frac{\partial(\mathbf{w} \cdot \mathbf{x})d)}{\partial w_i} = x_{i,d}$$</p>
<p>so:</p>
<p>$$\frac{\partial E}{\partial w_i} = - \sum_{d \in D}(t_d - o_d) o_d (1 - o_d) x_{i,d}$$</p>
<h2 id="backpropagation-algorithm">Backpropagation Algorithm</h2>
<ul>
<li>Initialize all weights to small random numbers</li>
<li>Until satisified, do<ul>
<li>For each training example, do<ul>
<li>Input the training example to the network and compute the network outputs</li>
<li>For each output unit $k$
$$\delta_k \leftarrow o_k(1-o_k)(t_k-o_k)$$</li>
<li>For each hidden unit $h$
$$\delta_h \leftarrow o_h(1-o_h) \sum_{k \in outputs} w_{kh} \delta_k$$</li>
<li>Update each network weight $w_{ji}$
$$w_{ji} \leftarrow w_{ji} + \Delta w_{ji}$$
where
$$\Delta w_{ji} = \eta \delta_j x_{ji}$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>A solution for learning highly complex models:</p>
<ul>
<li>gradient descent over entire network weight vector</li>
<li>Easily generalised to arbitrary directed graphs</li>
<li>Can learn probabilistic models by maximising likelihood</li>
</ul>
<p>Minimises error over all training examples</p>
<ul>
<li>Training can take thousands of iterations -&gt; slow</li>
<li>Using network after training is very fast</li>
</ul>
<p>Will converge to a local, not necessarily global, error minimum</p>
<ul>
<li>Might exist many such local minima, but in practice often works well</li>
<li>Often include weight momentum $a$
$$\Delta w_{ji}(n) = \eta \delta_j x_{ji} + \alpha \Delta w_{ji} (n-1)$$</li>
<li>Stochastic gradient descent using &quot;mini-batches&quot;</li>
</ul>
<p>Many ways to regularise network, making it less likely to overfit</p>
<ul>
<li>Add term to error that increases with magnitude of weight vector
$$E(\mathbf{w}) = \frac{1}{2} \sum_{d \in D} \sum_{k \in outputs} (t_{kd} - o_{kd})^2 + \gamma \sum_{i,j} w_{ji}^2$$</li>
<li>Other ways to penalise large weights, e.g. weight decay</li>
<li>Using &quot;tied&quot; or shared set of weights, e.g. by setting all weights to their mean after computing the weight updates</li>
</ul>
<h2 id="neural-networks-for-classification">Neural Networks for Classification</h2>
<p>Minimizing square error does not work so well for classification.</p>
<p>If we take the output $o(x)$ as the probability of the class $\mathbf{x}$ being 1, the preferred loss function is the cross-entropy</p>
<p>$$- \sum_{d \in D} t_d \log o_d + (1 - t_d) \log (1 - o_d)$$</p>
<p>where:</p>
<p>$t_d \in \{0,1\}$ is the class label for training example $d$, and $o_d$ is the output of the sigmoid unit, interpreted as the probability of the class of training example $d$ being 1.</p>
<p>To train sigmoid units for classification using this setup, one can use gradient descent and backpropagation algorithm - this will yield the maximum likelihood solution.</p>
<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>
<p>CNNs are very similar to regular Neural Networks</p>
<ul>
<li>Made up of neurons with learnable weights</li>
</ul>
<p>CNN architecture assumes that inputs are images</p>
<ul>
<li>So we have local features</li>
<li>Which allows us to<ul>
<li>encode certain properties in the architecture that makes the forward pass more efficient nad</li>
<li>significantly reduces the number of parameters need for the network</li>
</ul>
</li>
</ul>
<p>The problem with regular NNs is that they do not scale well with dimensions.
In contrast, CNNS, consider 3D volumes of neurons and propose a parameter sharing scheme that minimises the number of params required by the network.</p>
<p>CNN neurons are arranged in 3 dimensions: Width, Height, and Depth.
Neurons in a layer are only connected to a small region of the layer before it (hence not fully connected).</p>
<p>Main layers:</p>
<ul>
<li>Convolutional<ul>
<li>Applies convolution operation (converts all pixels in its receptive field into a single value)</li>
</ul>
</li>
<li>Pooling<ul>
<li>the purpose is to reduce the spacial size of the representation to reduce the number of params and computation in the network and control overfitting</li>
</ul>
</li>
<li>Rectified Linear Unit (ReLU)<ul>
<li>Really an activation function ($f(x) = \max(0,x)$) favoured over traditional activation functions like Sigmoid or Tanh<ul>
<li>To accelerate the convergence of stochastic gradient descent</li>
<li>Be computationally inexpensive compared to traditional ones</li>
</ul>
</li>
</ul>
</li>
<li>Fully-connected<ul>
<li>A fully connected layer to all activations</li>
</ul>
</li>
<li>Drop-out<ul>
<li>A layer where each forward pass, some neurons are randomly set to zero</li>
<li>This reduces overfitting</li>
</ul>
</li>
<li>Output layers</li>
</ul>
<h1 id="unsupervised-learning"><a href="#unsupervised-learning">Unsupervised Learning</a></h1>

<p><strong>Supervised learning</strong></p>
<ul>
<li>Classes are known and need a &quot;definition&quot; in terms of the data</li>
<li>Methods are known as:<ul>
<li>classification</li>
<li>discriminant analysis</li>
<li>class prediction</li>
<li>supervised pattern recognition</li>
</ul>
</li>
</ul>
<p><strong>Unsupervised learning</strong></p>
<ul>
<li>Classes are initially unknown and need to be discovered with their definitions from the data</li>
<li>Methods are known as:<ul>
<li>cluster analysis</li>
<li>class discovery</li>
<li>unsupervised pattern recognition</li>
</ul>
</li>
</ul>
<h2 id="clustering">Clustering</h2>
<p>Clustering algorithms form two broad categories:</p>
<ul>
<li><strong>Partitioning</strong> methods<ul>
<li>Typically require specification of the number of clusters</li>
</ul>
</li>
<li><strong>Hierarchical</strong> methods<ul>
<li>Hierarchical algorithms are either <strong>agglomerative</strong> i.e., bottom-up or <strong>divisive</strong> i.e., top-down</li>
<li>In practice, hierarchical agglomerative methods are often used</li>
<li>But more importantly to users, the dendrogram, or tree, which can be visualized in hierarchical methods</li>
</ul>
</li>
</ul>
<p>Let $X = \{x_1, ..., x_m \}$ be a set of instances</p>
<p>Let $C = (C_1, ..., C_k)$ be a partition of $m$ elements into $k$ subsets.</p>
<p>Each subset is called a cluster, and $C$ is called a clustering.</p>
<h2 id="k-means-clustering">K-means Clustering</h2>
<p>Set value for $k$, th number of clusters</p>
<p>Initialize: choose points for centres (means) of $k$ clusters (at random)</p>
<p>Procedure:</p>
<ol>
<li>Assign each instance $x$ to the closest of the $k$ points to form $k$ clusters</li>
<li>Re-assign the $k$ points to be the means of each of the $k$ clusters</li>
<li>Repeat 1 and 2 until convergence to a reasonably stable clustering</li>
</ol>
<p>Despite weaknesses (suffering from outliers, or trapped in local minimum), $k$-means is still the most popular algorithm due to simplicity and efficiency.
There is no clear evidence that any other clustering algorithm performs better in general.</p>
<h2 id="expectation-maximization">Expectation Maximization</h2>
<p>For $k = 2$, think of the full description of each instance as $y_i = (z_i, z_{i1}, z_{i2})$, where</p>
<ul>
<li>$z_{ij}$ is 1 if $x_i$ generated by $j^{th}$ Gaussian, otherwise zero</li>
<li>$x_i$ is observable, from instance set $x_1, x_2, ..., x_m$</li>
<li>$z_{ij}$ is unobservable</li>
</ul>
<p>Given:</p>
<ul>
<li>Instances from $X$ generated by mixture of $k$ Gaussian distributions</li>
<li>Unknown means of the $k$ Gaussians</li>
<li>Assuming identity covariance matrix</li>
<li>Don&#39;t know whcih instance $x_i$ was generated by which Guassian</li>
</ul>
<p>Determine Maximum likelihood estimates of</p>
<ul>
<li>$\text{means}(\mu_1, ...,\mu_k)$</li>
</ul>
<p><strong>Initialize:</strong> Pick random initial $h = (\mu_1, ..., \mu_k$</p>
<p><strong>Iterate:</strong>
<strong>E step:</strong> Calculate expected value $E[z_{ij}$ of each hidden variable $z_{ij}$, assuming current hypothesis $h = (\mu_1, ..., \mu_k)$ holds:</p>
<p>$$
\begin{align*}
E[z_{ij}]
  &amp;= \frac{P(x=x_i|\mu=\mu_j)}{\sum_{n=1}^k p(x=x_i|\mu=\mu_n)} \\
  &amp;= \frac{\exp \left( - \frac{1}{2 \sigma^2} (x_i - \mu_j)^2 \right)}{\sum_{n=1}^k \exp \left( - \frac{1}{2 \sigma^2} (x_i - \mu_n)^2 \right)}
\end{align*}
$$</p>
<p><strong>M step:</strong> Calculate new maximum likelihood hypthesis $h&#39; = (\mu_1&#39;, ..., \mu_k&#39;)$ assuming value taken on each hidden variable $z_{ij}$ is the expected value $E[z_{ij}]$ calculated before.
Replace $h = (\mu_1, ..., \mu_k)$ by $h&#39; = (\mu_1&#39;, ..., \mu_k&#39;)$.</p>
<p>$$\mu_j \leftarrow \frac{\sum_{i=1}^m E[z_{ij}] x_i}{\sum_{i=1}^m E[z_{ij}]}$$</p>
<p>Converges to local maximum likelihood $h$ and provides estimates of hidden variables $z_{ij}$.</p>
<p>In fact, local maximum in $E[\ln P(Y|h)]$</p>
<ul>
<li>$Y$ is complete (observable plus unobservable variables) data</li>
<li>Expected value taken over possible values of unobserved variables in $Y$</li>
</ul>
<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>
<p><strong>Bottom up:</strong> at each step join two closest clusters (starting with single-instance clusters)</p>
<ul>
<li>Design decision: distance between clusters E.g., two closest instances in clusters vs distance between means</li>
</ul>
<p><strong>Top down:</strong> find two clusters and then proceed recursively for the two subsets</p>
<ul>
<li>Can be very fast</li>
</ul>
<h3 id="determining-number-of-clusters">Determining number of clusters</h3>
<p>Elbow method:</p>
<ul>
<li>Measure within-cluster dispersion (sum of squared distances from each point to cluster centroid)</li>
<li>Compute this for various $k$ choices</li>
<li>Choose the $k$ that doesn&#39;t improve the dispersion that much</li>
</ul>
<p>Gap statistics:</p>
<ul>
<li>Cluster observed data and compute the corresponding total with-cluster variation $W_k$</li>
<li>Generate $b$ reference data sets with random uniform distribution.
Cluster each of these reference data sets and compute the corresponding total within-cluster variation $W_{kb}$</li>
<li>Compute the estimated gap statistic as the deviation of the observed $W_k$ value from its expected value $W_{kb} : Gap(k) = \frac{1}{B} \sum_{b} \log(W_{kb}) - \log(W_k)$.
Compute also the std dev of the statistics</li>
<li>Choose $k$ as the smallest value s.t. the gap statistic is within 1 std dev of the gap at $k + 1$
$$Gap(k) \geq Gap(k + 1) - s_{k+1}$$</li>
</ul>
<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>Key idea: look for features in a transformed space so that each dimension in the new space captures the most variation in the original data when it is projected onto that dimension.</p>
<ol>
<li>Take the data as an $m \times n$ matrix $X$</li>
<li>&quot;centre&quot; the data by subtracting the mean of each column</li>
<li>Construct covariance matrix $C$ from centred matrix</li>
<li>Compute eigenvector matrix $V$ (rotation) and eigenvalue matrix $S$ (scaling) such that $V^{-1}CV = S$ and $S$ is a diagonal $n \times n$ matrix</li>
<li>Sort columns of $S$ in decreasing order (decreasing variance) and their corresponding eigenvectors</li>
<li>Remove columns of $S$ and $V$ that the eigenvalues are below some minimum threshold</li>
<li>Transform the original data using the remaining eigenvectors</li>
</ol>
<ul>
<li>PCA complexity is cubic in number of original features (not feasible for high-dimensional datasets)</li>
<li>Alternatively can use random projections to approximate the sort of projection found by PCA</li>
</ul>
<h2 id="semi-supervised-learning">Semi-supervised Learning</h2>
<ol>
<li>Learn initial classifier using labelled set</li>
<li>Apply classifier to unlabelled set</li>
<li>The most confident predictions of each classifier on the unlabelled data are retained</li>
<li>Learn new classifier from now-labelled data</li>
<li>Repeat until convergence</li>
</ol>
<h2 id="self-training-algorithm">Self-training algorithm</h2>
<p><strong>Given:</strong> labelled data $(x,y)$ and unlabelled data $(x)$</p>
<p><strong>Repeat:</strong></p>
<ul>
<li>Train classifier $H$ from labelled data using supervised learning</li>
<li>Label unlabelled data using classifier $h$</li>
</ul>
<p><strong>Assumes:</strong> classifications by $h$ will tend to be correct (especially high probability ones)</p>
<h2 id="co-training">Co-Training</h2>
<p><strong>Key idea:</strong> two views of an instance, $f_1$ and $f_2$</p>
<ul>
<li>assume $f_1$ and $f_2$ independent and compatible</li>
<li>If we hve a good attribute set, leverage similarity between attribute values in each view, assuming they predict the class, to classify the unlabelled data</li>
</ul>
<p>Multi-view learning</p>
<ul>
<li>Given two (or more) perspectives on data, e.g., different attribute sets</li>
<li>Train separate models for each perspective on small set of labelled data</li>
<li>Use models to label a subset of the unlabelled data</li>
<li>Repeat until no more unlabelled examples</li>
</ul>
<h1 id="learning-theory"><a href="#learning-theory">Learning Theory</a></h1>

<h2 id="computational-learning-theory">Computational Learning Theory</h2>
<p><strong>Machine learning:</strong> Have a computer solve problems by learning from data
rather than being explicitly programmed.</p>
<ul>
<li>Regression</li>
<li>Classification</li>
<li>Clustering</li>
<li>Ranking</li>
<li>Reinforcement Learning</li>
<li>...</li>
</ul>
<p><strong>Inductive learning:</strong> learning from examples and all machine learning
algorithms are a kind of inductive learning and there are some
questions that we are interested to be able to answer in such
framework:</p>
<ul>
<li>Probability of successful learning</li>
<li>Number of training examples</li>
<li>Complexity of hypothesis space</li>
<li>Time complexity of learning algorithm</li>
<li>Accuracy to which target concept is approximated</li>
<li>Manner in which training examples presented</li>
<li>...</li>
</ul>
<p>Instead of focusing on particular algorithms, learning theory aims to characterize classes of algorithms.</p>
<h2 id="probably-approximately-correct">Probably Approximately Correct</h2>
<p>Probably Approximately Correct (PAC) is a framework for mathematical analysis of learning which was proposed by Valiant in 1984.</p>
<p>The framework of PAC learning can be used to address questions such as:</p>
<ul>
<li>How many training examples?</li>
<li>How much computational effort required?</li>
<li>How complex a hypothesis class needed?</li>
<li>How to quantify hypothesis complexity?</li>
<li>How many mistakes will be made?</li>
</ul>
<h3 id="sample-complexity">Sample Complexity</h3>
<p><strong>Given:</strong></p>
<ul>
<li>set of instances $X$</li>
<li>set of hypotheses $H$</li>
<li>set of possible target concepts $C$</li>
<li>training instances generated by a fixed, unknown probability distribution $\mathcal{D}$ over $X$</li>
</ul>
<p>Learner observes a sequence $D$ of training examples of form $(x,c(x))$, for some target concept $c \in C$</p>
<ul>
<li>Instances $x$ are drawn from distribution $\mathcal{D}$</li>
<li>teacher provides target value $c(x)$ for each $x$</li>
</ul>
<p>Learner must output a hypothesis $h$ estimating $c$</p>
<ul>
<li>$h$ is evaluated by its performance on subsequent instances drawn according to $\mathcal{D}$</li>
</ul>
<p>Note: randomly drawn instances</p>
<p><strong>True Error of a Hypothesis</strong>: The <strong>true error</strong> (denoted $error_{\mathcal{D}}(h)$) of hypothesis $h$ with respect to target concept $c$ and distribution $\mathcal{D}$ is the probability that $h$ will misclassify an instance drawn at random according to $\mathcal{D}$.</p>
<p>$$error_{\mathcal{D}}(h) \equiv \text{Pr}_{X \in \mathcal{D}} [c(x) \neq h(x)]$$</p>
</article></span></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"contents":"\u003ch1 id=\"statistical-techniques-for-data-analysis\"\u003e\u003ca href=\"#statistical-techniques-for-data-analysis\"\u003eStatistical Techniques for Data Analysis\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eProbability vs Statistics\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProbability:\u003c/strong\u003e reasons from populations to samples (deductive reasoning)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStatistics:\u003c/strong\u003e reasons from samples to populations (inductive reasoning)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSampling\u003c/strong\u003e is a way to draw conclusions about the population without measuring the whole population.\nThe characteristics of an ideal sampling model include\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNo systematic bias\u003c/li\u003e\n\u003cli\u003eThe chance of obtaining an unrepresentative sample can be calculated\u003c/li\u003e\n\u003cli\u003eThe chance of obtaining an unrepresentative sample decreases with the size of the sample\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"estimation\"\u003eEstimation\u003c/h2\u003e\n\u003cp\u003eIn statistics, estimation refers to the process by which one makes inferences about a population.\u003c/p\u003e\n\u003ch2 id=\"mean\"\u003eMean\u003c/h2\u003e\n\u003cp\u003eThe arithmetic mean is $m = \\frac{1}{n}\\sum_{i=1}^n x_i$, where the observations are $x_1, x_2, ..., x_n$.\nIf $x_i$ occurs $f_i$ times, and we define relative frequencies as $p_i = \\frac{f_i}{n}$, then the mean is $m = \\sum_i x_i p_i$.\nTherefore the expected value of a discrete random variable $X$ is:\u003c/p\u003e\n\u003cp\u003e$$E(X) = \\sum_i x_i P(X=x_i)$$\u003c/p\u003e\n\u003ch2 id=\"variance\"\u003eVariance\u003c/h2\u003e\n\u003cp\u003eVariance measures how far a set of random numbers are spread out from their average value.\nStandard deviation (square root of variance) is estimated as:\u003c/p\u003e\n\u003cp\u003e$$s = \\sqrt{\\frac{1}{n-1} \\sum_i (x_i - m)^2}$$\u003c/p\u003e\n\u003cp\u003eWe can write this in terms of expected values $E(X)$ as\u003c/p\u003e\n\u003cp\u003e$$Var(x) = E((X - E(X))^2) = E(X^2) - [E(x)]^2$$\u003c/p\u003e\n\u003cp\u003eVariance is the \u0026quot;mean of squares minus the squares of means\u0026quot;\u003c/p\u003e\n\u003ch2 id=\"covariance-and-correlation\"\u003eCovariance and Correlation\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCovariance\u003c/strong\u003e is a measure of relationship between two random variables:\u003c/p\u003e\n\u003cp\u003e$$cov(x,y) = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{(\\sum_i x_i y_i) - n \\bar{x} \\bar{y}}{n-1}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCorrelation\u003c/strong\u003e is a measure to show how strongly a pair of random variables are related:\u003c/p\u003e\n\u003cp\u003e$$r = \\frac{cov(x,y)}{\\sqrt{var(x)}\\sqrt{var(y)}}$$\u003c/p\u003e\n\u003cp\u003eThis is also called Pearson\u0026#39;s correlation coefficient.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe correlation coefficient is a number between $-1$ and $+1$ that shows whether a pair of variables $x$ and $y$ are associated or not and whether their scatter in the association is high or low:\u003cul\u003e\n\u003cli\u003eA value close to 1 shows signifies strong positive association between $x$ and $y$, while a value close to $-1$ shows a strong inverse association\u003c/li\u003e\n\u003cli\u003eA value near 0 indicates there is no association and there is a large scatter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThis is only approximate when $x$ and $y$ are roughly linearly associated (does not work well with association is curved)\u003c/li\u003e\n\u003cli\u003eDo not use correlation to imply $x$ causes $y$ or the other way around\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"bias-and-variance\"\u003eBias and Variance\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBias\u003c/strong\u003e is the difference between the average prediction of our model and the correct value.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eModels with high bias pays very little attention to the training data and oversimplifies the model.\u003c/li\u003e\n\u003cli\u003eIt always leads to high error on training and test data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eVariance\u003c/strong\u003e is the variability of model prediction for a given data point or a value which tells us spread of our data.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eModels with high variances pays a lot of attention to training data and does not generalize unseen data.\u003c/li\u003e\n\u003cli\u003eAs a result, such models perform very well on training data but has high error rates on test data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"bias-variance-decomposition\"\u003eBias-Variance Decomposition\u003c/h3\u003e\n\u003cp\u003eWhen we assume $y = f + \\epsilon$ and we estimate $f$ with $\\hat{f}$, the expectation of error:\u003c/p\u003e\n\u003cp\u003e$$E[(y - \\hat{f})^2] = (f - E[\\hat{f}])^2 + Var(\\hat{f}) + Var(\\epsilon)$$\u003c/p\u003e\n\u003cp\u003eSo, the mean of squared error (MSE) can be written as\u003c/p\u003e\n\u003cp\u003e$$MSE = Bias^2 + Variance + Irreducible Error$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIrreducible error\u003c/strong\u003e is associated with a natural variability in a system (noise).\nIt can not be reduced since it is due to unknown factors or due to chance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReducible error\u003c/strong\u003e, as the name suggests, can be and should be minimized further by adjustments to the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"bias-variance-tradeoff\"\u003eBias-Variance Tradeoff\u003c/h3\u003e\n\u003cp\u003eWhen comparing unbiased estimators, we would like select the one with minimum variance\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf our model is too simple and has very few parameters, then it may have high bias and low variance.\u003c/li\u003e\n\u003cli\u003eIf it has a large number of parameters, it may have high variance and low bias.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"regression\"\u003e\u003ca href=\"#regression\"\u003eRegression\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eRegression can be used to predict numeric values from numeric attributes.\nIn linear models, the outcome is a linear combination of attributes:\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n = \\sum_{i=0}^n \\theta_i x_i = h(x)$$\u003c/p\u003e\n\u003cp\u003eThere is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnivariate regression: one input variable/feature/attribute is used to predict one output variable\u003c/li\u003e\n\u003cli\u003eMultiple regression / multivariable regression: more than one variable/features/attributes are used to predict one output variable\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"least-squares\"\u003eLeast Squares\u003c/h2\u003e\n\u003cp\u003eThe most popular estimation model is \u0026quot;Least Squares\u0026quot; for fitting a linear regression.\nError is defined as difference between the predicted and actual value, i.e.\u003c/p\u003e\n\u003cp\u003e$$J(\\theta) = \\sum_{y=1}^m \\left(y_j - \\sum_{i=0}^n \\theta_i x_{ji} \\right)^2 = \\sum_{j=1}^m \\left(y_j - x_j^T \\theta \\right)^2 = (y - X \\theta)^T (y - X \\theta)$$\u003c/p\u003e\n\u003cp\u003eWe want to minimize the error over all samples.\u003c/p\u003e\n\u003ch2 id=\"gradient-descent\"\u003eGradient Descent\u003c/h2\u003e\n\u003cp\u003eThe goal of Linear Regression is to minimise $J(\\theta)$.\nIf we compute $J(\\theta)$ for different values of $\\theta$, we get a convex function with one global minima.\nTo find the value of $\\theta$ that provides this minimum, we can use gradient descent.\u003c/p\u003e\n\u003cp\u003eGradient descent starts with some initial $\\theta$, and repeatedly performs an update:\u003c/p\u003e\n\u003cp\u003e$$\\theta_i^{(t+1)} := \\theta_t^{(t)} - \\alpha \\frac{\\partial}{\\partial \\theta_i} J \\left( \\theta_i^{(t)} \\right)$$\u003c/p\u003e\n\u003cp\u003ewhere $\\alpha$ is the learning rate / step size, and each iteration it takes a step in the direction of the steepest decrease in $J(\\theta)$.\u003c/p\u003e\n\u003cp\u003eThe partial derivative term for $J(\\theta)$ can be determined as\u003c/p\u003e\n\u003cp\u003e$$\\frac{\\partial}{\\partial \\theta_i} J(\\theta) = -2 \\left( y_j - h_\\theta (x_j) \\right) x_{ji}$$\u003c/p\u003e\n\u003cp\u003eSo, for a \u003cstrong\u003esingle training sample\u003c/strong\u003e, the update Least Mean Squares (LMS) rule is:\u003c/p\u003e\n\u003cp\u003e$$\\theta_{i+1} := \\theta_i + 2 \\alpha \\left( y_j - h_\\theta (x_j) \\right) x_{ji}$$\u003c/p\u003e\n\u003cp\u003eFor other samples, there are two methods.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eBatch Gradient Descent:\u003c/strong\u003e\n$$\\theta_i^{t+1} = \\theta_i^{(t)} + \\alpha \\frac{2}{m} \\sum_{j=1}^m \\left( y_j - h_{\\theta^{(t)}} (x_j) \\right) x_{ji}$$\nReplace the gradient with the sum of gradient for all samples and continue until convergence (when the estimated $\\theta$ is stabilized).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStochastic Gradient Descent:\u003c/strong\u003e\n$$ \\text{ for } j = 1 \\text{ to } m \\left\\{ \\theta _ i = \\theta_i + 2 \\alpha \\left( y_jj - h _ \\theta (x _ j) \\right) x _ ji \\text{ for every } i \\right\\}$$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn stochastic gradient descent, $\\theta$ gets updated at any sample separately, so it is faster to calculate, but may never converge to the minimum.\u003c/p\u003e\n\u003ch2 id=\"minimizing-squared-error-normal-equations\"\u003eMinimizing Squared Error (normal equations)\u003c/h2\u003e\n\u003cp\u003eGradient descent is an iterative algorithm, however, you may find the minimum of $J(\\theta)$ by finding and setting it\u0026#39;s derivatives to 0.\nThis is also called the exact or closed-form solution.\u003c/p\u003e\n\u003cp\u003e$$\n\\frac{\\partial}{\\partial \\theta} J(\\theta) = 0 \\\\\nJ(\\theta) = ( \\mathbb{y} - X \\theta )^T (\\mathbb{y} - X \\theta) \\\\\n\\frac{\\partial}{\\partial \\theta} J(\\theta) = - 2X^T (\\mathbb{y} - X \\theta) = 0 \\\\\nX^T (\\mathbb{y} - X \\theta) = 0 \\\\\n\\theta = (X^TX)^{-1}X^T \\mathbb{y}\n$$\u003c/p\u003e\n\u003ch2 id=\"minimizing-squared-error-normal-equations-1\"\u003eMinimizing Squared Error (normal equations)\u003c/h2\u003e\n\u003cp\u003eWe can write the relationship between input variable $x$ and output variable $y$ as:\u003c/p\u003e\n\u003cp\u003e$$y_j = x_j^T \\theta + \\epsilon_j$$\u003c/p\u003e\n\u003cp\u003eAnd $\\epsilon_j$ is an error term which might be unmodeled effect or random noise.\nAssume $\\epsilon_j$\u0026#39;s are independent and identically distributed (i.i.d.) according to a Gaussian distribution, i.e.:\u003c/p\u003e\n\u003cp\u003e$$\n\\epsilon_j \\sim N \\left( 0, \\sigma^2 \\right) \\\\\np(\\epsilon_j) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{e_j^2}{2 \\sigma^2} \\right)\n$$\u003c/p\u003e\n\u003cp\u003eThis implies that:\u003c/p\u003e\n\u003cp\u003e$$p(\\epsilon_j) = p(y_j | x_j; \\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{e_j^2}{2 \\sigma^2} \\right)$$\u003c/p\u003e\n\u003cp\u003eSo we want to estimate $\\theta$ s.t. we maximise the probability of output $y$ given input $x$ over all $m$ training samples:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\mathcal{L}(\\theta)\n    \u0026amp;= p(\\mathbb{y} | X, \\theta) \\\\\n    \u0026amp;= \\prod_{j=1}^m \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{\\left( y_j - x_j^T \\theta \\right)^2}{2 \\sigma^2} \\right)\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eNote this is called the \u003cstrong\u003eLikelihood\u003c/strong\u003e function.\u003c/p\u003e\n\u003cp\u003eHowever, to find $\\theta$ that maximises $\\mathcal{L}(\\theta)$, we can also maximise any strictly increasing function of $\\mathcal{L}(\\theta)$.\nIn this case, we can find \u003cstrong\u003emaximize the log likelihood\u003c/strong\u003e $mathcal{l}(\\theta)$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\mathcal{l}(\\theta)\n    \u0026amp;= \\log \\mathcal{L}(\\theta) \\\\\n    \u0026amp;= \\log \\prod_{j=1}^m \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{\\left( y_j - x_j^T \\theta \\right)^2}{2 \\sigma^2} \\right) \\\\\n    \u0026amp;= m \\log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right) - \\frac{1}{\\sigma^2} \\frac{1}{2} \\sum_{j=1}^m \\left( y_j - x_j^T \\theta \\right)^2\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eSo maximizing $\\mathcal{l}(\\theta)$ is equal to minimising $\\sum_{j=1}^m \\left( y_j - x_j^T \\theta \\right)^2$.\nThis means under certain assumptions, the least squared regression is equivalent to finding maximum likelihood estimate of $\\theta$.\u003c/p\u003e\n\u003ch2 id=\"linear-regression-assumptions\"\u003eLinear Regression Assumptions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLinearity:\u003c/strong\u003e The relationship between $x$ and the mean of $y$ is linear\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHomoscedasticity:\u003c/strong\u003e The variance of residual is the same of any value of $x$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndependence:\u003c/strong\u003e Observations are independent of each other\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNormality:\u003c/strong\u003e For any fixed value of $x$, $y$ is normally distributed\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"univariate-linear-regression\"\u003eUnivariate Linear Regression\u003c/h2\u003e\n\u003cp\u003eIn univariate regression we aim to find the relationship between $y$ and one independent variable $x$.\u003c/p\u003e\n\u003cp\u003eAs an example, if we want to investigate the relationship between height and weight, we could collect $m$ measurements\u003c/p\u003e\n\u003cp\u003e$$(h_j, w_j), \\quad j = 1, ..., m$$\u003c/p\u003e\n\u003cp\u003eUnivariate linear regression assumes a linear relation, $\\hat{w} = \\theta_0 + \\theta_1 h$.\u003c/p\u003e\n\u003ch2 id=\"multiple-linear-regression\"\u003eMultiple Linear Regression\u003c/h2\u003e\n\u003cp\u003eHere, we model the relationship of $y$ to several other variables.\u003c/p\u003e\n\u003cp\u003eAs an example, say we now want to predict people\u0026#39;s weight from their height and body frame size (i.e. wrist size).\nWe collect $m$, height, weight and body frame measurements:\u003c/p\u003e\n\u003cp\u003e$$(h_j, f_j, w_j), \\quad j = 1, ..., m$$\u003c/p\u003e\n\u003cp\u003eThe linear regression model is now:\u003c/p\u003e\n\u003cp\u003e$$\\hat{w} = \\theta_0 + \\theta_1 h + \\theta_2 f$$\u003c/p\u003e\n\u003ch2 id=\"linear-regression-for-curves\"\u003eLinear Regression for curves\u003c/h2\u003e\n\u003cp\u003eWe can also apply some changes to produce curves with linear regression, i.e.\u003c/p\u003e\n\u003cp\u003eTo model:\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2$$\u003c/p\u003e\n\u003cp\u003ewe can treat $x_2 = x_1^2$ and use the model\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\u003c/p\u003e\n\u003cp\u003eThese nonlinear models can still be treated like linear regression and can fit curvature.\nThey are still linear in parameters.\nNon linear regression is not linear in parameters, i.e., $\\hat{y} = \\frac{\\theta_1 x}{\\theta_2 + x}$\u003c/p\u003e\n\u003cp\u003eHowever, increasing the degree of the model can result in overfitting.\u003c/p\u003e\n\u003ch2 id=\"regularisation\"\u003eRegularisation\u003c/h2\u003e\n\u003cp\u003eRegularisation is a method to avoid overfitting by adding constraints to the weight vector.\nAn approach is to ensure the weights are, small in magnitude: this is called shrinkage.\u003c/p\u003e\n\u003cp\u003eThe cost function, given data $(x_1, y_1), ..., (x_m, y_m)$ is\u003c/p\u003e\n\u003cp\u003e$$J(\\theta) = \\sum_j (y_j - h_\\theta (x_j))^2 + \\lambda \\sum_i \\theta_i^2$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eParameter estimation by optimisation wil attempt to find values for $\\theta_0, ..., \\theta_n$ s.t. $J(\\theta)$ is a minimum\u003c/li\u003e\n\u003cli\u003eSimilar to before, this can be solved by gradient descent or finding the closed form solution\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe multiple least-squares regression problem is an optimisation problem, and can be written as:\u003c/p\u003e\n\u003cp\u003e$$\\theta* = \\arg \\min_\\theta (y - X \\theta)^T (y - X\\theta)$$\u003c/p\u003e\n\u003cp\u003eThe regularised version of this is then as follows:\u003c/p\u003e\n\u003cp\u003e$$\\theta* = \\arg \\min (y - X \\theta)^T (y - X \\theta) + \\lambda || \\theta ||^2$$\u003c/p\u003e\n\u003cp\u003eWhere $||\\theta||^2 = \\sum_i \\theta_i^2$ is the squared norm of the vector $\\theta$, or equivalently, the dot product $\\theta^T\\theta$; $\\lambda$ is a scalar determining the amount of regularisation.\u003c/p\u003e\n\u003cp\u003eThis still has a closed form solution\u003c/p\u003e\n\u003cp\u003e$$\\theta = (X^TX + \\lambda I)^{-1} X^T y$$\u003c/p\u003e\n\u003cp\u003ewhere $I$ is the identity matrix.\nRegularisation amounts to adding $\\lambda$ to the diagonal of $X^TX$, improves the numerical stability of matrix inversion.\nThis form of least-squares regression is known as \u003cstrong\u003eridge regression\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAn alterative of regularised regression is provided by the \u003cstrong\u003elasso\u003c/strong\u003e.\nIt replaces the term $\\sum_i \\theta_i^2$ with $\\sum_i |\\theta_i |$.\nThe result is that some weights are shrunk, but others are set to 0, hence this favours sparse solutions.\u003c/p\u003e\n\u003ch2 id=\"model-selection\"\u003eModel Selection\u003c/h2\u003e\n\u003cp\u003eSuppose there are a lot of variables / features $(x)$.\nTaking all the features will lead to an overly complex model.\nThere are 3 ways to reduce complexity.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSubset-selection\u003c/strong\u003e, by search over subset lattice.\nEach subset results in a new model, and the problem is so select one of the models.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShrinkage\u003c/strong\u003e, or regularization of coefficients to zero, by optimization.\nThere is a single model, and unimportant variables have near-zero coefficients.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDimensionality-reduction\u003c/strong\u003e, by projecting points into a lower dimensional space (this is different to subset-selection)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"classification\"\u003e\u003ca href=\"#classification\"\u003eClassification\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eClassification doesn\u0026#39;t have convenient mathematical properties, so in classification, train a classifier, which is usually a function that maps from an input data point to a set of discrete outputs (i.e. the classes).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGenerative algorithm:\u003c/strong\u003e\nbuilds some models for each of the classes and then makes classification predictions based on looking at the test example see it is more similar to which of the models\u003cul\u003e\n\u003cli\u003eLearns $p(x|y)$\u003c/li\u003e\n\u003cli\u003eSo, we can get $p(x,y) = p(x|y)p(y)$\u003c/li\u003e\n\u003cli\u003eIt learns the mechanism by which the data has been generated\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDiscriminative algorithm:\u003c/strong\u003e\nDo not build models for difference classes, but rather focuses on finding a decision boundary that separates classes\u003cul\u003e\n\u003cli\u003eLearns $p(y|x)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"linear-classification\"\u003eLinear Classification\u003c/h2\u003e\n\u003cp\u003eFor a linear classifier of 2 features and 2 classes\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe a line that separates the two classes: $ax_1 + bx_2 + c = 0$.\u003c/li\u003e\n\u003cli\u003eWe define a weight vector $w^T = [a,b]$, $x^T = [x_1,x_2]$.\u003c/li\u003e\n\u003cli\u003eSo, the line can be defined by $x^Tw = -c = t$\u003c/li\u003e\n\u003cli\u003e$w$ is perpendicular to the decision boundary\u003c/li\u003e\n\u003cli\u003e$t$ is the decision threshold (if $x^Tw \u0026gt; t$, $x$ belongs to $P$ but if $x^Tw \u0026lt; t$, $x$ belongs to $N$)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf $p$ and $n$ is the respective centers of mass of the positive and negative points, the basic linear classifier is described by the equation $x^Tw = t$ and $w = p - n$.\u003c/p\u003e\n\u003cp\u003eAs we know, $\\frac{p+n}{2}$ is on the decision boundary, so we have:\u003c/p\u003e\n\u003cp\u003e$$t = \\left( \\frac{p+n}{2} \\right)^T \\cdot (p-n) = \\frac{||p||^2 - ||n||^2}{2}$$\u003c/p\u003e\n\u003cp\u003ewhere $||x||$ denotes the length of vector $x$.\u003c/p\u003e\n\u003ch2 id=\"generalisation\"\u003eGeneralisation\u003c/h2\u003e\n\u003cp\u003eGeneralisation means how well a trained model can classify or forecast unseen data.\nThere are 3 basic assumptions for generalisation\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExamples are drawn independent and identically (i.i.d) at random for the distribution\u003c/li\u003e\n\u003cli\u003eThe distribution is stationary; that is the distribution doesn\u0026#39;t change within the data set\u003c/li\u003e\n\u003cli\u003eWe always pull from the same distribution (for training, validation and test samples)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"cross-validation\"\u003eCross-validation\u003c/h2\u003e\n\u003cp\u003eCV is a validation technique to assess the results of a model to an independent data set.\nA portion of the dataset is used to train the model, and another is used to test.\nTypes of CV include:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eHoldout method\u003c/strong\u003e\nRandomly assign data points to two sets $d_0$ (training set) and $d_1$ (test set).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLeave-One-Out Cross validation (LOOCV)\u003c/strong\u003e\nA variation of the leave-p-out cross validation where 1 sample is left out, to test, whilst the rest is used for training.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eK-fold Cross Validation\u003c/strong\u003e\nPartition the dataset into $k$ equally sized subsamples.\nOf the $k$ subsamples, one subsample is used as for testing, and the rest for training.\nRepeat with all subsets to produce a single estimation\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThere are certain parameters that need to be estimated during learning.\nWe use the data, but NOT the training set, OR the test set.\nInstead we use a separate validation or development set.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eValidation set:\u003c/strong\u003e To make the hyperparameter tuning and model selection independent from the test set, we define another set within the train set.\u003c/p\u003e\n\u003ch2 id=\"evaluation-of-error\"\u003eEvaluation of error\u003c/h2\u003e\n\u003cp\u003eIf we have a binary classification, we can have a contingency table, AKA confusion matrix:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003ePredicted Positive\u003c/th\u003e\n\u003cth\u003ePredicted Negative\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eActual Positive\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eTrue Positive (TP)\u003c/td\u003e\n\u003ctd\u003eFalse Negative (FN)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eActual Negative\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eFalse Positive (FN)\u003c/td\u003e\n\u003ctd\u003eTrue Negative (TN)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eClassification Accuracy on a sample of labelled pairs $(x,c(x))$ given a learned classification model that predicts, for each instance $x$, a class value $\\hat{c}(x)$:\u003c/p\u003e\n\u003cp\u003e$$acc = \\frac{1}{|Test|} \\sum_{x \\in Test} I[ \\hat{c}(x) = c(x) ]$$\u003c/p\u003e\n\u003cp\u003ewhere $Test$ is a test set, and $I[]$ is the indicator function which is 1 iff its argument evaluates to true and 0 otherwise.\nThe classification Error $= 1 - acc$.\u003c/p\u003e\n\u003cp\u003eOther evaluation metrics:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePrecision / correctness\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eis the number of relevant objects classified correctly divided byt he total number of relevant objects classified\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$Precision = \\frac{TP}{TP + FP}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRecall / sensitivity / completeness / true positive rate (TPR)\u003c/strong\u003e\nis the number of relevant objects classified correctly divided by total number of relevant / correct objects\u003c/p\u003e\n\u003cp\u003e$$Recall = \\frac{TP}{TP + FN}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eF1 score\u003c/strong\u003e\nis the harmonic mean of precision and recall and is defined as:\u003c/p\u003e\n\u003cp\u003e$$F_1 = \\frac{2 \\times precision \\times recall}{precision + recall}$$\u003c/p\u003e\n\u003cp\u003eThis measure gives equal importance to precision and recall which is sometime undesirable; so, we have to decide which metric to use depending on the task and what\u0026#39;s important for the task.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAUC-ROC (Area Under the Curve - Receiver Operating Characteristics)\u003c/strong\u003e\nis an important curve for performance of classification models.\nIt evaluates the model at different threshold settings and can inform the capability of the model in distinguishing between classes.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$TPR = \\frac{TP}{TP+FN}$\u003c/li\u003e\n\u003cli\u003e$FPR = \\frac{FP}{FP+TN}$\u003c/li\u003e\n\u003cli\u003eA good model has AUC close to 1, a very poor model has AUC close to 0, if the AUC = 0.5, it means there is no class separation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-2ea4818fbebd91c5001d9ebf2ca346e6.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"missing-values\"\u003eMissing Values\u003c/h2\u003e\n\u003cp\u003eOften times data may be incomplete / missing labels.\nTo handle this, there are several strategies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDelete samples with missing values\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003ePros:\nA robust and probably more accurate model.\u003c/li\u003e\n\u003cli\u003eCons:\nLoss of information and data.\nBecomes a poorly trained model if percentage of missing values is high.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReplace missing value with mean/median/mode\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003ePros:\nWhen data size is small, it is better than deleting and can prevent data loss.\u003c/li\u003e\n\u003cli\u003eCons:\nInputting the approximations add bias to the model (it reduces the variance).\nAlso works poorly compared to other models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIf categorical, assigning a unique category or the most frequent category\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003ePros:\nWorks well with small datasets and easy to implement and results in no loss of data.\u003c/li\u003e\n\u003cli\u003eCons:\nUnique category works only for categorical features.\nAdding another feature (e.g., a new unique category) to the model may result in high variance in the model.\nAdding the most frequent category can increase the bias in the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePredicting the missing values\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003ePros:\nInputting the missing variable is an improvement as long as the bias from it is smaller than the omitted variable bias.\nAlso yields unbiased estimates of the model parameters.\u003c/li\u003e\n\u003cli\u003eCons:\nBias also arises when an incomplete conditioning set is used for a categorical variable.\nConsidered only as a proxy for the true values.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUsing algorithms that support missing values\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003ePros:\nDoes not require creation of a predictive model however, correlation of the data is neglected.\u003c/li\u003e\n\u003cli\u003eCons:\nSome of these algorithms are very time-consuming and it can be critical in data mining where large databases are being extracted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"nearest-neighbour\"\u003eNearest Neighbour\u003c/h2\u003e\n\u003cp\u003eNearest Neighbour is a regression or classification algorithm that predicts whatever is the output value of the nearest data point to some query.\u003c/p\u003e\n\u003cp\u003eTo find the nearest data point, we have to find the distance between the query and other points. So we have to decide how to define the distance.\u003c/p\u003e\n\u003ch3 id=\"minkowski-distance\"\u003eMinkowski Distance\u003c/h3\u003e\n\u003cp\u003eIf $\\mathcal{X} \\rightarrow \\mathbb{R}^d$, $x, in \\in \\mathcal{X}$, the Minkowski distance of order $p \u0026gt; 0$ is defined as:\u003c/p\u003e\n\u003cp\u003e$$Dist_p(x,y) = \\left( \\sum_{j=1}^d |x_j - y_j|^P \\right)^{\\frac{1}{p}} = ||x-y||_p$$\u003c/p\u003e\n\u003cp\u003eWhere $||z||_p = \\left( \\sum_{j=1}^d |z_j|^p \\right)^{\\frac{1}{p}}$ is the p-norm (sometimes denoted $L_p$ norm) of the vector $z$.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe 2-norm refers to the euclidean distance\u003c/li\u003e\n\u003cli\u003eThe 1-norm denotes manhattan distance, also called cityblock distance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf we let $p$ grow larger, the distance will be more dominated by the largest coordinate-wise distance, from which we can infer that $Dist_\\infty = max_j | x_j - y_j|$; this is also called Chebyshev distance.\u003c/p\u003e\n\u003cp\u003eIf the data is not in $\\mathcal{R}^d$, but we can turn it into Boolean features / character sequences, we can still apply distance measures, i.e.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eData type\u003c/th\u003e\n\u003cth\u003eDistance Measure\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eEqual Length Binary String\u003c/td\u003e\n\u003ctd\u003eHamming Distance\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUnequal Length Non-Binary String\u003c/td\u003e\n\u003ctd\u003eLevenshtein Distance\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch3 id=\"distance-metrics\"\u003eDistance Metrics\u003c/h3\u003e\n\u003cp\u003eGiven an instance space $\\mathcal{X}$, a distance metric is a function $Dist : \\mathcal{X} \\times \\mathcal{X} \\rightarrow [0, \\infty)$ such that for any $x,y,z \\in \\mathcal{X}$\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDistances between a point and itself are zero\u003c/li\u003e\n\u003cli\u003eAll other distances are larger than zero\u003c/li\u003e\n\u003cli\u003eDistances are symmetric\u003c/li\u003e\n\u003cli\u003eDetours can not shorten the distance (triangle inequality)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf the second condition is weakened to a non-strict inequality - i.e. $Dist(x,y) = 0, x \\neq y$, the function $Dist$ is called a pseudo-metric.\u003c/p\u003e\n\u003cp\u003eThe arithmetic mean $\\mu$ of a set of data points $D$ in a Euclidean space is the unique point that minimises the sum of squared Euclidean distances to those data points.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote minimising the sum of squared Euclidean distances of a given set of points is the same as minimising the average squared Euclidean distance\u003c/li\u003e\n\u003cli\u003eIf we drop the squares, the point is known as the geometric median\u003cul\u003e\n\u003cli\u003eFor univariate data, it corresponds to the median / middle value of a set of numbers\u003c/li\u003e\n\u003cli\u003eFor multivariate data, there is no closed-form expression, and needs to be calculated by successive approximation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"nearest-centroid-classifier\"\u003eNearest Centroid Classifier\u003c/h2\u003e\n\u003cp\u003eThis is a classifier based on minimum distance principle, where the class exemplars are just the centroids (or means).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimple and fast\u003c/li\u003e\n\u003cli\u003eWorks well when classes are compact and far from each other\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor complex classes (e.g. Multimodal, non-spherical) may give very poor results\u003c/li\u003e\n\u003cli\u003eCannot handle outliers or noisy data well and cannot handle missing data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"nearest-neighbour-classification\"\u003eNearest Neighbour Classification\u003c/h2\u003e\n\u003cp\u003eThis is related to the simplest form of learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining instances are searched for instance that most closely resembles new or query instances\u003c/li\u003e\n\u003cli\u003eThe instances themselves represent the knowledge\u003c/li\u003e\n\u003cli\u003eCalled: instance based, memory based learning or case based learning; often a form of local learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNearest neighbour:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGiven query instance $x_q$, first locate nearest training example $x_n$, then estimate $\\hat{f}(x_q) \\leftarrow f(x_n)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$k$-Nearest neighbour:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGiven $x_q$ take vote among its $k$ nearest neighbours (if discrete-valued targe function)\u003c/li\u003e\n\u003cli\u003eTake mean of $f$ values of $k$ nearest neighours (if real valued)\n$$\\hat{f}(x_q) \\leftarrow \\frac{\\sum_{j=1}^kf(x_j)}{k}$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCan be very accurate\u003c/li\u003e\n\u003cli\u003eTraining is very fast\u003c/li\u003e\n\u003cli\u003ecan learn complex target functions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSlow at query time: basic algorithm scans entire training data to derive a prediction, and suffers from \u0026quot;curse of dimensionality\u0026quot;\u003c/li\u003e\n\u003cli\u003eAssumes all attributes are equally important, so easily fooled by irrelevant attributes (can be remedied by attribute selection or weights)\u003c/li\u003e\n\u003cli\u003eProblem of noisy instances (can be remedied by removing those from data set, but it\u0026#39;s difficult to determine which are noisy)\u003c/li\u003e\n\u003cli\u003eFinding the optimal $k$ can be challenging\u003cul\u003e\n\u003cli\u003e1NN perfectly separates training data, so low bias but high variance\u003c/li\u003e\n\u003cli\u003eBy increasing $k$, we increase bias and decrease variance\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNeeds homogenous feature type and scale\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that the data needs to be normalized because different attributes may be measured on different scales, i.e.\n$$x\u0026#39; = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\u003c/p\u003e\n\u003cp\u003ewhere $x$ is the actual value of attribute / feature, and $x\u0026#39;$ is the normalized value.\u003c/p\u003e\n\u003cp\u003eNearest Neighbour should be considered when\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstances map to points in $\\mathbb{R}^d$\u003c/li\u003e\n\u003cli\u003eThere are less than 20 attributes per instance (or number of attributes can be reduced)\u003c/li\u003e\n\u003cli\u003eThere is lots of training data\u003c/li\u003e\n\u003cli\u003eNo requirement for \u0026quot;explanatory\u0026quot; model to be learned\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"distance-weighted-knn\"\u003eDistance-Weighted KNN\u003c/h2\u003e\n\u003cp\u003eWe might want to use the distance function to construct a weight $w_i$.\nThe final line of the classification algorithm can be changed to be\u003c/p\u003e\n\u003cp\u003e$$\\hat{f}(x_q) \\leftarrow \\arg \\max_{v \\in V} \\sum_{i=1}^k w_i \\delta(v,(f(x_j)))$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cp\u003e$$w_i = \\frac{1}{Dist(x_q, x_i)^2}$$\u003c/p\u003e\n\u003cp\u003eFor real-valued target functions, the final line can be changed to be\u003c/p\u003e\n\u003cp\u003e$$\\hat{f}(x_q) \\leftarrow \\frac{\\sum_{i=1}^k w_i f(x_i)}{\\sum_{i=1}^k w_i}$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote the denominator normalizes the contribution of individual weights\u003c/li\u003e\n\u003cli\u003eNow we can consider using all the training examples.\u003cul\u003e\n\u003cli\u003eUsing all examples (i.e. when $k = m$) with the rule above is called Shepard\u0026#39;s method\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLazy learners like this do not construct a model, i.e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1-NN: training set error is always 0\u003c/li\u003e\n\u003cli\u003e$k$-NN: overfitting may be hard to detect\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe solution is to use Leave-one-out cross-validation (LOOCV), ie. leave out one example and predict it given the rest.\u003c/p\u003e\n\u003ch3 id=\"curse-of-dimensionality\"\u003eCurse of Dimensionality\u003c/h3\u003e\n\u003cp\u003eAs dimension increases, the effectiveness of distance metrics decrease, and the concept of proximity may not be qualitatively meaningful as all points look equidistant.\u003c/p\u003e\n\u003cp\u003eSome other problems include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt becomes polynomially harder to estimate many parameters (e.g. covariances)\u003c/li\u003e\n\u003cli\u003eIt becomes more difficult to visualize data\u003c/li\u003e\n\u003cli\u003eEnormous amount of data is needed to train a model\u003c/li\u003e\n\u003cli\u003eNumber of \u0026quot;cells\u0026quot; / data points in the instance space grows exponentially in the number of features\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOne approach to overcome the curse of dimensionality:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStretch $j^{th}$ axis by weight $z_j$, where $z_1, ..., z_d$ is chosen to minimize prediction error\u003c/li\u003e\n\u003cli\u003eUse cross-validation to automatically choose weights $z_1, ..., z_d$\u003c/li\u003e\n\u003cli\u003eNote setting $z_j$ to zero eliminates this dimension altogether\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"inductive-bias\"\u003eInductive Bias\u003c/h2\u003e\n\u003cp\u003eInductive Bias is the combination of assumptions and restrictions placed on the models and algorithms used to solve a learning problem.\nIt means the algorithm and model combination you are using to solve the learning problem is appropriate for the task.\u003c/p\u003e\n\u003ch2 id=\"bayesian-methods\"\u003eBayesian Methods\u003c/h2\u003e\n\u003cp\u003eProvides practical learning algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNaive Bayes classifier learning\u003c/li\u003e\n\u003cli\u003eBayesian networking learning, etc\u003c/li\u003e\n\u003cli\u003eCombines prior knowledge (prior probabilities) with observed data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eProvides useful conceptual framework\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProvides a \u0026quot;gold standard\u0026quot; for evaluating other learning algorithms\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBayes theorem is stated as\u003c/p\u003e\n\u003cp\u003e$$P(h|D) = \\frac{P(D|h)P(h)}{P(D)}$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$P(h)$ = prior probability of hypothesis $h$\u003c/li\u003e\n\u003cli\u003e$P(D)$ = prior probability of training data $D$\u003c/li\u003e\n\u003cli\u003e$P(h|D)$ = probability of $h$ given $D$\u003c/li\u003e\n\u003cli\u003e$P(D|h)$ = probability of $D$ given $h$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"choosing-hypotheses\"\u003eChoosing Hypotheses\u003c/h3\u003e\n\u003cp\u003eIf the output belongs to a set of $k$ classes: $y \\in \\{ C_1, C_2, ..., C_k \\}$ for $1 \\leq i \\leq k$.\u003c/p\u003e\n\u003cp\u003eThen in Bayesian framework:\u003c/p\u003e\n\u003cp\u003e$$P(y = C_i | x) = \\frac{P(x|C_i)P(C_i)}{P(x)}$$\u003c/p\u003e\n\u003cp\u003ewhere $P(x) = \\sum_i P(x|C_i)(P(C_i)$\u003c/p\u003e\n\u003cp\u003eThe decision rule is to select a class which maximises the posterior probability for the prediction.\u003c/p\u003e\n\u003cp\u003eGenerally we want the most probable hypothesis given the training data, Maximum a posteriori hypothesis $h_{MAP}$:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nh_{MAP}\n    \u0026amp;= \\arg \\max_{h \\in H} P(h|D) \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} \\frac{P(D|h)P(h)}{P(D)} \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} P(D|h)P(h)\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eTo get the posterior probability of a hypothesis $h$\u003c/p\u003e\n\u003cp\u003eDivide $P$ ($\\oplus$) (probability of data) to normalize result for $h$:\u003c/p\u003e\n\u003cp\u003e$$P(h|D) = \\frac{P(D|h) P(h)}{\\sum_{h_i \\in H}P(D|h_i)P(h_i)}$$\u003c/p\u003e\n\u003cp\u003eDenominator ensures we obtain posterior probabilities that sum to 1.\nSum for all possible numerator values, since hypotheses are mutually exclusive.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProduct Rule: probabiltiy $P(A \\land B)$ of conjunction of two events $A$ and $B$:\n$$P(A \\land B) = P(A|B)P(B) = P(B|A)P(A)$$\u003c/li\u003e\n\u003cli\u003eSum rule: probability of disjunction of two events $A$ and $B$:\n$$P(A \\lor B) = P(A) + P(B) - P(A \\land B)$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTheorem of total probability: if events $A_1, ..., A_n$ are mutually exclusive with $\\sum_{i=1}^n P(A_i) = 1$, then:\u003c/p\u003e\n\u003cp\u003e$$P(B) = \\sum_{i=1}^n P(B|A_i)P(A_i)$$\u003c/p\u003e\n\u003ch3 id=\"bayesian-expected-loss\"\u003eBayesian Expected Loss\u003c/h3\u003e\n\u003cp\u003eSo far, we decide $h_1$ if $P(h_1|D) \u0026gt; P(h_2|D)$ else $h_2$.\nAlternatively, we can use a loss function $L(h)$, where $L(h)$ is the loss that occurs when decision $h$ is made.\u003c/p\u003e\n\u003cp\u003eFor example, if the cost of misclassifying a patient who has cancer as \u0026quot;not cancer\u0026quot; is 10 times more than classifying a patient who doesn\u0026#39;t have cancer as \u0026quot;cancer\u0026quot;, who will that affect our decision?\u003c/p\u003e\n\u003cp\u003eIf the cost of misclassification is not the same for different classes, then instead of maximizing a posteriori, we have to minimize the expected loss:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSo, if we define the loss associated to action $\\alpha_i$ as $\\lambda(\\alpha_i|h)$\u003c/li\u003e\n\u003cli\u003eThen the expected loss associated to action $\\alpha_i$ si:\n$$E[L(\\alpha_i)] = R(\\alpha_i | x) = \\sum_{h \\in H} \\lambda (a_i|h) P(h|x)$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn optimal Bayesian decision strategy is to minimize the expected loss.\u003c/p\u003e\n\u003ch2 id=\"learning-a-real-valued-function\"\u003eLearning a real valued function\u003c/h2\u003e\n\u003cp\u003eConsider any real-valued target function $f$.\nTraining examples $\\langle x_i, y_i \\rangle$ where $y_i$ is noisy training value\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$y_i = \\hat{f}(x_i) + \\epsilon_i$\u003c/li\u003e\n\u003cli\u003e$\\epsilon_i$ is random variable (noise) drawn independently fore ach $x_i$ according to some Gaussian (normal) distribution with mean zero\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen the maximum likelihood hypothesis $H_{ML}$ is the one that minimizes the sum of squared errors\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nh_{ML}\n    \u0026amp;= \\arg \\max_{h \\in H} P(D|h) \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} \\prod_{i=1}^m P(y_i|\\hat{f}) \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} \\prod_{i=1}^m \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{1}{2} \\left(\\frac{y_i - \\hat{f}(x_i)}{\\sigma} \\right)^2 \\right)\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $\\hat{f} = h_{ML}$.\u003c/p\u003e\n\u003cp\u003eWe can maximise the natural log to give a simpler expression:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nh_{ML}\n    \u0026amp;= \\arg \\max_{h \\in H} \\sum_{i=1}^m \\ln \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{1}{2} \\left( \\frac{y_i - \\hat{f}(x_i)}{\\sigma} \\right)^2 \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} \\sum_{i=1}^m - \\frac{1}{2} \\left( \\frac{y_i - \\hat{f}(x_i)}{\\sigma} \\right)^2 \\\\\n    \u0026amp;= \\arg \\max_{h \\in H} \\sum_{i=1}^m - \\left(y_i - \\hat{f}(x_i)\\right)^2 \\\\\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eEquivalently, we can minimise the positive version of the expression:\u003c/p\u003e\n\u003cp\u003e$$h_{ML} = \\arg \\min_{h \\in H} \\sum_{i=1}^m \\left( y_i - \\hat{f}(x_i) \\right)^2$$\u003c/p\u003e\n\u003ch2 id=\"discriminative-vs-generative-probabilistic-models\"\u003eDiscriminative vs Generative Probabilistic Models\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDiscriminative models\u003c/strong\u003e model the posterior probability distribution $P(y|x)$.\nThat is, given $x$ they return a probability distribution over $y$.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGenerative models\u003c/strong\u003e model the joint distribution $P(y,x)$.\nOnce we have the joint distribution, we can derive any conditional or marginal distribution involving the same variables.\u003c/p\u003e\n\u003cp\u003eSuch models are called \u0026#39;generative\u0026#39; because we can sample from the joint distribution to obtain new data points together with their labels.\u003c/p\u003e\n\u003ch2 id=\"bayesian-optimal-classifier\"\u003eBayesian Optimal Classifier\u003c/h2\u003e\n\u003cp\u003eIn Bayesian optimal classification, the most probable classification\nis obtained by combining the predictions of ALL hypotheses, weighted by their posterior probabilities:\u003c/p\u003e\n\u003cp\u003e$$\\arg \\max_{v_j \\in V} \\sum_{h_i \\in H} P(v_j|h_i)P(h_i|D)$$\u003c/p\u003e\n\u003cp\u003ewhere $v_j$ is a class value.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMathematically shown no other classification method using the same hypothesis space and prior knowledge outperforms the Bayes Optimal Classifier method on average.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eBayes rule depends on unknown quantities so we need to use the data to find some approximation of those quantities\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIs very inefficient.\nAn alternative is the \u003cstrong\u003eGibbs algorithm\u003c/strong\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChoose one hypothesis at random, according to $P(h|D)$\u003c/li\u003e\n\u003cli\u003eUse this to classify new instance\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAssuming target concepts are drawn at random from $H$\n$$E[error_{Gibbs}] \\leq 2 \\times E[error_{BayesOptimal}]$$\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"naive-bayes-classifier\"\u003eNaive Bayes Classifier\u003c/h2\u003e\n\u003cp\u003eWhen to use\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eModerate or large training set available\u003c/li\u003e\n\u003cli\u003eAttributes that describe instances are conditionally independent given classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSuccessful applications:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClassifying text documents\u003c/li\u003e\n\u003cli\u003eGaussian Naive Bayes for real-valued data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAssume target function $f : X \\rightarrow V$, where each instance $x$ described by attributes $(x_1, x_2, ..., x_n)$.\u003c/p\u003e\n\u003cp\u003eThe most probable value of $f(x)$ is:\u003c/p\u003e\n\u003cp\u003e$$v_{MAP} = \\arg \\max_{v_j \\in V} P(x_1, x_2, ..., x_n|v_j)P(v_j)$$\u003c/p\u003e\n\u003cp\u003eNaive Bayes assumes attributes are statistically independent i.e.\u003c/p\u003e\n\u003cp\u003e$$P(x_1, x_2, ..., x_n|v_j) = \\prod_i P(x_i|v_j)$$\u003c/p\u003e\n\u003cp\u003eWhich gives \u003cstrong\u003eNaive Bayes classifier\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$$v_{NB} = \\arg \\max_{v_j \\in V} P(v_j) \\prod_i P(x_i|v_j)$$\u003c/p\u003e\n\u003cp\u003ePsuedocode:\u003c/p\u003e\n\u003cp\u003e$\\text{for each target value }v_j:$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat{P}(v_j) \\leftarrow \\text{estimate}P(v_j)$\u003c/li\u003e\n\u003cli\u003e$\\text{for each attribute value } x_i:$\u003cul\u003e\n\u003cli\u003e$\\hat{P}(x_i|v_j) \\leftarrow$ $\\text{ estimate }$ $P(x_i|v_j)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf none of the training instances with target value $v_j$ have attribute $x_i$, then:\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{P}(x_i|v_j) = 0 \\\\\n\\therefore \\hat{P}(v_j) \\prod_i \\hat{P}(x_i|v_j) = 0\n$$\u003c/p\u003e\n\u003cp\u003ePseudo-counts add 1 to each count (a version of the Laplace Estimator).\u003c/p\u003e\n\u003ch3 id=\"naive-bayes-numeric-attributes\"\u003eNaive Bayes: numeric attributes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eUsual assumption: attributes have a normal or Gaussian probability distribution (given the class)\n$$x|v_j \\sim N(\\mu,\\sigma^2)$$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ethe probability density function for the normal distribution is defined by two parameters\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe sample mean $\\mu$:\n$$\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$$\u003c/li\u003e\n\u003cli\u003eThe standard deviation $\\sigma$:\n$$\\sigma = \\frac{1}{n-1} \\sum_{i=1}^n(x_i - \\mu)^2$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis gives the density function $f(x)$:\u003c/p\u003e\n\u003cp\u003e$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)$$\u003c/p\u003e\n\u003ch2 id=\"categorical-random-variables\"\u003eCategorical Random Variables\u003c/h2\u003e\n\u003cp\u003eCategorical variables appear often in ML, i.e. text classification\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe most common form is the Bernoulli distribution model; whether or not a word occurs in a document.\u003cul\u003e\n\u003cli\u003eFor the $i^{th}$ word, we have a random variable $X_i$ governed by a Bernoulli distribution.\u003c/li\u003e\n\u003cli\u003eThe joint distribution over the bit vector $X = (X_1, ..., X_k)$ is called a \u003cstrong\u003emultivariate Bernoulli distribution\u003c/strong\u003e which shows whether a word occurs or not\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVariables with more than two outcomes are also common.\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003emultinomial distribution\u003c/strong\u003e manifests itself as a count vector: a histogram of the number of occurrences of all vocabulary words\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasy and fast at prediction. Also does well in multi class prediction.\u003c/li\u003e\n\u003cli\u003eWhen assumption of independence holds, performs better than other models like logistic regression with less training data.\u003c/li\u003e\n\u003cli\u003ePerforms well in case of categorical input variables compared to numerical.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf zero-frequency case happens, it is unable to make a prediction (solution: use the smoothing technique).\u003c/li\u003e\n\u003cli\u003eOn the other side Naive Bayes is known as a bad estimator, so the probability outputs cannot be taken too seriously.\u003c/li\u003e\n\u003cli\u003eStrong assumption of independent attributes. In real life, it is almost impossible we get a set of attributes which are completely independent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"logistic-regression\"\u003eLogistic Regression\u003c/h2\u003e\n\u003cp\u003eLogistic regression can be used for binary classification, where we transform the $y$ values into probability values (in the range $[0,1]$).\u003c/p\u003e\n\u003cp\u003eWe can model this with a sigmoid curve.\u003c/p\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-1dd589ccab73f60638a2ce0e9a3a1db6.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cp\u003eNow $f(x)$ can have a value inbetween $-\\infty$ and $+\\infty$ and in Logistic Regression we estimate $f(x)$ with a line.\u003c/p\u003e\n\u003cp\u003e$$\\hat{f}(x) = x^T\\beta \\Rightarrow \\log \\frac{P(y=1|x)}{1-P(y=1|x)}$$\u003c/p\u003e\n\u003cp\u003eLogistic regression seeks to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eModel the probability of a class given the values of independent input variables\u003c/li\u003e\n\u003cli\u003eEstimate the probability that a class occurs for a random observation\u003c/li\u003e\n\u003cli\u003eClassify an observation based on the probability estimations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\\hat{P}(y=1|x) = \\frac{1}{1+e^{-x^T\\beta}}$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIf $P(y=1|x) \\geq 0.5$ (same as saying $x^T\\beta \\geq 0$) then predict as class 1\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf $P(y=1|x) \u0026lt; 0.5$ (same as saying $x^T\\beta \u0026lt; 0$) then predict as class 0\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis is equivalent of having a linear decision boundary separating the two classes (because we have a linear solution to our problem, this is what makes Logistic Regression a linear model)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"logistic-regression-parameter-estimation\"\u003eLogistic Regression Parameter Estimation\u003c/h3\u003e\n\u003cp\u003eCannot use cost function in Linear Regression because it will result in a non-convex function with many local minimums.\nInstead, the following cost function is used:\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s define $\\hat{P}(y=1|x) = h_\\beta(x)$\u003c/p\u003e\n\u003cp\u003e$$\ncost \\left( h_\\beta(x),y \\right) =\n\\begin{cases}\n  -\\log \\left( h_\\beta(x) \\right) \u0026amp; \\text{if $y = 1$} \\\\\n  -\\log \\left( 1-h_\\beta(x) \\right) \u0026amp; \\text{if $y = 0$}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003e$$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log \\left( h_\\beta \\left( x^{(i)} \\right) \\right) + \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\beta \\left( x^{(i)} \\right) \\right) \\right]$$\u003c/p\u003e\n\u003cp\u003eThe values of the parameters that minimise $J(\\beta)$ can be found using gradient descent.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRelatively easy to implement and interpret and relatively fast at training and testing\u003c/li\u003e\n\u003cli\u003eCan easily extend to multi-classes\u003c/li\u003e\n\u003cli\u003eProvides probabilistic predictions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProne to overfitting in high-dimensional data (one remedy: regularization)\u003c/li\u003e\n\u003cli\u003eProvides linear decision boundary\u003c/li\u003e\n\u003cli\u003eRequires moderate or no correlation (collinearity) between input variables and may lead to poor model, and also sensitive to outliers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"tree-learning\"\u003e\u003ca href=\"#tree-learning\"\u003eTree Learning\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eProbably the single most popular tool\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasy to understand, implement and use\u003c/li\u003e\n\u003cli\u003eComputationally cheap (efficient, even on big data)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDecision tree representation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach internal node tests an attribute\u003c/li\u003e\n\u003cli\u003eEach branch corresponds to attribute value\u003c/li\u003e\n\u003cli\u003eEach leaf node assigns a classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"decision-tree-expressiveness\"\u003eDecision Tree: Expressiveness\u003c/h2\u003e\n\u003cp\u003eDecision Trees can be used to represent Boolean functions like AND ($\\land$), OR ($\\lor$), XOR ($\\oplus$)\u003c/p\u003e\n\u003cp\u003e$$X \\land Y$$\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003eif X == True:\n    if Y == True: return True\n    if Y == False: return False\nif X == False:\n    return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e$$X \\lor Y$$\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003eif X == True: return True\nif X == False:\n    if Y == True: return True\n    if Y == False: return True\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e$$X \\oplus Y$$\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003eif X == True:\n    if Y == True: return False\n    if Y == False: return True\nif X == False:\n    if Y == True: return True\n    if Y == False: return False\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn general, decisions trees represent a \u003cstrong\u003edisjunction of conjunctions\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen to use decision trees:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstance described by a mix of numeric features and discrete attribute value pairs\u003c/li\u003e\n\u003cli\u003eTarget function is discrete valued (otherwise use regression trees)\u003c/li\u003e\n\u003cli\u003ePossibly noisy training data\u003c/li\u003e\n\u003cli\u003eInterpretability is an advantage\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe main loop for top-down induction of decision trees (TDIDT)\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e$A \\leftarrow$ the \u0026quot;best\u0026quot; decision attribute for the next node to split examples\u003c/li\u003e\n\u003cli\u003eAssign $A$ as decision attribute for node\u003c/li\u003e\n\u003cli\u003eFor each value of $A$, create new descendant of node (child node)\u003c/li\u003e\n\u003cli\u003eSplit training examples to child nodes\u003c/li\u003e\n\u003cli\u003eIf training examples perfectly classified (pure subset), then STOP ,else iterate over new child nodes\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"entropy\"\u003eEntropy\u003c/h2\u003e\n\u003cp\u003eIf we want to determine yes or no, is \u003ccode\u003eoutlook\u003c/code\u003e or \u003ccode\u003ewind\u003c/code\u003e a better attribute\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-py\"\u003edef use_outlook(outlook: attribute) -\u0026gt; dict:\n    match outlook:\n        case sunny: return {\u0026quot;yes\u0026quot;: 2, \u0026quot;no\u0026quot;: 3}\n        case overcast: return {\u0026quot;yes\u0026quot;: 4, \u0026quot;no\u0026quot;: 0}\n        case rain: return {\u0026quot;yes\u0026quot;: 3, \u0026quot;no\u0026quot;: 2}\n\ndef use_wind(wind: attribute) -\u0026gt; dict:\n    match wind:\n        weak: return {\u0026quot;yes\u0026quot;: 6, \u0026quot;no\u0026quot;: 2}\n        strong: return {\u0026quot;yes\u0026quot;: 3, \u0026quot;no\u0026quot;: 3}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eWe are using a split with higher \u0026quot;purity\u0026quot;\u003c/li\u003e\n\u003cli\u003eWe need to measure the purity of the split\u003cul\u003e\n\u003cli\u003eMore certain about our classes after a split\u003cul\u003e\n\u003cli\u003eA set with all examples belonging to one class is 100% pure\u003c/li\u003e\n\u003cli\u003eA set with 50% examples in one class and 50% in the other is 100% uncertain and impure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFrom a statistical point of view\u003c/li\u003e\n\u003cli\u003eFrom information theory point of view: The amount of information (in the Shannon sense) needed to specify the full state of a system\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEntropy measures the \u0026quot;impurity\u0026quot; of $S$.\u003c/p\u003e\n\u003cp\u003e$$Entropy(S) = H(S) = -p_\\oplus \\log_2 p_\\oplus - p_\\ominus \\log_2 p_\\ominus$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$S$ is subset of training examples\u003c/li\u003e\n\u003cli\u003e$p_\\oplus$, $p_\\ominus$ are the portion (%) of positive and negative examples in $S$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInterpretation: if item $x$ belongs to $S$, how many bits are needed to tell if $x$ is positive or negative?\u003c/p\u003e\n\u003cdiv class=\"tikz-diagram\"\u003e\n  \u003cimg src=\"/tikz-cache/tikz-d9df73e9a719e07e339e88aaf2a059ba.svg\" alt=\"TikZ Diagram\" class=\"tikz-svg\" /\u003e\n\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u0026quot;High Entropy\u0026quot; / \u0026quot;impure set\u0026quot; means $X$ is very uniform and boring\u003cul\u003e\n\u003cli\u003eE.g. (3 samples from $\\oplus$, 3 samples from $\\ominus$)\u003c/li\u003e\n\u003cli\u003e$H(S) = - \\frac{3}{6}\\log_2\\frac{3}{6} - \\frac{3}{6}\\log_2\\frac{3}{6} = 1$ (can be interpreted as 1 bits)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u0026quot;Low Entropy\u0026quot; / \u0026quot;pure set\u0026quot; means $X$ is not uniform and interesting\u003cul\u003e\n\u003cli\u003eE.g. (6 samples from $\\oplus$, 0 samples from $\\ominus$)\u003c/li\u003e\n\u003cli\u003e$H(S) = - \\frac{6}{6}\\log_2\\frac{6}{6} - \\frac{0}{6}\\log_2 \\frac{0}{6} = 0$ (can be interpreted as 0 bits)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"information-gain\"\u003eInformation Gain\u003c/h2\u003e\n\u003cp\u003e$Gain(S,A)$ is the expected reduction in entropy due to sorting on $A$\u003c/p\u003e\n\u003cp\u003e$$Gain(S,A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$v$ is the possible values of attribute $A$\u003c/li\u003e\n\u003cli\u003e$S$ is the set of examples we want to split\u003c/li\u003e\n\u003cli\u003e$S_v$ is the subset of examples where $X_A = v$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe want to find the attribution which maximizes the gain (this is also called \u0026quot;mutual information\u0026quot; between attribute $A$ and class labels of $S$).\u003c/p\u003e\n\u003cp\u003eTo select the best attribute at each branch:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etake every/remaining attributes in your data\u003c/li\u003e\n\u003cli\u003eCompute information gain for that attribute\u003c/li\u003e\n\u003cli\u003eSelect the attribute that has the highest information gain\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLimitations\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInformation gain is more biased towards attributes with large numbers of values / categories\u003cul\u003e\n\u003cli\u003eSubsets are more likely to be pure if there is a large number of values\u003c/li\u003e\n\u003cli\u003eThis can result in overfitting which doe snot generalize well to unseen data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSuggested solution: gain ratio\u003cul\u003e\n\u003cli\u003eA modification of information gain that reduces the bias\u003c/li\u003e\n\u003cli\u003eTakes number and size of branches into account\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGain Ratio\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$$SplitEntropy(S,A) = - \\sum_{v \\in Values(A)} \\frac{S_v}{S} \\log_2 \\frac{S_v}{S}$$\nWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A$: candidate attribute\u003c/li\u003e\n\u003cli\u003e$v$: possible values of $A$\u003c/li\u003e\n\u003cli\u003e$S$: Set of examples ($X$) at the node\u003c/li\u003e\n\u003cli\u003e$S_v$: subset where $X_A = v$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$GainRatio(S,A) = \\frac{Gain(S,A)}{SplitEntropy(S,A)}$$\u003c/p\u003e\n\u003ch2 id=\"overfitting-in-decision-trees\"\u003eOverfitting in Decision Trees\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCan always classify training examples perfectly\u003cul\u003e\n\u003cli\u003eIf necessary, keep splitting until each node contains 1 example\u003c/li\u003e\n\u003cli\u003eSingleton subset (leaf nodes with one example) which is by definition pure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBut this is not always ideal because on leaf nodes with singleton subsets, you have no confidence in your decision\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eConsider error of hypothesis $h$ over\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining data: $error_{train}(h)$\u003c/li\u003e\n\u003cli\u003eEntire distribution $D$ of data $error_{D}(h)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition\u003c/strong\u003e\nHypothesis $h \\in H$ overfits training data if there is an alternative hypothesis $h\u0026#39; \\in H$ such that\u003c/p\u003e\n\u003cp\u003e$$error_{train}(h) \u0026lt; error_{train}(h\u0026#39;) \\land error_D(h) \u0026gt; error_D(h\u0026#39;)$$\u003c/p\u003e\n\u003cp\u003eTo avoid overfitting in decision trees, we can use \u003cstrong\u003epruning\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003epre-pruning:\u003c/strong\u003e Stop growing when data split not statistically significant.\u003cul\u003e\n\u003cli\u003eSome stopping conditions:\u003cul\u003e\n\u003cli\u003eLower than some lower-bound on the number of examples in a leaf\u003c/li\u003e\n\u003cli\u003eStop when Entropy changes is smaller than a lower-bound\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epost-pruning:\u003c/strong\u003e Grow full tree, then remove sub-trees which are overfitting (based on validation set). This avoids the problem of \u0026quot;early stopping\u0026quot;\u003cul\u003e\n\u003cli\u003eMethods of finding subtrees:\u003cul\u003e\n\u003cli\u003eSplit data into training and validation set, and prune subtrees until further pruning is harmful\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasy to interpret\u003c/li\u003e\n\u003cli\u003eCan handle irrelevant attributes (Gain = 0)\u003c/li\u003e\n\u003cli\u003eCan handle categorical and numerical data, along with missing data\u003c/li\u003e\n\u003cli\u003eCan handle missing data\u003c/li\u003e\n\u003cli\u003eVery compact (number of nodes \u0026lt;\u0026lt; number of examples)\u003c/li\u003e\n\u003cli\u003eVery fast at testing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOnly axis-aligned splits of the data\u003c/li\u003e\n\u003cli\u003eTend to overfit\u003c/li\u003e\n\u003cli\u003eGreedy (may not find the best tree)\u003cul\u003e\n\u003cli\u003eExponentially many possible trees\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"regression-tree\"\u003eRegression Tree\u003c/h2\u003e\n\u003cp\u003eMention things here\u003c/p\u003e\n\u003ch1 id=\"kernel-methods\"\u003e\u003ca href=\"#kernel-methods\"\u003eKernel Methods\u003c/a\u003e\u003c/h1\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eTask\u003c/th\u003e\n\u003cth\u003eLabel and Output Space\u003c/th\u003e\n\u003cth\u003eLearning Problem\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eClassification\u003c/td\u003e\n\u003ctd\u003e$\\mathcal{L} = \\mathcal{C}$, $\\mathcal{Y} = \\mathcal{C}$\u003c/td\u003e\n\u003ctd\u003eLearn an approximation $\\hat{c} : \\mathcal{X} \\rightarrow \\mathcal{C}$ to the true labelling function $c$\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eScoring and ranking\u003c/td\u003e\n\u003ctd\u003e$\\mathcal{L} = \\mathcal{C}$, $\\mathcal{Y} = \\mathbb{R}^{|\\mathcal{C}|}$\u003c/td\u003e\n\u003ctd\u003eLearn a model that outputs a score vector over classes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eProbability estimation\u003c/td\u003e\n\u003ctd\u003e$\\mathcal{L} = \\mathcal{C}$, $\\mathcal{Y} = [0,1]^{|\\mathcal{C}|}$\u003c/td\u003e\n\u003ctd\u003eLearn a model that outputs a probability vector over classes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRegression\u003c/td\u003e\n\u003ctd\u003e$\\mathcal{L} = \\mathbb{R}$, $\\mathcal{Y} = \\mathbb{R}$\u003c/td\u003e\n\u003ctd\u003eLearn an approximation $\\hat{f} : \\mathcal{X} \\rightarrow \\mathbb{R}$ to the true labelling function $f$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch2 id=\"perceptron\"\u003ePerceptron\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePerceptron\u003c/strong\u003e is an algorithm for binary classification that uses a linear prediction function.\nNote the perceptron predicts a binary class label, but linear regression predicts a real value.\u003c/p\u003e\n\u003cp\u003eFor a general case with $n$ attributes\u003c/p\u003e\n\u003cp\u003e$$\nf(x) =\n\\begin{cases}\n    +1 \u0026amp; \\text{if $w_0 + w_1x_1 + ... + w_nx_n \u0026gt; 0$} \\\\\n    -1 \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eIf we add $x_0 = 1$ to the feature vector:\u003c/p\u003e\n\u003cp\u003e$$\nf(x) =\n\\begin{cases}\n    +1 \u0026amp; \\text{if $\\sum_{i=0}^n w_ix_i \u0026gt; 0$} \\\\\n    -1 \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = f(x) = sign(w \\cdot x)$$\u003c/p\u003e\n\u003cp\u003ewhere $sign$ is the sign function.\nNow to find a good set of weights using our training set.\u003c/p\u003e\n\u003cp\u003eThe perceptron algorithm initializes all weights $w_i$ to zero, and learns the weights using the following update rule:\u003c/p\u003e\n\u003cp\u003e$$w := w + \\frac{1}{2} \\left( y_j - f(x_j) \\right)x_j$$\u003c/p\u003e\n\u003cp\u003eThere are 4 cases:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e$y=+1, f(x)=+1 \\Rightarrow (y-f(x))=0$\u003c/li\u003e\n\u003cli\u003e$y=+1, f(x)=-1 \\Rightarrow (y-f(x))=+2$\u003c/li\u003e\n\u003cli\u003e$y=-1, f(x)=+1 \\Rightarrow (y-f(x))=-2$\u003c/li\u003e\n\u003cli\u003e$y=-1, f(x)=-1 \\Rightarrow (y-f(x))=0$\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"perceptron-training-algorithm\"\u003ePerceptron training algorithm\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e Perceptron($D$) / perceptron training for linear classification\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInput\u003c/strong\u003e labelled training data $D$ in homogeneous coordinates\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOutput\u003c/strong\u003e weight vector $w$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$w \\leftarrow 0$\u003c/li\u003e\n\u003cli\u003e$converged \\leftarrow false$\u003c/li\u003e\n\u003cli\u003e$\\textbf{while } coverged = false \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$converged \\leftarrow true$\u003c/li\u003e\n\u003cli\u003e$\\textbf{for } i = 1 ..|D| \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$\\textbf{if } y_iw \\cdot x_i \\leq 0 \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$w \\leftarrow w + y_ix_i$\u003c/li\u003e\n\u003cli\u003e$converged \\leftarrow false$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"extending-linear-classification\"\u003eExtending Linear Classification\u003c/h3\u003e\n\u003cp\u003eLinear classification cannot model nonlinear class boundaries.\nHowever we can map attributes into new space consisting combination of attribute values.\u003c/p\u003e\n\u003cp\u003eE.g. for 2 attributes\u003c/p\u003e\n\u003cp\u003e$$y = w_1x_1^3 + w_2x_1^2x_2 + w_3x_1x_2^2 + w_4x_2^3$$\u003c/p\u003e\n\u003cp\u003e$y$ is predicted output for instances with two attributes $x_1$ and $x_2$.\u003c/p\u003e\n\u003cp\u003eHowever, there are issues with this approach\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEfficiency:\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eWith 10 attributes and polynomial function with order 5, more than 2000 coefficients have to be learned\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOverfitting:\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003e\u0026quot;Too nonlinear\u0026quot; - number of coefficients large relative to number of training instances\u003c/li\u003e\n\u003cli\u003eCurse of dimensionality\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"dual-form\"\u003eDual Form\u003c/h2\u003e\n\u003cp\u003eWith an optimisation problem, we can construct another optimisation problem which is called \u003cstrong\u003edual problem\u003c/strong\u003e and is related to our original problem (\u003cstrong\u003eprimal problem\u003c/strong\u003e).\u003c/p\u003e\n\u003cp\u003eAfter training a perceptron, each example has been misclassified zero or more times.\nDenoting this number as $\\alpha_i$ for example $x_i$, the weight vector for $m$ observations can be expressed as\u003c/p\u003e\n\u003cp\u003e$$w = \\sum_{i=1}^m \\alpha_iy_ix_i$$\u003c/p\u003e\n\u003cp\u003eIn the dual instance-based view, we are learning instance weights $\\alpha_i$ rather than feature weights $w_j$.\nAn instance $x$ is classified as\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = f(x) = sign(w \\cdot x)$$\n$$\\hat{y} = sign \\left( \\sum_{i=1}^m a_iy_i(x_i \\cdot x) \\right)$$\u003c/p\u003e\n\u003ch3 id=\"perceptron-training-algorithm-in-dual-form\"\u003ePerceptron training algorithm in dual form\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e Dual-Perceptron($D$) / perceptron training for linear classification in dual form\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInput\u003c/strong\u003e labelled training data $D$ in homogeneous coordinates\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOutput\u003c/strong\u003e coefficients $\\alpha_i$ defining weight vector $W = \\sum_{i=1}^{|D|} \\alpha_i y_i x_i$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\alpha_i \\leftarrow 0$\u003c/li\u003e\n\u003cli\u003e$converged \\leftarrow false$\u003c/li\u003e\n\u003cli\u003e$\\textbf{while } coverged = false \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$converged \\leftarrow true$\u003c/li\u003e\n\u003cli\u003e$\\textbf{for } i = 1 ..|D| \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$\\textbf{if } y_i \\sum_{j=1}^{|D|} a_j y_j x_j \\cdot x_i\\leq 0 \\textbf{ do}$\u003cul\u003e\n\u003cli\u003e$\\alpha_i \\leftarrow \\alpha_i + 1$\u003c/li\u003e\n\u003cli\u003e$converged \\leftarrow false$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e$\\textbf{end}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"nonlinear-dual-perceptron\"\u003eNonlinear dual perceptron\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eWe can use nonlinear mapping to map attributes into new space consisting of combinations of attribute values.\n$$x \\rightarrow \\varphi(x)$$\u003c/li\u003e\n\u003cli\u003eThe perceptron decision will be:\n$$\\hat{y} = sign \\left( \\sum_{i=1}^m a_j y_i (\\varphi(x_i) \\cdot \\varphi(x))\\right)$$\u003c/li\u003e\n\u003cli\u003eSo the only thing we need is the \u003cstrong\u003edot product in the new feature space\u003c/strong\u003e $(\\varphi(x_i) \\cdot \\varphi(x))$ or $\\langle \\varphi(x_i), \\varphi(x) \\rangle$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet $x = (x_1, x_2)$ and $x\u0026#39; = (x_1\u0026#39;, x_2\u0026#39;)$ be two data points, and consider the following mapping to a three-dimensional feature space:\u003c/p\u003e\n\u003cp\u003e$$(x_1, x_2) \\rightarrow (x_1^2, x_2^2, \\sqrt{2}x_1x_2)$$\u003c/p\u003e\n\u003cp\u003e$$\\text{(original feature space) } \\mathcal{X} \\rightarrow \\mathcal{Z} \\text{ (new feature space)}$$\u003c/p\u003e\n\u003cp\u003eThe points in feature space corresponding to $x$ and $x\u0026#39;$ are\u003c/p\u003e\n\u003cp\u003e$z = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)$ and $z\u0026#39; = (x_1\u0026#39;^2, x_2\u0026#39;^2, \\sqrt{2}x_1\u0026#39;x_2\u0026#39;)$\u003c/p\u003e\n\u003cp\u003eThe dot product of these two feature vectors is\u003c/p\u003e\n\u003cp\u003e$$z \\cdot z\u0026#39; = x_1^2x_1\u0026#39;^2 + x_2^2x_2\u0026#39;^2 + 2x_1x_1\u0026#39;x_2x_2\u0026#39; = (x_1x_1\u0026#39; + x_2x_2\u0026#39;)^2 = (x \\cdot x\u0026#39;)^2$$\u003c/p\u003e\n\u003cp\u003eBy squaring in original space, we obtain the dot product in the new space.\nA function that directly calculates the dot product in the new feature space from vectors in the original space is called a kernel - here the kernel is $K(x_1,x_2) = (x_1 \\cdot x_2)^2$.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003evalid kernel\u003c/strong\u003e function is equivalent to a \u003cstrong\u003edot product in some space\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e$$K(x,x\u0026#39;) = \\varphi(x) \\cdot \\varphi(x\u0026#39;)$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA kernel function is a \u003cstrong\u003esimilarity\u003c/strong\u003e function that corresponds to a dot product in some expanded feature space.\u003c/li\u003e\n\u003cli\u003eSome very useful kernels in machine learning are \u003cstrong\u003epolynomial kernel\u003c/strong\u003e and \u003cstrong\u003eradial basis function kernel (RBF kernel)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ePolynomial kernel is defined as:\n$$K(x,x\u0026#39;) = (x \\cdot x\u0026#39; + c)^q$$\u003c/li\u003e\n\u003cli\u003eRBF kernel is defined as:\n$$K(x,x\u0026#39;) = \\exp \\left( - \\frac{||x-x\u0026#39;||^2}{2 \\sigma^2} \\right)$$\n(Using Taylor expansion, it can be shown that RBF kernel is equivalent of mapping features into infinite dimensions)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing the kernel trick, the nonlinear perceptron can be solved using the dual form\u003c/p\u003e\n\u003cp\u003e$$\\hat{y} = sign\\left( \\sum_{i=1}^m a_i y_i (\\varphi(x_i) \\cdot \\varphi(x)) \\right) = sign\\left( \\sum_{i=1}^m a_i y_i K(x_i, x) \\right)$$\u003c/p\u003e\n\u003ch2 id=\"support-vector-machine\"\u003eSupport Vector Machine\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSupport Vector Machines (SVMs) can find the optimal linear classification by fitting the maximum margin hyperplane that has the greatest separation between classes\u003c/li\u003e\n\u003cli\u003eCan avoid overfitting - learn a form of decision boundary called the maximum margin hyperplane\u003c/li\u003e\n\u003cli\u003eFast for mappings to nonlinear spaces\u003cul\u003e\n\u003cli\u003eemploy a mathematical trick (kernel) to avoid the actual creation of new \u0026quot;pseudo-attributes\u0026quot; in transformed instance spaces\u003c/li\u003e\n\u003cli\u003ei.e. the nonlinear space is created implicitly\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet $x_s$ be the closest point to the separating hyperplane (line in 2D) with the following equation:\u003c/p\u003e\n\u003cp\u003e$$w \\cdot x = t$$\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s have 2 minor technicalities to simply the math later:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003ePull out $w_0 : w = [w_0, ..., w_n]$ and $w_0 = -t$, therefore we will have:\n$$w \\cdot x - t = 0$$\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNormalize $w$:\u003c/p\u003e\n\u003cp\u003eWe know that $|w \\cdot x_s - t| \u0026gt; 0$ and we know that we can sacle $w$ and $t$ together without having any effect on the hyperplane, so we choose the scale such that:\n$$|w \\cdot x_s - t| = 1$$\u003c/p\u003e\n\u003cp\u003eThis means $m = 1$.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e$w$ is perpendicular to the line (hyperplane).\u003c/p\u003e\n\u003cp\u003eFor every two points $x\u0026#39;$ and $x\u0026#39;\u0026#39;$ on the line (hyperplane), we can write:\u003c/p\u003e\n\u003cp\u003e$$\nw \\cdot x\u0026#39; - t = 0 \\text{ and } w \\cdot x\u0026#39;\u0026#39; - t = 0 \\\\\n\\Rightarrow \\quad w \\cdot (x\u0026#39; - x\u0026#39;\u0026#39;) = 0\n$$\u003c/p\u003e\n\u003cp\u003eSince their dot product is 0, it means they are perpendicular.\u003c/p\u003e\n\u003cp\u003eSince, distance between the point $x_s$ and the hyperplane can be found as\u003c/p\u003e\n\u003cp\u003e$$distance = \\frac{|w \\cdot x_s - t|}{||w||}$$\u003c/p\u003e\n\u003cp\u003eAnd we have $|w \\cdot x_s - t|=1$, so:\u003c/p\u003e\n\u003cp\u003e$$distance = \\frac{1}{||w||}$$\u003c/p\u003e\n\u003cp\u003eThis distance is the \u003cstrong\u003emargin\u003c/strong\u003e of our classifier which we want to minimize:\u003c/p\u003e\n\u003cp\u003e$$\\max \\frac{1}{||w||} \\text{ subject to } \\min_{t=1,...,m} |w \\cdot x_i - t| = 1$$\u003c/p\u003e\n\u003cp\u003e(This is not a friendly optimisation as it has \u0026quot;min\u0026quot; in the constraint).\u003c/p\u003e\n\u003cp\u003eWe can transform the maximization problem into the following minimization problem:\u003c/p\u003e\n\u003cp\u003e$$\n\\min_w \\frac{1}{2} ||w||^2 \\text{ subject to } y_i(w \\cdot x_i - t) \\geq 1 \\\\\n\\text{ for } i = 1,...,m, w \\in \\mathbb{R}^n, t \\in \\mathbb{R}\n$$\u003c/p\u003e\n\u003cp\u003eThis can be solved using Lagrangian multipliers\u003c/p\u003e\n\u003ch2 id=\"lagrangian-multipliers\"\u003eLagrangian multipliers\u003c/h2\u003e\n\u003cp\u003eIn Lagrangian form, the optimization problem becomes:\u003c/p\u003e\n\u003cp\u003e$$\\max_{\\alpha_1,...,\\alpha_m} \\min_{x_1,...,x_n} \\mathcal{L}(x_1,...,x_n,\\alpha_1,...,\\alpha_m) \\text{ s.t. } \\alpha_j \\geq 0 \\forall j$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst minimizing with respect to $x_1,...,x_n$\u003c/li\u003e\n\u003cli\u003ethen maximizing with respect to $\\alpha_1,...,\\alpha_m$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdding the constraints with multiplier $\\alpha_i$ for each training example gives the Lagrange function:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\mathcal{w,t,\\alpha_1,...,\\alpha_m}\n  \u0026amp;= \\frac{1}{2} ||w||^2 - \\sum_{i=1}^m \\alpha_i (y_i (w \\cdot x_i - t) - 1) \\\\\n  \u0026amp;= \\frac{1}{2} ||w||^2 - \\sum_{i=1}^m \\alpha_iy_i(w \\cdot x_i) + \\sum_{i=1}^m \\alpha_i y_i t + \\sum_{i=1}^m \\alpha_i \\\\\n  \u0026amp;= \\frac{1}{2} w \\cdot w - w \\cdot \\left( \\sum_{i=1}^m \\alpha_iy_ix_i \\right) + t \\left( \\sum_{i=1}^m \\alpha_i y_i \\right) + \\sum_{i=1}^m \\alpha_i\n\\end{align*}\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst we have to minimize $\\mathcal{L}$ with respect to $w$ and $t$\u003c/li\u003e\n\u003cli\u003eBy taking the partial derivative of the Lagrange function with respect to $t$ and setting it to 0, we find:\n$$\\sum_{i=1}^m\\alpha_iy_i = 0$$\u003c/li\u003e\n\u003cli\u003eSimilarly, by taking the partial derivative of the Lagrange function with respect to $w$ and setting to 0 we obtain:\n$$w = \\sum_{i=1}^m \\alpha_i y_i x_i$$\u003cul\u003e\n\u003cli\u003eThe same expression as we derived for the perceptron\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThese expressions allows us to eliminate $w$ and $t$ and lead to the dual Lagrangian\n$$\n\\begin{align*}\n\\mathcal{L}(\\alpha_1, ..., \\alpha_n)\n  \u0026amp;= - \\frac{1}{2} \\left( \\sum_{i=1}^m \\alpha_i y_i x_i \\right) \\cdot \\left( \\sum_{i=1}^m \\alpha_i y_i x_i \\right) + \\sum_{i=1}^m \\alpha_i \\\\\n  \u0026amp;= - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j + \\sum_{i=1}^m \\alpha_i\n\\end{align*}\n$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dual optimization problem for SVMs is to maximize the dual Lagrangian under positive constraints and one equality constraint:\u003c/p\u003e\n\u003cp\u003e$$\\alpha_i*, ..., \\alpha_m* = \\arg \\max_{\\alpha_1, ..., \\alpha_m} - \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j + \\sum_{i=1}^m \\alpha_i $$\u003c/p\u003e\n\u003cp\u003e$$\\text{subject to } \\alpha_i \\geq 0, 1 \\leq i \\leq m, \\sum_{i=1}^m \\alpha_i y_i = 0$$\u003c/p\u003e\n\u003cp\u003eSolving for $\\alpha_1*, ..., \\alpha_m*$, they will be mostly zero, except for points that are closest to the hyperplane.\nThese points are called the support vectors.\u003c/p\u003e\n\u003cp\u003e$$w = \\sum_{X_i \\in \\{ support : vectors\\}} \\alpha_i y_i x_i$$\u003c/p\u003e\n\u003cp\u003eSolve for $t$ using any of support vectors:\u003c/p\u003e\n\u003cp\u003e$$y_i (w \\cdot x_i - t) = 1$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEffective in high dimensional space and when dimension \u0026gt; num samples\u003c/li\u003e\n\u003cli\u003eMemory efficient\u003c/li\u003e\n\u003cli\u003eWorks very well if classes are separable even if non linearly (through kernels)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDoesn\u0026#39;t scale well to big data (by increasing the number of samples, the number of support vector grows and the prediction will be slow)\u003c/li\u003e\n\u003cli\u003eDoesn\u0026#39;t directly provide probabilistic explanation\u003c/li\u003e\n\u003cli\u003eHard to interpret, especially when using kernels\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"ensemble-learning\"\u003e\u003ca href=\"#ensemble-learning\"\u003eEnsemble Learning\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eEnsemble learning is a form of multi-level learning: learning a number of base-level models from the data, and learning to combine these models as an ensemble.\u003c/p\u003e\n\u003cp\u003eIt can help to reduce \u003cstrong\u003ebias\u003c/strong\u003e (expected error due to mismatch between learner\u0026#39;s hypothesis space and space of target concepts) and \u003cstrong\u003evariance\u003c/strong\u003e (expected error due to differences in the training sets used).\u003c/p\u003e\n\u003cp\u003eThree commonly used methods to find a good bias-variance tradeoff are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRegularization\u003c/li\u003e\n\u003cli\u003eBagging\u003c/li\u003e\n\u003cli\u003eBoosting\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"bias-variance-with-big-data\"\u003eBias-variance with \u0026quot;Big Data\u0026quot;\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHigh bias algorithms are often used for efficiency\u003cul\u003e\n\u003cli\u003eUsually simpler to compute\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBig data can reduce variance\u003cul\u003e\n\u003cli\u003e\u0026quot;small\u0026quot; concepts will occur more frequently\u003c/li\u003e\n\u003cli\u003elow bias algorithms can be applied\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, it is difficult to compute big data efficiently.\u003c/p\u003e\n\u003cp\u003eThe follow scenarios happen in real-world AI\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTraining-set error is observed to be high compared to human-level\u003cul\u003e\n\u003cli\u003eBias is too high - solution: move to a more expressive (lower bias) model\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTraining-set error is observed to be similar to human-level, but validation set error is high compared to human-level\u003cul\u003e\n\u003cli\u003eVariance is too high - solution: get more data, try regularization, ensembles, move to a different model architecture\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"stability\"\u003eStability\u003c/h2\u003e\n\u003cp\u003eFor some given data $\\mathcal{D}$, train an algorithm $L$ on training sets $S_1$, $S_2$ sampled from $\\mathcal{D}$.\nIf the model from $L$ is very similar on both $S_1$ and $S_2$, $L$ is a stable learning algorithm, otherwise it is unstable.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003etypical stable algorithm\u003c/th\u003e\n\u003cth\u003etypical unstable algorithm\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eExample model\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e$k$NN (for some $k$)\u003c/td\u003e\n\u003ctd\u003edecision-tree learning\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCons\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ehigh bias\u003c/td\u003e\n\u003ctd\u003ehigh variance\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eNote parameters have effect on stability, i.e. in $k$NN:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1NN perfectly separates training data, so low bias but high variance\u003c/li\u003e\n\u003cli\u003eBy increasing $k$, we increase bias and decrease variance\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"supervised-learning\"\u003eSupervised Learning\u003c/h2\u003e\n\u003cp\u003eEnsemble methods are meta-algorithms that \u003cstrong\u003ecombine different models\u003c/strong\u003e into one model and they can:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDecrease variance\u003c/li\u003e\n\u003cli\u003eDecrease bias\u003c/li\u003e\n\u003cli\u003eImprove performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis idea relates to the \u0026quot;wisdom of crowd\u0026quot; phenomenon.\u003c/p\u003e\n\u003cp\u003eThere are different ways of combining predictors\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eSimple ensembles like majority vote or unweighted average\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWeighted averages / weighted votes: Every model gets a weight (i.e. depending on its performance)\u003c/p\u003e\n\u003cp\u003eIn practice:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eLearning algorithms may not be independent\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSome better fit the data so make less error\u003c/p\u003e\n\u003cp\u003eWe can define weights in different ways:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDecrease weight of correlated learners\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIncrease weight of good learners\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eTreat the output of each model as a feature and train a model on that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the task is binary classification and we choose the fusion model to be a linear model, then this will become a weighted vote\u003c/li\u003e\n\u003cli\u003eWe train the fusion model on unseen data otherwise it will be biased towards the models that performed better on the training data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMixture of experts\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeight $\\alpha_i(x)$ indicates \u0026quot;expertise\u0026quot;\u003c/li\u003e\n\u003cli\u003eIt divides the feature space into homogeneous regions\u003c/li\u003e\n\u003cli\u003eIt may use a weighted average or just pick the model with largest expertise\u003c/li\u003e\n\u003cli\u003eIt is a kind of local learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u0026quot;Bagging\u0026quot; method: (\u0026quot;\u003cstrong\u003eB\u003c/strong\u003eootstrap \u003cstrong\u003eAgg\u003c/strong\u003eregation\u0026quot;)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining many classifiers, but each with only a portion of the data\u003c/li\u003e\n\u003cli\u003eThen aggregate through model averaging / majority voting\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"bagging\"\u003eBagging\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBoostrap:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a random subset of data by sampling with replacement\u003c/li\u003e\n\u003cli\u003eDraw $m\u0026#39;$ samples from $m$ sample with replacement $(m\u0026#39; \\leq m)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eBagging:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRepeat $k$ times to generate $k$ subsets\u003cul\u003e\n\u003cli\u003eSome of the samples get repeated and some will not be left out\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTrain one classifier on each subset\u003c/li\u003e\n\u003cli\u003eTo test, aggregate the output of $k$ classifiers that you trained in the previous step using either majority vote / unweighted average\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eError of any model has two components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBias:\u003c/strong\u003e due to model choice which\u003cul\u003e\n\u003cli\u003ecan be reduced by increasing complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVariance:\u003c/strong\u003e due to small sample size or high complexity of the model\u003cul\u003e\n\u003cli\u003eCan be reduced by increasing the data or reducing the complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBagging is applied on a collection of low-bias high-variance models and by averaging them, the bias is unaffected, but the variance reduces.\u003c/p\u003e\n\u003cp\u003eTo calculate \u003cstrong\u003ebagging error:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf learners are independent\u003c/li\u003e\n\u003cli\u003eIf each learner makes an error with probability $p$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe probability that $k\u0026#39;$ out of $k$ learners make an error is:\u003c/p\u003e\n\u003cp\u003e$$\\binom{k}{k\u0026#39;}p^{k\u0026#39;}(1-p)^{k-k\u0026#39;}$$\u003c/p\u003e\n\u003cp\u003eIf we use majority voting to decide the output, then the error happens if more than $\\frac{k}{2}$ of learners make an error, so the error for majority voting is:\u003c/p\u003e\n\u003cp\u003e$$\\sum_{k\u0026#39; \u0026gt; \\frac{k}{2}} \\binom{k}{k\u0026#39;}p^{k\u0026#39;}(1-p)^{k-k\u0026#39;}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAdvantage of bagging:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReduces overfitting (harder for aggregated model to memorize full dataset)\u003c/li\u003e\n\u003cli\u003eThis improves performance in almost all cases esp if learning scheme is unstable\u003c/li\u003e\n\u003cli\u003eCan be applied to numeric prediction and classification\u003c/li\u003e\n\u003cli\u003eCan help a lot if data is noisy\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"random-forests\"\u003eRandom Forests\u003c/h2\u003e\n\u003cp\u003eTree models can be bagged.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReduces the overfitting\u003c/li\u003e\n\u003cli\u003eGeneralizes better\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen we bag a model, any simple structure is lost\u003cul\u003e\n\u003cli\u003eThis is because a bagged tree is no longer a tree, but a forest so this reduces claim to interpretability\u003c/li\u003e\n\u003cli\u003estable models like nearest neighbor not very affected by bagging but unstable models like trees most affected by bagging\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWith lots of data, we usually learn the same classifier, so averaging them does not help\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe solution for ensemble decision trees with lots of data is \u003cstrong\u003erandom forests\u003c/strong\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHave extra variability in the learners and introduce more randomness to the procedure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo produce a random forest\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSelect a random subsample from the dataset with replacement\u003c/li\u003e\n\u003cli\u003eSelect a subset of features randomly\u003c/li\u003e\n\u003cli\u003eBuild a full tree without pruning using the selected features and samples\u003c/li\u003e\n\u003cli\u003eRepeat previous steps $k$ times\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"boosting\"\u003eBoosting\u003c/h2\u003e\n\u003cp\u003eA problem with parallel learners is that they can all be mistaken in the same region.\nBoosting tries to solve this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUses \u0026quot;weak\u0026quot; learners which are trained sequentially\u003c/li\u003e\n\u003cli\u003eNew learners focus on errors of earlier learners\u003c/li\u003e\n\u003cli\u003eNew learners try to get these \u0026quot;hard\u0026quot; examples right by operating on a weighted train set in favor of misclassified instances\u003cul\u003e\n\u003cli\u003eStart with the same weight for all the instances\u003c/li\u003e\n\u003cli\u003eMisclassified instances gain higher weights: so the next classifier is more likely to classify it correctly\u003cul\u003e\n\u003cli\u003eGive different weights to the loss function for different instances\u003c/li\u003e\n\u003cli\u003eCreate a new collection of data with multiple copies of samples with higher weight\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCorrectly classified instances lose weight\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCombine all learners in the end\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eset $w_i = 1/m$ for $i = 1,...,m$\u003c/li\u003e\n\u003cli\u003eRepeat until sufficient number of hypothesis\u003cul\u003e\n\u003cli\u003eTrain model $L_j$ using the dataset with weight $w$\u003c/li\u003e\n\u003cli\u003eIncrease $w_i$ for misclassified instances of $L_j$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEnsemble hypothesis is the weighted majority / weighted average of $k$ learners $L_1, ..., L_k$ with weight $\\lambda_1, ..., \\lambda_k$ which are proportional to the accuracy of $L_j$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe always aim to minimize some cost function:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eUnweighted average loss\u003c/th\u003e\n\u003cth\u003eWeighted average loss\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e$J(\\theta) = \\frac{1}{N} \\sum_i J_i (\\theta, x_i)$\u003c/td\u003e\n\u003ctd\u003e$J(\\theta) = \\sum_i w_iJ_i (\\theta, x_i)$\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eBoosting works well as long as we use \u003cstrong\u003eweak learners\u003c/strong\u003e (weak learner is a model that is slightly better than random), i.e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePerceptron\u003c/li\u003e\n\u003cli\u003eDecision stumps (trees with one node)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"adaboost-adaptive-boosting\"\u003eAdaBoost (Adaptive Boosting)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAdaBoost usually uses \u003cstrong\u003estump trees\u003c/strong\u003e (trees with one node and two leaves) as the base learner\u003cul\u003e\n\u003cli\u003eNot very accurate at classification on their own\u003c/li\u003e\n\u003cli\u003eAdaBoost combines stumps to boost the performance so it creates a forest of stumps instead of forest of trees\u003c/li\u003e\n\u003cli\u003eIn AdaBoost, stumps are created sequentially\u003c/li\u003e\n\u003cli\u003eThe error of each stump affects the training data weight in the next stump\u003c/li\u003e\n\u003cli\u003eDepending on the performance, each stump gets different weight ($\\lambda_i$) in the final classification decision\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor boosting:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePros:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNo need to use complex models (can boost performance of any weak learner)\u003c/li\u003e\n\u003cli\u003eVery simple to implement\u003c/li\u003e\n\u003cli\u003eDecreases the bias and variance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCons:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLack of interpretability\u003c/li\u003e\n\u003cli\u003eSlow during training and potentially testing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"gradient-boosting\"\u003eGradient Boosting\u003c/h3\u003e\n\u003cp\u003eGradient boosting is apply similar ideas for regression (but can be used for classification as well).\u003c/p\u003e\n\u003cp\u003eSimple linear regression or simple regression trees can be used as weak learners.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLearn a regression predictor\u003c/li\u003e\n\u003cli\u003eCompute the error residual\u003c/li\u003e\n\u003cli\u003eLearn to predict residual\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eBagging\u003c/th\u003e\n\u003cth\u003eBoosting\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eVariance reduction\u003c/td\u003e\n\u003ctd\u003eBias reduction\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eModels Used\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eUsed with high variance models\u003c/td\u003e\n\u003ctd\u003eUsed with high bias models\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eModel Examples\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eRandom Forests\u003c/td\u003e\n\u003ctd\u003eLinear classifiers / univariate decision trees (decision stumps)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch1 id=\"neural-learning\"\u003e\u003ca href=\"#neural-learning\"\u003eNeural Learning\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003eArtificial Neural Networks (NN) are inspired by human nervous systems.\nNNs are composed of a large number of interconnected processing elements known as neurons.\nThey use supervised error correcting rules with back-propagation to learn a specific task.\u003c/p\u003e\n\u003cp\u003eA single perceptron has multiple inputs $x_1, ..., x_n$ and a binary output, where the output $o$ can be modeled as\u003c/p\u003e\n\u003cp\u003e$$\no(x_1, ..., x_n =\n\\begin{cases}\n  +1 \u0026amp; \\text{if } w_0 + w_1x_1 + ... + x_0x_0 \u0026gt; 0 \\\\\n  -1 \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eOr in vector notation:\u003c/p\u003e\n\u003cp\u003e$$\no(\\mathbf{x} =\n\\begin{cases}\n  +1 \u0026amp; \\text{if } \\mathbf{w} \\cdot \\mathbf{x} \u0026gt; 0\n  -1 \u0026amp; \\text{otherwise}\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003eAs a result a perceptron is able to represent some useful functions which are linearly separable\u003c/p\u003e\n\u003ch2 id=\"perceptron-learning\"\u003ePerceptron Learning\u003c/h2\u003e\n\u003cp\u003ePerceptron learning is simply an iterative weight-update scheme to find a good set of weights.\u003c/p\u003e\n\u003cp\u003e$$w_{i+1} \\leftarrow w_i + \\Delta w_i$$\u003c/p\u003e\n\u003cp\u003ewhere the weight update $\\Delta w_i$ depends only on misclassified examples and is modulated by a \u0026quot;smoothing\u0026quot; parameter $\\eta$ AKA learning rate.\u003c/p\u003e\n\u003cp\u003eFor example, the update rule can be written as\u003c/p\u003e\n\u003cp\u003e$$w_{i+1} \\leftarrow w_i + \\eta y_ix_i$$\u003c/p\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$w_t$ is the weight at iteration $t$\u003c/li\u003e\n\u003cli\u003e$\\eta$ is the learning rate\u003c/li\u003e\n\u003cli\u003e$y_i \\in \\{+1, 0, -1\\}$ acts to chance the sign (so if the current weights resulted in a missclassification, $y_i$ would reflect the update that needs to be applied to the weight)\u003c/li\u003e\n\u003cli\u003e$x_i$ is the instance value\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePerception training will converge (under some mild assumptions) for linearly separable classification problems.\u003c/p\u003e\n\u003cp\u003eHowever, with a relatively minor modification many perceptrons can be combined together to form one model\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultilayer perceptrons, the classic \u0026quot;neural network\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"gradient-descent-1\"\u003eGradient Descent\u003c/h2\u003e\n\u003cp\u003eGradient descent / ascent is an optimisation algorithm that seeks to minimize / maximize a function.\u003c/p\u003e\n\u003cp\u003eFor a simple linear unit, where\u003c/p\u003e\n\u003cp\u003e$$o = w_0 + w_1x_1 + ... + x_nx_n$$\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s learn $w_i$ that minimizes the squared error\u003c/p\u003e\n\u003cp\u003e$$E[w] = \\frac{1}{2} \\sum_{d \\in D} (t_d - o_d)^2$$\u003c/p\u003e\n\u003cp\u003ewhere $D$ is the set of training samples.\u003c/p\u003e\n\u003cp\u003eThe gradient is given by\u003c/p\u003e\n\u003cp\u003e$$\\nabla E[w] = \\left[ \\frac{\\partial E}{\\partial w_0}, \\frac{\\partial E}{\\partial w_1}, ..., \\frac{\\partial E}{\\partial w_n} \\right]$$\u003c/p\u003e\n\u003cp\u003eThe gradient vector gives the direction of steepest increase in error $E$.\nNegative of the gradient, i.e. steepest decrease, is what we want.\u003c/p\u003e\n\u003cp\u003eTraining rule: $\\Delta \\mathbf{w} = -\\eta \\nabla E[\\mathbf{w}]$, i.e. $\\Delta w_i = - \\eta \\frac{\\partial E}{\\partial w_i}$.\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\frac{\\partial E}{\\partial w_i}\n  \u0026amp;= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} \\sum_{d \\in D} (t_d - o_d)^2 \\\\\n  \u0026amp;= \\frac{1}{2} \\sum_d \\frac{\\partial}{\\partial w_i} (t_d - o_d)^2 \\\\\n  \u0026amp;= \\frac{1}{2} \\sum_d 2(t_d - o_d) \\frac{\\partial}{\\partial w_i}(t_d - o_d) \\\\\n  \u0026amp;= \\sum_d (t_d - o_d) \\frac{\\partial}{\\partial w_i} (t_d - \\mathbf{w} \\cdot \\mathbf{x}_d) \\\\\n  \u0026amp;= \\sum_d (t_d - o_d)(-x_{i,d})\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003ePerceptron training rule guaranteed to succeed if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining examples are linearly separable\u003c/li\u003e\n\u003cli\u003eSufficiently small learn rate $\\eta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLinear unit training rule uses gradient descent\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGuaranteed to converge to hypothesis with minimum squared error\u003c/li\u003e\n\u003cli\u003eGiven sufficiently small learning rate $\\eta$\u003c/li\u003e\n\u003cli\u003eEven when training data contains noise and or not separable by $H$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"stochastic-gradient-descent\"\u003eStochastic Gradient Descent\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBatch mode\u003c/strong\u003e Gradient Descent\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDo until satisfied\u003cul\u003e\n\u003cli\u003eCompute the gradient $\\nabla E_D[\\mathbf{w}] = \\frac{1}{2} \\sum_{d \\in D} (t_d - o_d)^2$\u003c/li\u003e\n\u003cli\u003e$\\mathbf{w} \\leftarrow \\mathbf{w} \\eta \\nabla E_D[\\mathbf{w}]$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eStochastic (incremental) mode\u003c/strong\u003e Gradient Descent\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDo until satisfied\u003cul\u003e\n\u003cli\u003eFor each training example $d \\in D$\u003cul\u003e\n\u003cli\u003eCompute the gradient $\\nabla E_d[\\mathbf{w}] = \\frac{1}{2}(t_d - o_d)^2$\u003c/li\u003e\n\u003cli\u003e$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla E_d[\\mathbf{w}]$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSGD can approximate Batch Gradient Descent arbitrarily closely, if $\\eta$ made small enough\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVery useful for training large networks, or online learning from data streams\u003c/li\u003e\n\u003cli\u003eStochastic implies examples should be selected at random\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"multilayer-networks\"\u003eMultilayer Networks\u003c/h2\u003e\n\u003cp\u003eMultilayer networks can represent arbitrary functions.\nIt typically consists of an input, hidden, and output layer, each fully connected to the next.\nThe weights determine the function computed.\nGiven an arbitrary number of hidden units, any boolean function can be computed with a single hidden layer.\u003c/p\u003e\n\u003cp\u003eProperties of Artificial Neural Networks (ANN\u0026#39;s):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMany neuron-like threshold switching units\u003c/li\u003e\n\u003cli\u003eMany weighted interconnections among units\u003c/li\u003e\n\u003cli\u003eHighly parallel, distributed process\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eConsider when\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput is high-dimensional\u003c/li\u003e\n\u003cli\u003eOutput can be a vector of values or discrete, or real valued\u003c/li\u003e\n\u003cli\u003eForm of target function is unknown\u003c/li\u003e\n\u003cli\u003eInterpretability of result is not important\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExamples include speech recognition, image classification, and many others.\u003c/p\u003e\n\u003ch2 id=\"sigmoid-unit\"\u003eSigmoid Unit\u003c/h2\u003e\n\u003cp\u003eSame as a perceptron except step function is replaced by a nonlinear sigmoid function.\u003c/p\u003e\n\u003cp\u003e$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\nNonlinearity makes it easy for the model to generalise or adapt with variety of data and to differentiate between the output.\u003c/p\u003e\n\u003cp\u003eThe sigmoid function is used because its derivative has a nice property\u003c/p\u003e\n\u003cp\u003e$$\\frac{d \\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))$$\u003c/p\u003e\n\u003cp\u003eWe can derive gradient descent rules to train\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOne sigmoid unit\u003c/li\u003e\n\u003cli\u003eMultilayer networks of sigmoid units -\u0026gt; Backpropagation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote: in practice, particularly for deep networks, sigmoid functions are less common than other non-linear activation functions that are easier to train, however sigmoids are mathematically convenient.\u003c/p\u003e\n\u003cp\u003eTo determine the error gradient of a sigmoid unit\u003c/p\u003e\n\u003cp\u003eStart by assuming we want to minimize squared error over a set of training examples $D$.\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\n\\frac{\\partial E}{\\partial w_i}\n  \u0026amp;= \\frac{\\partial}{\\partial w_i} \\frac{1}{2} \\sum_{d \\in D}(t_d - o_d)^2 \\\\\n  \u0026amp;= \\frac{1}{2}  \\sum_{d \\in D} \\frac{\\partial}{\\partial w_i} (t_d - o_d)^2 \\\\\n  \u0026amp;= \\frac{1}{2}  \\sum_{d \\in D} 2(t_d - o_d)\\frac{\\partial}{\\partial w_i} (t_d - o_d) \\\\\n  \u0026amp;= \\sum_{d \\in D} 2(t_d - o_d) \\left( \\frac{\\partial o_d}{\\partial w_i} \\right) \\\\\n  \u0026amp;= -\\sum_{d \\in D} 2(t_d - o_d) \\frac{\\partial o_d}{\\partial net_d} \\frac{\\partial net_d}{\\partial w_i}\\\\\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003eWe know:\u003c/p\u003e\n\u003cp\u003e$$\\frac{\\partial o_d}{\\partial net_d} = \\frac{\\partial \\sigma(net_d)}{\\partial net_d} = o_d(1 - o_d)$$\u003c/p\u003e\n\u003cp\u003e$$\\frac{\\partial net_d}{\\partial w_i} = \\frac{\\partial(\\mathbf{w} \\cdot \\mathbf{x})d)}{\\partial w_i} = x_{i,d}$$\u003c/p\u003e\n\u003cp\u003eso:\u003c/p\u003e\n\u003cp\u003e$$\\frac{\\partial E}{\\partial w_i} = - \\sum_{d \\in D}(t_d - o_d) o_d (1 - o_d) x_{i,d}$$\u003c/p\u003e\n\u003ch2 id=\"backpropagation-algorithm\"\u003eBackpropagation Algorithm\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eInitialize all weights to small random numbers\u003c/li\u003e\n\u003cli\u003eUntil satisified, do\u003cul\u003e\n\u003cli\u003eFor each training example, do\u003cul\u003e\n\u003cli\u003eInput the training example to the network and compute the network outputs\u003c/li\u003e\n\u003cli\u003eFor each output unit $k$\n$$\\delta_k \\leftarrow o_k(1-o_k)(t_k-o_k)$$\u003c/li\u003e\n\u003cli\u003eFor each hidden unit $h$\n$$\\delta_h \\leftarrow o_h(1-o_h) \\sum_{k \\in outputs} w_{kh} \\delta_k$$\u003c/li\u003e\n\u003cli\u003eUpdate each network weight $w_{ji}$\n$$w_{ji} \\leftarrow w_{ji} + \\Delta w_{ji}$$\nwhere\n$$\\Delta w_{ji} = \\eta \\delta_j x_{ji}$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA solution for learning highly complex models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egradient descent over entire network weight vector\u003c/li\u003e\n\u003cli\u003eEasily generalised to arbitrary directed graphs\u003c/li\u003e\n\u003cli\u003eCan learn probabilistic models by maximising likelihood\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMinimises error over all training examples\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraining can take thousands of iterations -\u0026gt; slow\u003c/li\u003e\n\u003cli\u003eUsing network after training is very fast\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWill converge to a local, not necessarily global, error minimum\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMight exist many such local minima, but in practice often works well\u003c/li\u003e\n\u003cli\u003eOften include weight momentum $a$\n$$\\Delta w_{ji}(n) = \\eta \\delta_j x_{ji} + \\alpha \\Delta w_{ji} (n-1)$$\u003c/li\u003e\n\u003cli\u003eStochastic gradient descent using \u0026quot;mini-batches\u0026quot;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMany ways to regularise network, making it less likely to overfit\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd term to error that increases with magnitude of weight vector\n$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_{d \\in D} \\sum_{k \\in outputs} (t_{kd} - o_{kd})^2 + \\gamma \\sum_{i,j} w_{ji}^2$$\u003c/li\u003e\n\u003cli\u003eOther ways to penalise large weights, e.g. weight decay\u003c/li\u003e\n\u003cli\u003eUsing \u0026quot;tied\u0026quot; or shared set of weights, e.g. by setting all weights to their mean after computing the weight updates\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"neural-networks-for-classification\"\u003eNeural Networks for Classification\u003c/h2\u003e\n\u003cp\u003eMinimizing square error does not work so well for classification.\u003c/p\u003e\n\u003cp\u003eIf we take the output $o(x)$ as the probability of the class $\\mathbf{x}$ being 1, the preferred loss function is the cross-entropy\u003c/p\u003e\n\u003cp\u003e$$- \\sum_{d \\in D} t_d \\log o_d + (1 - t_d) \\log (1 - o_d)$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cp\u003e$t_d \\in \\{0,1\\}$ is the class label for training example $d$, and $o_d$ is the output of the sigmoid unit, interpreted as the probability of the class of training example $d$ being 1.\u003c/p\u003e\n\u003cp\u003eTo train sigmoid units for classification using this setup, one can use gradient descent and backpropagation algorithm - this will yield the maximum likelihood solution.\u003c/p\u003e\n\u003ch2 id=\"convolutional-neural-networks\"\u003eConvolutional Neural Networks\u003c/h2\u003e\n\u003cp\u003eCNNs are very similar to regular Neural Networks\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMade up of neurons with learnable weights\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCNN architecture assumes that inputs are images\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSo we have local features\u003c/li\u003e\n\u003cli\u003eWhich allows us to\u003cul\u003e\n\u003cli\u003eencode certain properties in the architecture that makes the forward pass more efficient nad\u003c/li\u003e\n\u003cli\u003esignificantly reduces the number of parameters need for the network\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe problem with regular NNs is that they do not scale well with dimensions.\nIn contrast, CNNS, consider 3D volumes of neurons and propose a parameter sharing scheme that minimises the number of params required by the network.\u003c/p\u003e\n\u003cp\u003eCNN neurons are arranged in 3 dimensions: Width, Height, and Depth.\nNeurons in a layer are only connected to a small region of the layer before it (hence not fully connected).\u003c/p\u003e\n\u003cp\u003eMain layers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConvolutional\u003cul\u003e\n\u003cli\u003eApplies convolution operation (converts all pixels in its receptive field into a single value)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePooling\u003cul\u003e\n\u003cli\u003ethe purpose is to reduce the spacial size of the representation to reduce the number of params and computation in the network and control overfitting\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRectified Linear Unit (ReLU)\u003cul\u003e\n\u003cli\u003eReally an activation function ($f(x) = \\max(0,x)$) favoured over traditional activation functions like Sigmoid or Tanh\u003cul\u003e\n\u003cli\u003eTo accelerate the convergence of stochastic gradient descent\u003c/li\u003e\n\u003cli\u003eBe computationally inexpensive compared to traditional ones\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFully-connected\u003cul\u003e\n\u003cli\u003eA fully connected layer to all activations\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDrop-out\u003cul\u003e\n\u003cli\u003eA layer where each forward pass, some neurons are randomly set to zero\u003c/li\u003e\n\u003cli\u003eThis reduces overfitting\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput layers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"unsupervised-learning\"\u003e\u003ca href=\"#unsupervised-learning\"\u003eUnsupervised Learning\u003c/a\u003e\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003eSupervised learning\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClasses are known and need a \u0026quot;definition\u0026quot; in terms of the data\u003c/li\u003e\n\u003cli\u003eMethods are known as:\u003cul\u003e\n\u003cli\u003eclassification\u003c/li\u003e\n\u003cli\u003ediscriminant analysis\u003c/li\u003e\n\u003cli\u003eclass prediction\u003c/li\u003e\n\u003cli\u003esupervised pattern recognition\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUnsupervised learning\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClasses are initially unknown and need to be discovered with their definitions from the data\u003c/li\u003e\n\u003cli\u003eMethods are known as:\u003cul\u003e\n\u003cli\u003ecluster analysis\u003c/li\u003e\n\u003cli\u003eclass discovery\u003c/li\u003e\n\u003cli\u003eunsupervised pattern recognition\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"clustering\"\u003eClustering\u003c/h2\u003e\n\u003cp\u003eClustering algorithms form two broad categories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePartitioning\u003c/strong\u003e methods\u003cul\u003e\n\u003cli\u003eTypically require specification of the number of clusters\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHierarchical\u003c/strong\u003e methods\u003cul\u003e\n\u003cli\u003eHierarchical algorithms are either \u003cstrong\u003eagglomerative\u003c/strong\u003e i.e., bottom-up or \u003cstrong\u003edivisive\u003c/strong\u003e i.e., top-down\u003c/li\u003e\n\u003cli\u003eIn practice, hierarchical agglomerative methods are often used\u003c/li\u003e\n\u003cli\u003eBut more importantly to users, the dendrogram, or tree, which can be visualized in hierarchical methods\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet $X = \\{x_1, ..., x_m \\}$ be a set of instances\u003c/p\u003e\n\u003cp\u003eLet $C = (C_1, ..., C_k)$ be a partition of $m$ elements into $k$ subsets.\u003c/p\u003e\n\u003cp\u003eEach subset is called a cluster, and $C$ is called a clustering.\u003c/p\u003e\n\u003ch2 id=\"k-means-clustering\"\u003eK-means Clustering\u003c/h2\u003e\n\u003cp\u003eSet value for $k$, th number of clusters\u003c/p\u003e\n\u003cp\u003eInitialize: choose points for centres (means) of $k$ clusters (at random)\u003c/p\u003e\n\u003cp\u003eProcedure:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAssign each instance $x$ to the closest of the $k$ points to form $k$ clusters\u003c/li\u003e\n\u003cli\u003eRe-assign the $k$ points to be the means of each of the $k$ clusters\u003c/li\u003e\n\u003cli\u003eRepeat 1 and 2 until convergence to a reasonably stable clustering\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDespite weaknesses (suffering from outliers, or trapped in local minimum), $k$-means is still the most popular algorithm due to simplicity and efficiency.\nThere is no clear evidence that any other clustering algorithm performs better in general.\u003c/p\u003e\n\u003ch2 id=\"expectation-maximization\"\u003eExpectation Maximization\u003c/h2\u003e\n\u003cp\u003eFor $k = 2$, think of the full description of each instance as $y_i = (z_i, z_{i1}, z_{i2})$, where\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$z_{ij}$ is 1 if $x_i$ generated by $j^{th}$ Gaussian, otherwise zero\u003c/li\u003e\n\u003cli\u003e$x_i$ is observable, from instance set $x_1, x_2, ..., x_m$\u003c/li\u003e\n\u003cli\u003e$z_{ij}$ is unobservable\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGiven:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstances from $X$ generated by mixture of $k$ Gaussian distributions\u003c/li\u003e\n\u003cli\u003eUnknown means of the $k$ Gaussians\u003c/li\u003e\n\u003cli\u003eAssuming identity covariance matrix\u003c/li\u003e\n\u003cli\u003eDon\u0026#39;t know whcih instance $x_i$ was generated by which Guassian\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDetermine Maximum likelihood estimates of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\text{means}(\\mu_1, ...,\\mu_k)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eInitialize:\u003c/strong\u003e Pick random initial $h = (\\mu_1, ..., \\mu_k$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIterate:\u003c/strong\u003e\n\u003cstrong\u003eE step:\u003c/strong\u003e Calculate expected value $E[z_{ij}$ of each hidden variable $z_{ij}$, assuming current hypothesis $h = (\\mu_1, ..., \\mu_k)$ holds:\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{align*}\nE[z_{ij}]\n  \u0026amp;= \\frac{P(x=x_i|\\mu=\\mu_j)}{\\sum_{n=1}^k p(x=x_i|\\mu=\\mu_n)} \\\\\n  \u0026amp;= \\frac{\\exp \\left( - \\frac{1}{2 \\sigma^2} (x_i - \\mu_j)^2 \\right)}{\\sum_{n=1}^k \\exp \\left( - \\frac{1}{2 \\sigma^2} (x_i - \\mu_n)^2 \\right)}\n\\end{align*}\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eM step:\u003c/strong\u003e Calculate new maximum likelihood hypthesis $h\u0026#39; = (\\mu_1\u0026#39;, ..., \\mu_k\u0026#39;)$ assuming value taken on each hidden variable $z_{ij}$ is the expected value $E[z_{ij}]$ calculated before.\nReplace $h = (\\mu_1, ..., \\mu_k)$ by $h\u0026#39; = (\\mu_1\u0026#39;, ..., \\mu_k\u0026#39;)$.\u003c/p\u003e\n\u003cp\u003e$$\\mu_j \\leftarrow \\frac{\\sum_{i=1}^m E[z_{ij}] x_i}{\\sum_{i=1}^m E[z_{ij}]}$$\u003c/p\u003e\n\u003cp\u003eConverges to local maximum likelihood $h$ and provides estimates of hidden variables $z_{ij}$.\u003c/p\u003e\n\u003cp\u003eIn fact, local maximum in $E[\\ln P(Y|h)]$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$Y$ is complete (observable plus unobservable variables) data\u003c/li\u003e\n\u003cli\u003eExpected value taken over possible values of unobserved variables in $Y$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"hierarchical-clustering\"\u003eHierarchical Clustering\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBottom up:\u003c/strong\u003e at each step join two closest clusters (starting with single-instance clusters)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDesign decision: distance between clusters E.g., two closest instances in clusters vs distance between means\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTop down:\u003c/strong\u003e find two clusters and then proceed recursively for the two subsets\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCan be very fast\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"determining-number-of-clusters\"\u003eDetermining number of clusters\u003c/h3\u003e\n\u003cp\u003eElbow method:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMeasure within-cluster dispersion (sum of squared distances from each point to cluster centroid)\u003c/li\u003e\n\u003cli\u003eCompute this for various $k$ choices\u003c/li\u003e\n\u003cli\u003eChoose the $k$ that doesn\u0026#39;t improve the dispersion that much\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGap statistics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCluster observed data and compute the corresponding total with-cluster variation $W_k$\u003c/li\u003e\n\u003cli\u003eGenerate $b$ reference data sets with random uniform distribution.\nCluster each of these reference data sets and compute the corresponding total within-cluster variation $W_{kb}$\u003c/li\u003e\n\u003cli\u003eCompute the estimated gap statistic as the deviation of the observed $W_k$ value from its expected value $W_{kb} : Gap(k) = \\frac{1}{B} \\sum_{b} \\log(W_{kb}) - \\log(W_k)$.\nCompute also the std dev of the statistics\u003c/li\u003e\n\u003cli\u003eChoose $k$ as the smallest value s.t. the gap statistic is within 1 std dev of the gap at $k + 1$\n$$Gap(k) \\geq Gap(k + 1) - s_{k+1}$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"principal-component-analysis-pca\"\u003ePrincipal Component Analysis (PCA)\u003c/h2\u003e\n\u003cp\u003eKey idea: look for features in a transformed space so that each dimension in the new space captures the most variation in the original data when it is projected onto that dimension.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTake the data as an $m \\times n$ matrix $X$\u003c/li\u003e\n\u003cli\u003e\u0026quot;centre\u0026quot; the data by subtracting the mean of each column\u003c/li\u003e\n\u003cli\u003eConstruct covariance matrix $C$ from centred matrix\u003c/li\u003e\n\u003cli\u003eCompute eigenvector matrix $V$ (rotation) and eigenvalue matrix $S$ (scaling) such that $V^{-1}CV = S$ and $S$ is a diagonal $n \\times n$ matrix\u003c/li\u003e\n\u003cli\u003eSort columns of $S$ in decreasing order (decreasing variance) and their corresponding eigenvectors\u003c/li\u003e\n\u003cli\u003eRemove columns of $S$ and $V$ that the eigenvalues are below some minimum threshold\u003c/li\u003e\n\u003cli\u003eTransform the original data using the remaining eigenvectors\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003ePCA complexity is cubic in number of original features (not feasible for high-dimensional datasets)\u003c/li\u003e\n\u003cli\u003eAlternatively can use random projections to approximate the sort of projection found by PCA\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"semi-supervised-learning\"\u003eSemi-supervised Learning\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eLearn initial classifier using labelled set\u003c/li\u003e\n\u003cli\u003eApply classifier to unlabelled set\u003c/li\u003e\n\u003cli\u003eThe most confident predictions of each classifier on the unlabelled data are retained\u003c/li\u003e\n\u003cli\u003eLearn new classifier from now-labelled data\u003c/li\u003e\n\u003cli\u003eRepeat until convergence\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"self-training-algorithm\"\u003eSelf-training algorithm\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGiven:\u003c/strong\u003e labelled data $(x,y)$ and unlabelled data $(x)$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRepeat:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTrain classifier $H$ from labelled data using supervised learning\u003c/li\u003e\n\u003cli\u003eLabel unlabelled data using classifier $h$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAssumes:\u003c/strong\u003e classifications by $h$ will tend to be correct (especially high probability ones)\u003c/p\u003e\n\u003ch2 id=\"co-training\"\u003eCo-Training\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e two views of an instance, $f_1$ and $f_2$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eassume $f_1$ and $f_2$ independent and compatible\u003c/li\u003e\n\u003cli\u003eIf we hve a good attribute set, leverage similarity between attribute values in each view, assuming they predict the class, to classify the unlabelled data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMulti-view learning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGiven two (or more) perspectives on data, e.g., different attribute sets\u003c/li\u003e\n\u003cli\u003eTrain separate models for each perspective on small set of labelled data\u003c/li\u003e\n\u003cli\u003eUse models to label a subset of the unlabelled data\u003c/li\u003e\n\u003cli\u003eRepeat until no more unlabelled examples\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"learning-theory\"\u003e\u003ca href=\"#learning-theory\"\u003eLearning Theory\u003c/a\u003e\u003c/h1\u003e\n\n\u003ch2 id=\"computational-learning-theory\"\u003eComputational Learning Theory\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eMachine learning:\u003c/strong\u003e Have a computer solve problems by learning from data\nrather than being explicitly programmed.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRegression\u003c/li\u003e\n\u003cli\u003eClassification\u003c/li\u003e\n\u003cli\u003eClustering\u003c/li\u003e\n\u003cli\u003eRanking\u003c/li\u003e\n\u003cli\u003eReinforcement Learning\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eInductive learning:\u003c/strong\u003e learning from examples and all machine learning\nalgorithms are a kind of inductive learning and there are some\nquestions that we are interested to be able to answer in such\nframework:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProbability of successful learning\u003c/li\u003e\n\u003cli\u003eNumber of training examples\u003c/li\u003e\n\u003cli\u003eComplexity of hypothesis space\u003c/li\u003e\n\u003cli\u003eTime complexity of learning algorithm\u003c/li\u003e\n\u003cli\u003eAccuracy to which target concept is approximated\u003c/li\u003e\n\u003cli\u003eManner in which training examples presented\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInstead of focusing on particular algorithms, learning theory aims to characterize classes of algorithms.\u003c/p\u003e\n\u003ch2 id=\"probably-approximately-correct\"\u003eProbably Approximately Correct\u003c/h2\u003e\n\u003cp\u003eProbably Approximately Correct (PAC) is a framework for mathematical analysis of learning which was proposed by Valiant in 1984.\u003c/p\u003e\n\u003cp\u003eThe framework of PAC learning can be used to address questions such as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow many training examples?\u003c/li\u003e\n\u003cli\u003eHow much computational effort required?\u003c/li\u003e\n\u003cli\u003eHow complex a hypothesis class needed?\u003c/li\u003e\n\u003cli\u003eHow to quantify hypothesis complexity?\u003c/li\u003e\n\u003cli\u003eHow many mistakes will be made?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"sample-complexity\"\u003eSample Complexity\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGiven:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eset of instances $X$\u003c/li\u003e\n\u003cli\u003eset of hypotheses $H$\u003c/li\u003e\n\u003cli\u003eset of possible target concepts $C$\u003c/li\u003e\n\u003cli\u003etraining instances generated by a fixed, unknown probability distribution $\\mathcal{D}$ over $X$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLearner observes a sequence $D$ of training examples of form $(x,c(x))$, for some target concept $c \\in C$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstances $x$ are drawn from distribution $\\mathcal{D}$\u003c/li\u003e\n\u003cli\u003eteacher provides target value $c(x)$ for each $x$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLearner must output a hypothesis $h$ estimating $c$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$h$ is evaluated by its performance on subsequent instances drawn according to $\\mathcal{D}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote: randomly drawn instances\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrue Error of a Hypothesis\u003c/strong\u003e: The \u003cstrong\u003etrue error\u003c/strong\u003e (denoted $error_{\\mathcal{D}}(h)$) of hypothesis $h$ with respect to target concept $c$ and distribution $\\mathcal{D}$ is the probability that $h$ will misclassify an instance drawn at random according to $\\mathcal{D}$.\u003c/p\u003e\n\u003cp\u003e$$error_{\\mathcal{D}}(h) \\equiv \\text{Pr}_{X \\in \\mathcal{D}} [c(x) \\neq h(x)]$$\u003c/p\u003e\n","metadata":{"title":"COMP9417","description":"Machine Learning and Data Mining","date":"2022-05-02","mathjax":true,"hljs":true},"toc":[{"id":"statistical-techniques-for-data-analysis","title":"Statistical Techniques for Data Analysis"},{"id":"regression","title":"Regression"},{"id":"classification","title":"Classification"},{"id":"tree-learning","title":"Tree Learning"},{"id":"kernel-methods","title":"Kernel Methods"},{"id":"ensemble-learning","title":"Ensemble Learning"},{"id":"neural-learning","title":"Neural Learning"},{"id":"unsupervised-learning","title":"Unsupervised Learning"},{"id":"learning-theory","title":"Learning Theory"}]},"__N_SSG":true},"page":"/writings/[slug]","query":{"slug":"comp9417"},"buildId":"ILkuclJ7g-R-hn-4-ve6L","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>